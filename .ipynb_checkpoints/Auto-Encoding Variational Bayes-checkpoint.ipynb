{
 "metadata": {
  "name": "",
  "signature": "sha256:38cba333abf4c1f38a5bb9cfd50e337c63600489a5e0e7e394085cc948661202"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Auto-Encoding Variational Bayes#\n",
      "\n",
      "Extending the classical variational inference framework to the stochastic (online and minibatch) version was completed by Hoffman et al (2013). In this work, Kingma and Welling make a further extension to non-factorable variational distributions. Their method has fewer assumptions than those of standard stochastic variational inference. In particular, their method, Auto-Encoding Variational Bayes, has no need for tractable expectations with respect to the marginal or conditional distributions at play.\n",
      "\n",
      "##Outline ##\n",
      "\n",
      "Stochastic variational inference\n",
      "\n",
      "Auto-Encoding Variational Bayes\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------------------\n",
      "###Relationship to Stochastic variational inference###\n",
      "\n",
      "Recall we have observations $x_i, i \\in 1, \\dotsc, n$, and a local hidden variable $z_i$ for each $x_i$. We assume $x_i$ has been generated by a model with some (global) parameters $\\theta$, which also influence the prior over $z_i$.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have a model with the following general setup:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$n$ observations $x_i, i = 1, \\ldots, n$\n",
      "\n",
      "$\\theta$, a vector of global hidden variables\n",
      "\n",
      "$z = z_{1:n}$, local hidden variables.\n",
      "\n",
      "$\\phi$, the local variational parameters associated with the approximating distribution $Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model can be visualized using a plate diagram (Kingma and Welling, 2013):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/AEVBPlateModel.png\" alt=\"A plate model for the model with N observations\" style=\"width:400px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In traditional stochastic variational inference, there is no directed connection from $x_i$ to $z_i$ (Hoffman et al 2013), but we're still interested in the posterior of $z_i$ given $x_i$ and $\\theta$. Classical mean-field and stochastic variational inference both already work in the case of an intractable posterior distribution $P_{\\theta} (\\mathbf{z} \\mid \\mathbf{x})$. That is, they're a reasonable alternative when the EM algorithm cannot be used. \n",
      "\n",
      "#### Algorithm for Stochastic Variational Inference ####\n",
      "\n",
      "Randomly initialize global parameter(s) $\\lambda$\n",
      "Set a step size schedule $\\epsilon_t$\n",
      "Repeat\n",
      "\n",
      "1) Sample an observation $x_i$ from your dataset, according to a DU[1, n] distribution.\n",
      "\n",
      "2) Compute the local variational parameter $\\phi_i$ for that particular observation, according to:\n",
      "\n",
      "  $$\\phi = E_{\\lambda^{(t-1)}}\\left[\\nabla_\\ell (x_i, \\theta) \\right]$$\n",
      "  \n",
      "  (Note: computing $\\phi$ here requires a closed-form expectation. This is not the case in AEVB).\n",
      "  \n",
      "3) Make believe that $x_i$ is the only observation in the dataset, and calculate\n",
      "\n",
      "  $$\\hat{\\lambda} = E_{\\phi} \\left[ \\eta_i (x_i, z_i)\\right]$$\n",
      "  \n",
      "4) Update the current estimate of the global variational parameter:\n",
      "\n",
      "  $$\\lambda^{t} = (1 - \\epsilon_t) \\lambda^{(t-1)} + \\epsilon_t\\hat{\\lambda}$$\n",
      "  \n",
      "\n",
      "By assuming complete conditionals that are in the exponential family, the classic stochastic variational inference algorithm ensures that the expectations that form the lower bound on the marginal likelihood are tractable. However, this assumption doesn't hold in general.\n",
      "\n",
      "In particular, if the likelihood is formed by a neural network with a nonlinear hidden layer (Kingma and Welling 2013), then the expectation of the log-likelihood will be intractable. \n",
      "\n",
      "A better algorithm would work when 1) the posterior is intractable (so we can't use the EM algorithm); 2) the integrals for mean-field variational inference are intractable (so the expectations required in mean-field and stochastic variational inference can't be computed); 3) we have so many datapoints that MCMC would be prohibitively computationally expensive. \n",
      "\n",
      "By 'work', the algorithm would allows us to: 1) conduct efficient ML or MAP estimation for $\\theta$, so we can generate more data; 2) conduct efficient approximate posterior inference for $\\mathbf{z}$ given $\\mathbf{x}$ and $\\theta$ (useful for data visualization); 3) conduct efficient approximate marginal inference for $\\mathbf{x}$ (useful for example, for image denoising).\n",
      "\n",
      "<!--\n",
      "For example, a neural network with a nonlinear hidden layer will have an intricate log-likelihood, making expectations difficult to calculate.\n",
      "-->\n",
      "\n",
      "Kingma and Welling (2013) use the following method, which they term $\\rm{\\mathbf{Variational\\ Auto-Encoding}}$:\n",
      "\n",
      "1) Decompose the marginal likelihood into the KL divergence and the lower bound.\n",
      "\n",
      "2) Choose a variational distribution $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$. This distribution is not subject to the previous (mean field) assumption of factoribility. It is a neural network (an MLP in this work). $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$ is also called the 'encoder' or the 'recognition model'. It finds a distributed representation $\\mathbf{z}$ of the input $\\mathbf{x}$ based on variational parameters $\\phi$. \n",
      "\n",
      "3) The objective function contains the lower bound, which is not differentiable, so must be sampled from. Sample from the lower bound to build an estimator for it.\n",
      "\n",
      "4) We now have an estimator (approximate expectation from step 3) for the lower bound. We can take derivatives.\n",
      "\n",
      "5) Use stochastic gradient descent to find optimal values of $\\theta$, $\\phi$.\n",
      "\n",
      "Summing up, the goal is to optimize the recognition model $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$, which approximates $P_{\\theta} (\\mathbf{z} \\mid \\mathbf{x})$, using the stochastic gradient variational bayes (SGVB) estimator, then use ancestral sampling of $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$ to conduct approximate posterior inference.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The steps, in detail ###\n",
      "\n",
      "In the paper, the variational distribution $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$ is called the recognition model, or the 'encoder'. The point of the encoder is to find a distributed representation of the input, $\\mathbf{x}$. Then to reconstruct the input, a decoder $P_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})$ is needed. To see the relationship in more detail requires examining the math behind this idea, using the steps outlined above.\n",
      "\n",
      "Step 1 is to decompose the marginal likelihood ${\\rm{ log\\ }}P_{\\theta} (\\mathbf{x}^{1}, \\ldots, \\mathbf{x}^{n}) = \\sum_{i = 1}^{n} {\\rm {log\\ }} P_{\\theta} (\\mathbf{x}^{(i)})$\n",
      "\n",
      "We've shown previously that the marginal likelihood can be written in terms of the KL divergence and the expectation lower bound (ELBO):\n",
      "\n",
      "${\\rm{ log\\ }}P_{\\theta} (\\mathbf{x}^{1}, \\ldots, \\mathbf{x}^{n}) = {\\rm{KL}}(Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})\\ ||\\ P_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})) + \\mathcal{L}(\\mathbf{\\theta}, \\mathbf{\\phi}; \\mathbf{x}^{(i)})$\n",
      "\n",
      "We've also shown that: (Note: I think the paper has a typo and these should be $\\mathbf{x}^{(i)}$'s, not $\\mathbf{x}$'s)\n",
      "\n",
      "$\\mathcal{L}(\\mathbf{\\theta}, \\mathbf{\\phi}; \\mathbf{x}^{(i)}) = E_{Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})}  \\left[-{\\rm log\\ } Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}) + {\\rm {log}\\ } P_{\\theta} (\\mathbf{x}, \\mathbf{z}) \\right] $ (first breakdown of the ELBO)\n",
      "\n",
      "Using the definition of expectation and log rules (shown [here](Equation2toEquation3KingmaWelling2013.ipynb)), we can rewrite the above lower bound as:\n",
      "\n",
      "$\\mathcal{L}(\\mathbf{\\theta}, \\mathbf{\\phi}; \\mathbf{x}^{(i)}) = -{\\rm{KL}}(Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})\\ ||\\ P_{\\theta} (\\mathbf{z})) + E_{Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})} \\left[ {\\rm{log}\\ } P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}) \\right] $ (second breakdown of the ELBO)\n",
      "\n",
      "\n",
      "Since optimizing $Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})$ amounts to to optimizing the ELBO, the goal is to maximize the ELBO with respect to both the variational parameters $\\phi$ and the generative (model) parameters $\\theta$. According to Kingma and Welling (2013), it is possible to take a gradient with respect to $\\theta$ but not with respect to $\\phi$. At this point I'm not sure why this is the case (notes page 4, 19/07/2015).\n",
      "\n",
      "The typical way to estimate a derivative when you can't calculate it analytically is to use a Monte Carlo method. \n",
      "\n",
      "Taking the first form of the ELBO, we would need to take the gradient of an expected value. The general Monte Carlo estimate is:\n",
      "\n",
      "$$\\nabla_{\\phi} E_{q_{\\phi} \\mathbf{(z)}} [f(\\mathbf{z})] = E_{q_{\\phi}(z)} [f(\\mathbf{z}) \\nabla_{q_{\\phi}(z)} {\\rm{log}} q_{\\phi} (\\mathbf{z})] \\approx \\frac{1}{L}\\sum_{\\ell = 1}^{L} f(\\mathbf{z}) \\nabla_{q_{\\phi}(\\mathbf{z^{\\ell}})}{\\rm {log}} q_{\\phi} (\\mathbf{z}^{(\\ell)}) $$\n",
      "\n",
      "<!--\n",
      "= E_{q_{\\phi}(z)} [f(\\mathbf{z} \\nabla_{q_{\\phi}(z)} {\\rm{log}} q_{\\phi} (\\mathbf{z})] \\approx \\frac{1}{L} \\sum_{\\ell = 1}^{L} f(\\mathbf{z}) \\nabla_{q_{\\phi}}(\\mathbf{z^{\\ell}}) {\\rm {log}} q_{\\phi} (\\mathbf{z}^{(\\ell)})$$,\n",
      "-->\n",
      "\n",
      "where $\\mathbf{z}^{(l)} \\sim q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})$\n",
      "\n",
      "The Monte Carlo estimator, though, has too high a variance to be useful here. Instead, the paper illustrates the use of a clever trick to get a good estimator. Other tricks that have been tried are Rao-Blackwellization and control variates (Black Box Variational Inference), but I haven't found anyone trying antithetic random variables yet. That might be a nice comparison to make."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The reparameterization trick ####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A common trick in computational statistics is to generate a sample from a complicated distribution by building it up from a sample from a simple distribution. The reparameterization trick borrows from this idea. \n",
      "\n",
      "For example, if $\\epsilon \\sim N(0,1)$, then $Y = \\mu + \\sigma\\epsilon \\sim N(\\mu, \\sigma^2)$. We transformed a standard Normal distribution to a non-standard Normal. \n",
      "\n",
      "In general, if we set the distribution of a noise variable $\\mathbf{\\epsilon}$, under certain 'mild' conditions we can express the distribution of a random variable $\\mathbf{z}$ as a deterministic, differentiable function of $\\mathbf{\\epsilon}$.\n",
      "\n",
      "Say $\\boldsymbol{\\epsilon} \\sim P(\\boldsymbol{\\epsilon})$. We want to sample from $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$ . The distribution of $\\mathbf{\\tilde{z}}$, a sample from $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$, can be expressed as a differentiable function $g_{\\phi}(\\boldsymbol{\\epsilon}, \\mathbf{x})$. Once $\\boldsymbol{\\epsilon}$ is determined, the function $g$ is fully determined, so this is a deterministic function.\n",
      "\n",
      "$$\\mathbf{\\tilde{z}} = g_{\\phi}(\\boldsymbol{\\epsilon}, \\mathbf{x}), \\ \\ \\ \\boldsymbol{\\epsilon} \\sim P(\\boldsymbol{\\epsilon})$$\n",
      "\n",
      "<!--\n",
      "For example, in individual-level infectious disease models, the probability that an individual $i$ will become infectious at time $t$ in a discrete time SI system is:\n",
      "\n",
      "$$P(i, t) = 1 - {\\rm{exp}}\\left(-\\alpha \\sum_{j \\in I(t)} {d_{ij}}^{-\\beta}\\right)$$\n",
      "\n",
      "assuming constant susceptiblity $\\alpha$ throughout the population, and constant spatial parameter $\\beta$. The distance kernel $d_{ij}$ is often the Euclidean distance, but could be other distance metrics such as road distance, between infectious individual $j$ and susceptible individual $i$.\n",
      "\n",
      "Naturally, getting to that probability is a bit of a nightmare. Here's how it's done:\n",
      "\n",
      "1) Set up a population\n",
      "\n",
      "2) Select a 'patient zero' randomly\n",
      "\n",
      "3) Based on proximity (or contact) with infectious individuals, susceptible individuals become infectious at time $t$ with probability $P(i,t)$. Keep track of proximity and use this. For simplicity, here we'll use a square grid population and constant $\\alpha$ and $\\beta$, but you could (and people do) put priors on these.\n",
      "\n",
      "4) Output will be the position of each individual and when they became infectious.\n",
      "\n",
      "Code is available [here](https://github.com/caugusta/variational-inference/raw/master/DemoInfDis.R)\n",
      "\n",
      "So we use\n",
      "\n",
      "-->\n",
      "\n",
      "<!--\n",
      "For example (R. Metzger, 18/07/2011, STAT 340, University of Waterloo), if we wanted to sample from a Poisson distribution, we could build up from a Uniform distribution:\n",
      "\n",
      "Recall if $X \\sim Poi(\\mu)$ then:\n",
      "\n",
      "$$P(X = i) = \\displaystyle{\\frac{{\\rm{exp}} (-\\mu) \\mu^{i}}{i!}}$$\n",
      "\n",
      "$$P(X = i + 1) = \\displaystyle{\\frac{{\\rm{exp}} (-\\mu) \\mu^{(i + 1)}}{(i + 1)!}}$$\n",
      "\n",
      "$$\\displaystyle{\\frac{P(X = i+1)}{P(X = i)}} = \\frac{\\mu}{i + 1}$$\n",
      "    \n",
      "So\n",
      "$P(X = i + 1) = P(X = i) \\cdot \\frac{\\mu}{i + 1}$, which relates the current probability to the previous probability.\n",
      "\n",
      "Recall also that if $i = 0$ then $P(X = 0) = \\displaystyle{\\frac{{\\rm{exp}}(-\\mu) \\cdot \\mu^0}{0!}} ={\\rm{exp}} (-\\mu)$\n",
      "\n",
      "\n",
      "Using this, we can write an function in R to generate samples from a Poisson random variable using a Uniform random variable.\n",
      "\n",
      "Essentially, we've reparameterized. We've taken a uniform random variable \n",
      "\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a useful reparameterization to make, since now we can write the expectation with respect to $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$ such that the Monte Carlo estimator is differentiable with resepct to $\\phi$.\n",
      "\n",
      "Recall optimizing $\\mathcal{L}$ means taking a derivative of an expectation, according to Kingma and Welling (2013)'s equation (2). Since we're not assuming $\\phi$ can be calculated from a closed-form expectation, we need to estimate the expectation. The usual Monte Carlo estimator would be:\n",
      "\n",
      "$$\\nabla_{\\phi} E_{q_{\\phi} \\mathbf{(z)}} [f(\\mathbf{z})] = E_{q_{\\phi}(z)} [f(\\mathbf{z}) \\nabla_{q_{\\phi}(z)} {\\rm{log}} q_{\\phi} (\\mathbf{z})] \\approx \\frac{1}{L}\\sum_{\\ell = 1}^{L} f(\\mathbf{z}) \\nabla_{q_{\\phi}(\\mathbf{z^{\\ell}})}{\\rm {log}} q_{\\phi} (\\mathbf{z}^{(\\ell)}) $$\n",
      "\n",
      "But using the reparameterization trick, the Monte Carlo estimator of a function of $\\mathbf{z}$ with respect to the variational distribution $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$  is:\n",
      "\n",
      "$E_{Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})} [f(\\mathbf{z})] = E_{P(\\boldsymbol{\\epsilon})} \\left[ f(g_{\\phi} (\\boldsymbol{\\epsilon}, \\mathbf{x}^{(i)})) \\right] \\approx \\frac{1}{L} \\sum_{\\ell = 1}^{L} f(g_{\\phi} (\\boldsymbol{\\epsilon}^{(\\ell)}, x^{(i)})), \\ \\ \\ $ where $\\boldsymbol{\\epsilon^{(\\ell)}} \\sim P(\\boldsymbol{\\epsilon})$\n",
      "\n",
      "where $\\mathbf{z}^{(l)} \\sim q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})$\n",
      "\n",
      "<!--\n",
      "\n",
      "The Monte Carlo estimator, though, has too high a variance to be useful here. Instead, the paper illustrates the use of a clever trick to get a good estimator. Other tricks that have been tried are Rao-Blackwellization and control variates (Black Box Variational Inference), but I haven't found anyone trying antithetic random variables yet. That might be a nice comparison to make.\n",
      "\n",
      "-->\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------\n",
      "\n",
      "Aside: why the reparameterization trick works.\n",
      "\n",
      "We have $z = g_{\\phi} (\\epsilon, x)$,  $\\epsilon \\sim P({\\epsilon})$\n",
      "\n",
      "\\begin{align}\n",
      "E_{Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})} [f(\\mathbf{z})] &= \\int Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) f(\\mathbf{z}) d{\\mathbf{z}} {\\rm{\\ \\ \\ by\\ definition\\ of\\ expectation}} \\\\[0.5em]\n",
      "&= \\int P(\\epsilon) f(\\mathbf{z}) d\\epsilon {\\rm{\\ \\ \\ since\\ }} \\epsilon {\\rm{\\ is\\ the\\ only\\ part\\ of\\ }} \\mathbf{z} {\\rm{\\ that\\ varies}} \\\\[0.5em]\n",
      "&= \\int P(\\epsilon) f(g_{\\phi}(\\epsilon, \\mathbf{x})) d\\epsilon \\\\[0.5em]\n",
      "&\\approx \\frac{1}{L} \\sum_{\\ell = 1}^{L} f(g_{\\phi}(\\epsilon^{\\ell}, \\mathbf{x}))\n",
      "\\end{align}\n",
      "    \n",
      "-------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the reparameterization trick on the ELBO gives the first Stochastic Gradient Variational Bayes (SGVB) estimator. the ELBO is:\n",
      "\n",
      "$\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}^{(i)}) = E_{Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})}  \\left[-{\\rm log\\ } Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)}) + {\\rm {log}\\ } P_{\\theta} (\\mathbf{x}^{(i)}, \\mathbf{z}) \\right] $\n",
      "\n",
      "So the first SGVB estimator is:\n",
      "\n",
      "$$\\tilde{\\mathcal{L}}^{A} (\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}^{(i)}) = \\frac{1}{L} \\sum_{\\ell = 1}^{L} \\left[ - {\\rm {log\\ }} Q_{\\phi}(\\mathbf{z}^{(i,\\ell)} \\mid \\mathbf{x}^{(i)}) + {\\rm {log\\ }} P_{\\theta} (\\mathbf{x}^{(i)}, \\mathbf{z}^{(i,\\ell)}) \\right]$$\n",
      "\n",
      "where $\\mathbf{z}^{(i,\\ell)} = g_{\\phi} (\\boldsymbol{\\epsilon}^{(i, \\ell)}, \\mathbf{x}^{(i)}) \\ \\ \\ {\\rm{and}} \\ \\ \\ \\boldsymbol{\\epsilon}^{(\\ell)} \\sim P(\\boldsymbol{\\epsilon})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second SGVB estimator (with lower variance than the first) is derived from the second breakdown of the ELBO by remembering that the KL divergence may be evaluated analytically in some situations, which means the breakdown of the ELBO has only one term that requires estimation via sampling: the negative reconstruction error, $E_{Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})} \\left[ {\\rm{log}\\ } P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}) \\right]$. So we have:\n",
      "\n",
      "$\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}^{(i)}) = -{\\rm{KL}}(Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})\\ ||\\ P_{\\theta} (\\mathbf{z})) + E_{Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})} \\left[ {\\rm{log}\\ } P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}) \\right] $\n",
      "\n",
      "$$\\tilde{\\mathcal{L}}^{B} (\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}^{(i)}) = -{\\rm{KL}}(Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})\\ ||\\ P_{\\theta} (\\mathbf{z})) + \\frac{1}{L} \\sum_{\\ell = 1}^{L} {\\rm{log\\ }} P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i,\\ell)})$$ where $\\mathbf{z}^{(i,\\ell)} = g_{\\phi} (\\boldsymbol{\\epsilon}^{(i, \\ell)}, \\mathbf{x}^{(i)}) \\ \\ \\ {\\rm{and}} \\ \\ \\ \\boldsymbol{\\epsilon}^{(\\ell)} \\sim P(\\boldsymbol{\\epsilon})$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we would rather use minibatches instead, we can modify these estimators in the same way:\n",
      "    \n",
      "$$\\tilde{\\mathcal{L}}^{(m)} (\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{X}^{m}) = \\frac{n}{m} \\sum_{i = 1}^{m} \\tilde{\\mathcal{L}}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\boldsymbol{x}^{(i)})$$\n",
      "\n",
      "where $n$ is the total number of elements in the dataset and $m$ is the number of elements in the minibatch."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "The goal is to maximize the lower bound, $\\mathcal{L}$, which means we need to take derivatives with respect to $\\phi$ and $\\theta$. With the original function $Q$, this was not possible. However, $g$ is a differentiable function of $\\phi$, so the problem has disappeared. \n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Whether we use SGVB estimator A or B, we can now take derivatives with respect to $\\theta$ and $\\phi$ of the estimator $\\tilde{\\mathcal{L}}$ of the lower bound, and use stochastic gradient descent to arrive at optimal values of $\\theta$ and $\\phi$.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Connection to Auto-Encoders ###\n",
      "\n",
      "Looking again at Kingma and Welling (2013)'s equation (7):\n",
      "\n",
      "$$\\tilde{\\mathcal{L}}^{B} (\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}^{(i)}) = -{\\rm{KL}}(Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})\\ ||\\ P_{\\theta} (\\mathbf{z})) + \\frac{1}{L} \\sum_{\\ell = 1}^{L} {\\rm{log\\ }} P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i,\\ell)})$$ where $\\mathbf{z}^{(i,\\ell)} = g_{\\phi} (\\boldsymbol{\\epsilon}^{(i, \\ell)}, \\mathbf{x}^{(i)}) \\ \\ \\ {\\rm{and}} \\ \\ \\ \\boldsymbol{\\epsilon}^{(\\ell)} \\sim P(\\boldsymbol{\\epsilon})$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second term on the right hand side is a kind of average negative reconstruction error; that is, the average error when we reconstruct observations $x$ using latent variables $z$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first term on the right hand side is the negative KL divergence between the prior on $z$ and the approximate posterior, $Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})$. Similar to sparse autoencoders, this is a regularization term that forces the approximate posterior distribution to be relatively close to the prior.\n",
      "\n",
      "We choose $g_{\\phi} (\\cdot)$ such that $\\mathbf{x}^{(i)}$ and $\\epsilon^{(\\ell)}$ are mapped to a sample from the approximate posterior for the point $\\mathbf{x}^{(i)}$:\n",
      "\n",
      "$$\\mathbf{z}^{(i,\\ell)} = g_{\\phi} (\\boldsymbol{\\epsilon}^{(i, \\ell)}, \\mathbf{x}^{(i)})$$\n",
      "\n",
      "$$\\mathbf{z}^{(i,\\ell)} \\sim Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then $\\mathbf{z}^{(i,\\ell)}$ is input for the negative reconstruction error ${\\rm{log\\ }} P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i,\\ell)})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### On the mild conditions under which the reparameterization works###\n",
      "\n",
      "There are particular distributions $Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})$ for which we can choose $g_{\\phi}(\\cdot)$ and auxiliary variable $\\epsilon$. Note there are other methods, but these are the three cited in the paper.\n",
      "\n",
      "1) Location-scale families. \n",
      "\n",
      "We've already seen an example of this with the Normal distribution. The trick is to choose $\\epsilon \\sim$ location $= 0$, scale $= 1$ as the auxiliary variable, then let $g(\\cdot) = $ location $+$ scale $\\cdot \\epsilon$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2) Tractable inverse cumulative distribution function (CDF) (the inverse transform theorem, IVT). \n",
      "\n",
      "Let $\\epsilon \\sim U(0, \\mathbf{I})$\n",
      "\n",
      "Then let $g_{\\phi} (\\epsilon, \\mathbf{x})$ be the inverse CDF of $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$\n",
      "\n",
      "(for a proof of IVT and an example, click [here](ProofIVTExample.ipynb))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) Composition: express a random variable ($\\mathbf{z}$ in our case) as a transformation of an auxiliary variable.\n",
      "\n",
      "(for an example of a composition, click [here](CompositionExample.ipynb))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example: Variational Auto-Encoder ###\n",
      "\n",
      "The paper gives an illustration of the use of a neural network (MLP) as an encoder $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$.\n",
      "\n",
      "Recall the encoder is an approximation to the true posterior, $P_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})$ of the generative model $P_{\\theta} (\\mathbf{x}, \\mathbf{z})$. I find this notation confusing, because I thought the likelihood, $P_{\\theta} (\\mathbf{x} \\mid \\mathbf{z})$ was the decoder (generative model)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let \n",
      "\n",
      "$$P_{\\theta}(\\mathbf{z}) \\sim N(\\mathbf{z}; 0, \\mathbf{I})$$\n",
      "\n",
      "$$P_{\\theta} (\\mathbf{x} \\mid \\mathbf{z}) \\sim N(\\mathbf{x}; \\mu, \\sigma^2)$$\n",
      "\n",
      "where $\\mu$ and $\\sigma^2$ are computed with an MLP with $\\mathbf{z}$ as input. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The true posterior $P_{\\theta} (\\mathbf{z} \\mid \\mathbf{x})$ is intractable, so we'll approximate it with a variational (encoding) distribution $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the paper, they let \n",
      "\n",
      "$${\\rm{log}} Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}^{(i)}) = {\\rm{log\\ }} N(\\mathbf{z} ; \\mu^{(i)}, \\sigma^{2(i)} \\mathbf{I})$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and say that this is not necessary, but useful. The parameters $\\mu^{(i)}$ and $\\sigma^{2(i)}$ are outputs of the encoding MLP. That is, they will be nonlinear functions of $\\mathbf{x}^{(i)}$ and the variational parameters $\\phi$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------\n",
      "\n",
      "Aside: why $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}^{(i)}) = N(\\mathbf{z} ; \\mu^{(i)}, \\sigma^{2(i)} \\mathbf{I})$ makes sense:\n",
      "\n",
      "If\n",
      "\n",
      "$$P_{\\theta}(\\mathbf{z}) \\sim N(\\mathbf{z}; 0, \\mathbf{I})$$\n",
      "\n",
      "$$P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}) \\sim N(\\mathbf{x}^{(i)}; \\mu, \\sigma^2)$$\n",
      "\n",
      "Then $P_{\\theta}(\\mathbf{z} \\mid \\mathbf{x}^{(i)}) \\propto P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}) P_{\\theta}(\\mathbf{z}) \\approx Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}^{(i)})$\n",
      "\n",
      "And the conjugate distribution of a Normal distribution is another Normal distribution, so since we have a Normal likelihood and a Normal prior, we should expect a Normal posterior distribution.\n",
      "\n",
      "-------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we have a Normal approximate posterior $Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}^{(i)})$, we can use the same form of the reparameterization as was shown in the location-scale family example. That is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "samples $\\mathbf{z}^{(i,\\ell)} \\sim Q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}^{(i)})$ can be generated using \n",
      "\n",
      "$$\\mathbf{z}^{(i,\\ell)} = g_{\\phi}(\\mathbf{x}^{(i)}, \\epsilon^{(\\ell)}) = \\boldsymbol{\\mu}^{(i)} + \\boldsymbol{\\sigma}^{(i)} \\odot \\epsilon^{(\\ell)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $\\odot$ denotes an element-wise product, and $\\epsilon^{(\\ell)} \\sim N(\\mathbf{0}, \\mathbf{I})$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "when both the prior and the posterior are Gaussian, the KL divergence between them will be analytically tractable (see their Appendix A), so we can use estimator (B) of the ELBO:\n",
      "\n",
      "$$\\tilde{\\mathcal{L}}^{B} (\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}^{(i)}) = -{\\rm{KL}}(Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x}^{(i)})\\ ||\\ P_{\\theta} (\\mathbf{z})) + \\frac{1}{L} \\sum_{\\ell = 1}^{L} {\\rm{log\\ }} P_{\\theta} (\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i,\\ell)})$$ where $\\mathbf{z}^{(i,\\ell)} = \\boldsymbol{\\mu}^{(i)} + \\boldsymbol{\\sigma}^{(i)} \\odot \\epsilon^{(\\ell)}\\ \\ \\ {\\rm{and}} \\ \\ \\ \\epsilon^{(\\ell)} \\sim N(\\mathbf{0}, \\mathbf{I})$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The KL divergence has this form (see their Appendix A - actually, I got confused here because they use a non-conditional distribution):\n",
      "\n",
      "$${\\rm{KL}}(Q_{\\phi} (\\mathbf{z} \\mid \\mathbf{x})\\ ||\\ P_{\\theta} (\\mathbf{z})) = \\frac{1}{2} \\sum_{j=1}^{J} \\left( 1 + {\\rm{log\\ }}((\\sigma_j)^{2}) - \\mu_j^2 - \\sigma_j^2 \\right)$$\n",
      "\n",
      "where $j$ is the dimensionality of the latent space.\n",
      "\n",
      "So:\n",
      "\n",
      "$$\\tilde{\\mathcal{L}}^{B} (\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}^{(i)}) = \\frac{1}{2} \\sum_{j=1}^{J} \\left( 1 + {\\rm{log }} \\left( (\\sigma_j^{(i)})^{2} \\right) - (\\mu_j^{(i)})^{2} - (\\sigma_j^{(i)})^{2} \\right) + \\frac{1}{L} \\sum_{\\ell = 1}^{L} {\\rm{log}} P_{\\theta}(\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i, \\ell)})$$\n",
      "\n",
      "where $\\mathbf{z}^{(i,\\ell)} = \\boldsymbol{\\mu}^{(i)} + \\boldsymbol{\\sigma}^{(i)} \\odot \\epsilon^{(\\ell)}\\ \\ \\ {\\rm{and}} \\ \\ \\ \\epsilon^{(\\ell)} \\sim N(\\mathbf{0}, \\mathbf{I})$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The pictures (I think) - note they're not symmetric, which bothers me.###\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The Encoder ###\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/EncoderAEVB.png\">\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The Decoder ###\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/DecoderAEVB.png\">\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Algorithm ###\n",
      "\n",
      "1) Initialize $\\theta$, $\\phi$ randomly.\n",
      "\n",
      "2) Until convergence of $\\theta, \\phi$:\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $X^{M} = $ independent sample of M elements from the dataset\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\epsilon = $ random samples from the noise distribution $P(\\epsilon)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $g_{\\phi}(\\cdot) = \\nabla_{\\theta, \\phi} \\tilde{\\mathcal{L}}^{M}(\\theta, \\phi; X^{M}, \\epsilon)$\n",
      "\n",
      "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta, \\phi = $ update using stochastic gradient descent or Adagrad"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Similar work ###\n",
      "\n",
      "The wake-sleep algorithm\n",
      "\n",
      "-Advantage: also applies to discrete latent variables (this approach only works for continuous latent variables)\n",
      "\n",
      "-Disadvantage: does not optimize a lower bound on the marginal likelihood, and requires 2 concurrent optimizations\n",
      "\n",
      "-But they have similar computational complexity.\n",
      "\n",
      "-SGVB doesn't have a regularization hyperparmaeter that gets in the way during training."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### How well does AEVB do compared to wake-sleep? ###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at the likelihood lower bound for MNIST digits\n",
      "\n",
      "AEVB converged faster, and to a better solution, than wake-sleep. Vertical axis: estimated average variational lower bound per datapoint."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}