{
 "metadata": {
  "name": "",
  "signature": "sha256:fe6745c795ae75989db03a4aa444e0311d7bc5bfaed3b658d1b35b5d7f396b2f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Stochastic variational inference on mixture of Gaussians#"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/GaussianMixturePlate.png\" alt=\"Plate model of a simplified Gaussian mixture model\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To simplify things, we assume the variance of $x_i$ is known, and we assume that the mixing proportions are known. We also assume that $\\mu_k$ is governed by $\\omega$, that is $\\mu \\sim G(0, \\omega^2)$. Also, per the usual Gaussian mixture model assumption, we say that each $x_i$ was generated by one and only one of the four Gaussian components in the model (so $x_i \\sim G(\\mu_{z_i}, \\sigma^2)$). Finally, we say $z_i \\sim Cat(\\pi)$. This model is used in Blei's 2014 lecture, [available here](http://www.cs.columbia.edu/~blei/fogm/lectures/mcmc.pdf)\n",
      "\n",
      "We still need to infer the cluster means (global hidden variables $\\mu_k$, with variational parameter $\\lambda_k$) and cluster assignments (local hidden variables $z_i$, with variational parameter $\\phi_i$). \n",
      "\n",
      "<!--\n",
      "NOTE! DO I NEED MORE THAN ONE VARIATIONAL PARAMETER FOR MU?\n",
      "-->\n",
      "\n",
      "As before, there are 4 Gaussian clusters. However, here we will be considering univariate Gaussians as a further simplification.\n",
      "\n",
      "To arrive at the algorithm for stochastic variational inference in the case of a univariate Gaussian mixture model, we have to (according to Hoffman et al 2013):\n",
      "\n",
      "1) Specify the model assumptions\n",
      "\n",
      "2) Get the complete conditionals\n",
      "\n",
      "3) Get the mean field variational family\n",
      "\n",
      "4) Get the stochastic variational inference algorithm.\n",
      "\n",
      "Skipping the intermediate steps to save time (I can show you the derivation afterward), the algorithm for stochastic variational inference, in this case, is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Algorithm ####\n",
      "\n",
      "1. Set $\\pi$, $\\sigma^2$, $\\omega$, initialize $z$\n",
      "\n",
      "2. Initialize $\\lambda$ randomly.\n",
      "\n",
      "3. Set the step size schedule to $\\rho_t = (1 + t)^{-\\kappa}$ ($\\kappa$ will be chosen via cross-validation)\n",
      "\n",
      "4. Repeat\n",
      "\n",
      "a) Sample $x_i$ uniformly from the dataset\n",
      "\n",
      "b) Compute $\\phi_{z_i} = \\ell(x_i; \\mu_{z_i}, \\sigma^2)$, holding $\\mu$ constant at the value $\\lambda$.\n",
      "\n",
      "c) Compute $\\hat{\\lambda} = \\hat{\\mu_k}$, holding $z_i$ constant at the level $\\phi_{z_i}$, where \n",
      "\n",
      "$$\\hat\\mu_k = \\left(\\frac{\\frac{\\sum_{i=1}^n z_i^{k}}{\\sigma^2}}{\\frac{\\sum_{i=1}^n z_i^{k}}{\\sigma^2} + \\frac{1}{\\omega^2}}\\right) \\frac{\\sum_{i=1}^n z_i^{k} x_i}{\\sum_{i=1}^n z_i^{k}}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymc\n",
      "import numpy as np\n",
      "import sklearn\n",
      "\n",
      "#1. Set pi, sigma2, and omega. Initialize z.\n",
      "\n",
      "    # Pi is a vector of length 4. \n",
      "    #We will assume that all clusters are equally likely to have generated observation x_i, a priori.\n",
      "\n",
      "pi = np.array([0.25, 0.25, 0.25, 0.25])\n",
      "\n",
      "    #sigma2 can just be unit variance\n",
      "\n",
      "sigma2 = 1\n",
      "\n",
      "    #omega is the variance of each cluster k. \n",
      "    #For simplicity, we will initialize each cluster to have equal variance\n",
      "\n",
      "omega = np.array([3, 3, 3, 3])\n",
      "\n",
      "z = pm.Categorical(pi, )\n",
      "\n",
      "#2. Initialize lambda randomly.\n",
      "\n",
      "    #We know from the complete conditional for mu that lambda \n",
      "    #follows a Normal distribution with mean \\hat\\mu_k and variance \\hat\\omega_k\n",
      "\n",
      "muk = \n",
      "lambdak = pm.Normal('lambdak', muk[0], omega[0], size = 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = 3\n",
      "ndata = 500\n",
      "\n",
      "dd = mc.Dirichlet('dd', theta=(1,)*n)\n",
      "category = mc.Categorical('category', p=dd, size=ndata)\n",
      "\n",
      "precs = mc.Gamma('precs', alpha=0.1, beta=0.1, size=n)\n",
      "means = mc.Normal('means', 0, 0.001, size=n)\n",
      "\n",
      "@mc.deterministic\n",
      "def mean(category=category, means=means):\n",
      "    return means[category]\n",
      "\n",
      "@mc.deterministic\n",
      "def prec(category=category, precs=precs):\n",
      "    return precs[category]\n",
      "\n",
      "v = np.random.randint( 0, n, ndata)\n",
      "data = (v==0)*(50+ np.random.randn(ndata)) \\\n",
      "       + (v==1)*(-50 + np.random.randn(ndata)) \\\n",
      "       + (v==2)*np.random.randn(ndata)\n",
      "obs = mc.Normal('obs', mean, prec, value=data, observed = True)\n",
      "\n",
      "model = mc.Model({'dd': dd,\n",
      "              'category': category,\n",
      "              'precs': precs,\n",
      "              'means': means,\n",
      "              'obs': obs})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}