{
 "metadata": {
  "name": "",
  "signature": "sha256:c0d49b893bb7c30445512df56f6645a672a743bdc6b27145901f813726f759b1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#An Introduction to Variational Inference#"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Carolyn Augusta, caugusta@uoguelph.ca"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "------------\n",
      "## Outline ##\n",
      "\n",
      "1. Why we need variational methods\n",
      "2. Mean field variational inference - theory\n",
      "3. Mean field variational inference - example: Variational mixture model\n",
      "4. The EM algorithm and how it relates to mean field variational inference\n",
      "5. Stochastic variational inference\n",
      "6. References\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. Why we need variational methods on graphical structures##\n",
      "\n",
      "Inference on probabilistic graphical models can only be performed exactly when the graph structure is relatively limited. For example, the junction tree algorithm is an efficient algorithm for tree models (Bayesian networks that take the form of directed acyclic graphs). Another common form of exact inference on tree models is belief propagation (BP); however, if the graph is densely connected, BP can be computationally infeasible [1 - the book]. \n",
      "\n",
      "On general graph structures, inference is computationally intractable due to the intractability of the normalization constant (also called the partition function, or the free energy). This constant often involves computation of a multidimensional integral, which cannot be evaluated analytically. Numerical methods are then required to arrive at a solution. \n",
      "\n",
      "These numerical methods can be stochastic or deterministic in origin. A popular stochastic method is Markov chain Monte Carlo (MCMC), and its many variants. MCMC methods, though exact, can be slow over large graphical structures. As modelling and inference problems evolve to match the speed of data, approximate inference methods become more ubiquitous. Instead of sampling from the unknown, intractable posterior distribution (as in MCMC), variational methods construct a known distribution to approximate the posterior. The parameters of the known distribution are then chosen to minimize the difference between the posterior and the variational distribution. In this way, we obtain a deterministic approximation to the posterior distribution, often in a faster way than with MCMC methods (although this is a controversial statement).\n",
      "\n",
      "The most widely used class of variational inference algorithms are derived from the so-called 'mean-field approximation' from the statistical physics literature. MCMC also had its origins in statistical physics. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------------------------------------\n",
      "### 2. Mean-field variational inference - theory###\n",
      "\n",
      "\n",
      "#### Hidden (latent) variables and mixture distributions ####\n",
      "\n",
      "A hidden or latent variable in a statistical model is a variable that is not directly observed in the process of interest, but whose value is inferred based on observed variables. The classic example comes from a mixture distribution. \n",
      "\n",
      "A mixture distribution results when a random variable could come from one of several subpopulations, or clusters. Say we have $n$ observations from a random variable $X$ (that is, we have $x_1, \\ldots, x_n$). Then each $x_i$ belongs to some cluster. We want to know which cluster $x_i$ came from, but that information is hidden. We only have the observations themselves, not the cluster labels. The plate diagram for a mixture distribution is shown below.\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/GMM.png\" alt=\"\" style=\"width:500px\">\n",
      "\n",
      "A plate model (above) is a schematic for a model distribution. The plate model is a description of the dependencies in the model distribution. In a plate model, a rectangle with a total (N) in the bottom right indicates that all variables or parameters inside the rectangle (the \"plate\") are indexed from 1 to N. So we have a pictoral representation of $z_n : n = 1, \\ldots, N$ and $x_n : n = 1, \\ldots, N$. The notation for unobserved variables (hidden units, in this case $z_n$) in a plate diagram is an unfilled circle. Observed variables (visible units, in this case $x_n$) are filled circles. Outside the plate are fixed parameters are denoted using a red dot (in this case, $\\pi$, which governs the mixture probability, $\\mu$ and $\\Sigma$, the parameters for each component of the mixture distribution). The arrows between components in the plate model show how each variable or parameter is related. For example, the arrow from $z_n$ to $x_n$ indicates that $x_n$ dependes on $z_n$. Taken as a whole, the plate model shows that the joint (model) distribution $P(x, z \\mid \\pi, \\mu, \\Sigma)$, and illustrates the dependencies in the model.\n",
      "\n",
      "To take an example from R, say we're looking at the Old Faithful dataset. We know the duration of each eruption, and the waiting time to an eruption (both in minutes). Using the example plot from R, we can see that the data follow a bimodal distribution (so you could say there are two clusters):\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/OldFaithful.png\" alt=\"\" style=\"width:500px\">\n",
      "\n",
      "We could model this process as a mixture distribution with two distributions (clusters): $g_s(X) = N(\\mu_1, \\sigma_1^2)$ and $g_\\ell(X) = N(\\mu_2, \\sigma_2^2)$, where $s$ is for short wait time and $\\ell$ is for long wait time.\n",
      "\n",
      "Suppose we have a sample of eruptions $\\boldsymbol{x} = \\{x_1, \\ldots, x_n\\}$. \n",
      "\n",
      "<!--\n",
      "and we observe their wait times $\\boldsymbol{y} = \\{y_1, \\ldots y_n\\}$. \n",
      "\n",
      "-->\n",
      "\n",
      "We assume there's an unobserved (hidden) cluster label $Z_i = \\{{\\rm{Short, Long}}\\}$, with one realization per wait time, $\\boldsymbol{z} = \\{z_1, \\ldots z_n\\}$.\n",
      "\n",
      "The distribution of cluster labels is Bernoulli$(\\pi)$ (since it can only be 1 of 2 possible clusters - if we had more clusters, we would use a Categorical distribution).\n",
      "\n",
      "\n",
      "$$P(Z) = \\prod_{i=1}^{n} P(Z_i \\mid \\pi) = \\prod_{i=1}^{n} \\pi^{Z_i} (1 - \\pi)^{1 - Z_i}$$\n",
      "\n",
      "where\n",
      "\n",
      "\\begin{equation}\n",
      "    Z_i = \n",
      "    \\begin{cases}\n",
      "      1, & {\\rm{if\\ }} x_i {\\rm{\\ is\\ Short}}\\  \\\\\n",
      "      0, & \\text{otherwise}\n",
      "    \\end{cases}\n",
      "  \\end{equation}\n",
      "\n",
      "and $$P(Z_i = {\\rm{Short}}) = \\pi$$\n",
      "\n",
      "\n",
      "\n",
      "The probability that we'd get a wait time from the first cluster is proportional to the area under the left hand curve. Similar for the right handed curve. The distribution of wait times (regardless of the cluster labels) can be represented as a mixture of normal distributions:\n",
      "\n",
      "$$P(X) = P(X_1=x_1, X_2 = x_2, \\ldots X_n = x_n) = \\prod_{i=1}^{n} \\left( \\pi g_s(x_i) + (1 - \\pi) g_\\ell(x_i) \\right)$$\n",
      "\n",
      "We also know the likelihood, since:\n",
      "\n",
      "$$X_i \\mid (Z_i = 1) \\sim g_s (x_i)$$\n",
      "\n",
      "$$X_i \\mid (Z_i = 0) \\sim g_\\ell (x_i)$$\n",
      "\n",
      "$$P(X \\mid Z) = \\prod_{i=1}^{n} \\left(g_s(x_i)^{z_i} + g_\\ell(x_i)^{1 - z_i} \\right)$$\n",
      "\n",
      "\n",
      "So we have the normalization constant $P(X)$, the likelihood $P(X \\mid Z)$, and the prior $P(Z)$. The cluster label $z_i$ is hidden. If we wanted to infer it based on the data we have, then we could use variational inference (although the posterior distribution $P(Z \\mid X)$ in this case is tractable, so we could just use Bayes' Theorem).\n",
      "\n",
      "\n",
      "\n",
      "<!--\n",
      "The only thing is, these distributions clearly overlap! If we draw $x = 68$ from this distribution, it's not clear whether $x$ came from the first subpopulation or the second. So the identity of the distribution from which $x$ came is a hidden variable.\n",
      "\n",
      "-->\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The Mean Field Assumption ####\n",
      "\n",
      "In statistical physics, mean field theory has long been used to model complex systems using relatively simple ones. A huge assumption is made, though, that all of the hidden variables $Z_1, Z_2, \\ldots, Z_n$ are mutually independent. This is often a bad assumption, because in reality there are a lot of ways for the model to have dependencies. However, the mean field (independence) assumption has some nice properties, so that'll be the focus of today's presentation.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "Remember, the posterior distribution $P(Z \\mid X)$ is intractable because the normalizing constant, $P(X)$ is intractable. We want to approximate this intractable posterior distribution with some known, nice function $Q$ (called the \"variational distribution\"). So:\n",
      "\n",
      "$$P(Z \\mid X) \\approx Q(Z)$$\n",
      "\n",
      "Using the mean field assumption, we'll assume Q is a factorized distribution. That is, we'll assume:\n",
      "\n",
      "$$Q(Z) = \\prod_{i=1}^n Q_i(Z_i)$$\n",
      "\n",
      "We have the likelihood (from the data and the model) and the prior (because we can always specify one). So the only thing to cover now is the normalization constant, $P(X)$. \n",
      "-->\n",
      "\n",
      "####The lower bound on the normalization constant ####\n",
      "\n",
      "\n",
      "To get to the objective function in mean-field variational inference, we need to minimize the distance between an approximating distribution $Q$ (called the \"variational distribution\") and the true posterior distribution $P(Z \\mid X)$. To do this, we need to get an appropriate distance metric. The distance metric we'll use is based on maximizing the log probability of the data regardless of the hidden variables (the log normalization constant, ${\\rm{ln\\ }} P(X)$). Remember any time a statistician says log, they usually mean ln. We can get a lower bound on the normalization constant in the following way:\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(X) &= {\\rm{ln}} \\int_Z P(X \\mid Z)P(Z)\\ dZ \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\int_Z P(X, Z)\\ dZ  {\\hspace{0.5in} \\rm {\\ by\\ conditional\\ probability}} \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\int_Z \\frac{P(X, Z)}{Q(Z \\mid X)}Q(Z \\mid X)\\ dZ \\\\[0.5em]\n",
      "&= {\\rm{ln\\ }} E_Q \\left[\\frac{P(X, Z)}{Q(Z \\mid X)} \\right] {\\hspace{0.5in} \\rm{\\, since\\ E_X(g(X)) = \\int_x g(x)f(x) dx,\\ by\\ definition,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(Z)}}\\\\[0.5em]\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[\\frac{P(X, Z)}{Q(Z \\mid X)} \\right] {\\hspace{0.5in} \\rm{\\, by\\ Jensen's\\ Inequality}} \\\\[0.5em]\n",
      "&= E_Q \\left[{\\rm{ln\\ }} P(X, Z)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(Z \\mid X)}\\right] \\\\[0.5em]\n",
      "&= {\\mathcal{L}}(Q) \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first expected value term here is the expected value of the log joint distribution, which is our model distribution. The second expected value term is the entropy of $Q$. The two terms together constitute a lower bound on the normalization constant. This lower bound, denoted $\\mathcal{L}(Q)$ is also called the \"expectation lower bound\", or ELBO. Also note, when I write $Q$, or $Q(Z)$, I mean $Q(Z \\mid X)$, it's just nicer notation.\n",
      "\n",
      "Aside: Jensen's Inequality applied here is ${\\rm{ln}}(E(X)) \\ge E({\\rm{ln}} (X))$, and more information is available [here](http://en.wikipedia.org/wiki/Jensen's_inequality). The vast majority of this derivation is given in Bishop (1998) and again in Hoffman (2013)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####The objective function ####\n",
      "\n",
      "In variational inference, we want to approximate that intractable posterior distribution by some known, nice function $Q$. To find the optimal $Q$, we need to define a relationship between the intractable log normalization constant ${\\rm{ln\\ }} P(X)$ and $Q$, and get an objective function based on that relationship.\n",
      "\n",
      "The mean-field assumption leads to a rather nice result, namely that we can use that ELBO $\\mathcal{L} (Q)$ (lower bound on the log probability of the data) in the following way:\n",
      "\n",
      "$${\\rm{ln}}\\ P(X) = {\\rm{\\mathcal{L}}}(Q) + {\\rm{KL}}(Q \\ \\| \\ P)$$\n",
      "\n",
      "where ${\\rm{\\mathcal{L}}}(Q)$ is the ELBO, and ${\\rm{KL}}(Q \\ \\| \\ P)$ is the KL (Kullback-Leibler) divergence between the approximating distribution $Q$ and the true posterior distribution $P(Z \\mid X)$. \n",
      "\n",
      "(Aside: Remember the KL divergence is a measure of the difference between two probability distributions. It's not the same as a distance function, because it isn't symmetric (but you can define it to be - that's done in stochastic variational inference, but not here). It is defined (for continuous distributions) as:\n",
      "\n",
      "$${\\rm{KL}}(Q(Z \\mid X) \\ \\| \\ P(Z\\mid X)) = - \\int_{Z} Q(Z \\mid X) {\\rm\\ ln}\\left[\\frac{P(Z \\mid X)}{Q(Z \\mid X)}\\right] dZ$$\n",
      "\n",
      "Or, in simplified notation:\n",
      "\n",
      "$${\\rm{KL}}(Q \\ \\| \\ P) = - \\int_Z Q {\\rm\\ ln}\\left[\\frac{P}{Q}\\right] dZ$$\n",
      "\n",
      "This relationship can be derived by expanding the KL divergence, and expanding the ELBO, then adding them together, cancelling out some terms and remembering that Q is a probability density function (which means the integral of $Q$ over its domain is equal to 1). I can send you the proof if you like, or you can find most of it in Bishop (2006).)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So maximizing the ELBO $\\mathcal{L}(Q)$, with respect to $Q$ is the same as minimizing the KL divergence between the variational distribution $Q$ and the true posterior distribution $P(Z \\mid X)$. This is more easily seen through a picture (from Bishop 1998):\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/KLpicture.png\" alt=\"Illustration of the KL divergence minimization being equivalent to maximization of the lower bound\" style=\"width:250px\">\n",
      "\n",
      "In this diagram, ${\\rm{ln\\ }} P(V \\mid w)$ is the equivalent of our ${\\rm {ln\\ }} P(X)$. Note that we can't compute the KL divergence here, since we can't compute the posterior $P(Z \\mid X)$. Instead, we focus on maximizing the ELBO:\n",
      "\n",
      "$$\\mathcal{L}(Q) = E_Q \\left[{\\rm{ln\\ }} P(X, Z)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(Z)}\\right]$$\n",
      "\n",
      "#### Choosing Q ####\n",
      "Recall the minimum value of the KL divergence metric is 0. If we could choose any $Q$ we like, we would choose it such that $Q = P(Z \\mid X)$, which would give a KL divergence of 0. That would mean, though, that we knew the form of the posterior distribution, which we don't. So instead, we choose a \"best\" $Q$ out of a particular family of functions. Corresponding to the mean-field assumption, we choose the following factorized form for the variational distribution:\n",
      "\n",
      "$$ Q(Z) = \\prod_{i=1}^n Q_i (Z_{i}) $$\n",
      "\n",
      "So there will be $n$ functions $Q_i$ corresponding to the $n$ different $Z_i$ hidden variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Maximizing the ELBO ####\n",
      "\n",
      "Finally, we have our objective function:\n",
      "    \n",
      "$$\\mathcal{L}(Q) = E_Q \\left[{\\rm{ln\\ }} P(X, Z)\\right] - E_Q \\left[{\\rm{ln\\ }} {\\prod_{i=1}^{n} Q_i(Z_i)}\\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to maximize $\\mathcal{L}(Q)$ with respect to each function $Q_i(Z_i)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final result is:\n",
      "    \n",
      "$$Q^{*}_j(Z_j)= \\displaystyle{\\frac{{\\rm{exp}}\\left\\{ E_{Q_{i \\ne j}} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\}}{\\int_{Z_j} \\rm{exp}\\left\\{ E_{Q_{i \\ne j}} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\} d{Z_j}}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So to arrive at a best distribution for all $Z_j,\\ j = 1, \\ldots n$, we need to initialize all of the factors $Q_i(Z_i), i \\ne j$, then calculate the expectation of $Z_j$ with respect to the other factors. We then update our $Q_j(Z_j)$, and move to another distribution, holding all the others constant. In this way, we will arrive at an optimal distribution for $Q$, since the bound is convex with respect to each of the $Q_i(Z_i)$ distributions (Bishop, 2006).\n",
      "\n",
      "####The complete algorithm is:####\n",
      "\n",
      "1. Initialize the parameters of all of the variational distributions $Q_j(Z_j),\\ j = 1, \\ldots, n$\n",
      "2. For j in 1:n\n",
      "    $$Q^{new}_j(Z_j) = \\displaystyle{\\frac{{\\rm{exp}}\\left\\{ E_{i \\ne j} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\}}{\\int_{Z_j} \\rm{exp}\\left\\{ E_{i \\ne j} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\} d{Z_j}}}$$\n",
      "    \n",
      "3. Iterate until convergence.\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3. Mean field variational inference - example: Variational Gaussian Mixture Model###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example is taken directly, verbatim from http://www.bayespy.org/examples/gmm.html. All of the mathematical derivations are available in section 10.2 of Bishop (2006).\n",
      "\n",
      "The Python 3.0 module BayesPy allows variational Bayes procedures to be implemented on linear regression, Gaussian mixture models, Bernoulli mixture models, Hidden Markov models, and more. Unfortunately the authors have not seen fit to provide a Python 2.7 implementation.\n",
      "\n",
      "Remember from the Old Faithful example that we had 2 component distributions in our mixture distribution. Here, we'll instead have 4 component distributions, (called clusters) to make the problem a little more interesting:\n",
      "\n",
      "<!--\n",
      "Recall the parameters of a Normal distribution are $\\mu$ and $\\sigma^2$. Just like the Normal distribution, each variational distribution will have some associated parameters. Arriving at an optimal distribution for each $Q_j(Z_j)$ means finding the best settings of the variational parameters for each distribution. \n",
      "\n",
      "In Bayesian multiple linear regression, we have the following model:\n",
      "\n",
      "$$y_i = \\beta_0 + \\beta_1 x_{i, 1} + \\beta_2x_{i, 2} + \\cdots + \\beta_k x_{i,k} + \\epsilon_i, \\hspace{0.5in} i = \\{1, \\ldots, n\\}$$\n",
      "\n",
      "$$\\epsilon_i \\sim N(0, \\tau),$$ \n",
      "\n",
      "$$\\beta \\sim N(0, \\alpha),$$ \n",
      "\n",
      "$$\\alpha \\sim Gamma(a_0, b_0)$$\n",
      "-->\n",
      "\n",
      "<!--\n",
      "where:\n",
      "\n",
      "the $n$-vector $y$ is the dependent variable (observed variable)\n",
      "\n",
      "the $n x k$ matrix $X$ is the design matrix\n",
      "\n",
      "the $k$-vector $\\beta$ is a vector of regression coefficients (hidden parameter)\n",
      "\n",
      "the $n$ vector $\\epsilon$ is an error (noise) term with distribution $N(0, \\tau)$\n",
      "\n",
      "$\\tau$ is the precision (inverse of the covariance matrix) of the distribution of the error term (hidden parameter)\n",
      "\n",
      "$\\alpha$ is the precision of the distribution on the regression coefficients $\\beta$ (hidden parameter)\n",
      "\n",
      "Remember in a Bayesian treatment, parameters have distributions, and are considered random variables.\n",
      "-->\n",
      "\n",
      "<!--\n",
      "If you have Python 3.0, you can use a very nice new package called BayesPy, which also does Variational inference on a Gaussian mixture model (follow the example [here](http://www.bayespy.org/examples/gmm.html)). Since I have Python 2.7, I'm going to be doing things a little differently.\n",
      "\n",
      "Say we have data from a Gaussian process, and we can plot the data. In this very simple example, we'll assume that the number fo clusters is known a priori, but of course you can always treat the number of clusters as a hidden variable and infer it in a similar way to the ohter hidden variables in the model. In this example, there are 300 observations in each cluster:\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/MyData.png\" alt=\"\" style=\"width:600px\">\n",
      "\n",
      "and we know the true value of the means is [2,2] and [10,10], and that each cluster has the 2x2 identity covariance matrix. Say we didn't know this, though. All we had were the raw data, and we didn't know the means and covariance matrices. We could use mean field variational inference to infer them, as well as the cluster label for each data point.\n",
      "-->"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This will work regardless of your Python version\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#Get 50 samples from each of 4 separate 2D Multivariate Normal distributions\n",
      "#x0 = np.random.multivariate_normal([0, 0], [[2, 0], [0, 0.1]], size=50)\n",
      "#x1 = np.random.multivariate_normal([0, 0], [[0.1, 0], [0, 2]], size=50)\n",
      "#x2 = np.random.multivariate_normal([2, 2], [[2, -1.5], [-1.5, 2]], size=50)\n",
      "#x3 = np.random.multivariate_normal([-2, -2], [[0.5, 0], [0, 0.5]], size=50)\n",
      "\n",
      "\n",
      "#concatenate all of the observations together\n",
      "obs = np.vstack([x0, x1, x2, x3]) #this version is used in the online example\n",
      "obsplot = np.r_[x0, x1, x2, x3]  #this version is for my plot. The only difference is formatting.\n",
      "\n",
      "#and plot them\n",
      "plt.plot(obs[:,0][0:49], obs[:,1][0:49], 'ro', obs[:,0][50:99], obs[:,1][50:99], 'bo', obs[:,0][100:149], obs[:,1][100:149], 'go', obs[:,0][150:199], obs[:,1][150:199], 'mo',)\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('y')\n",
      "plt.axis([-3, 6, -3, 5])\n",
      "plt.title('Example data from four MVN distributions')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEZCAYAAAB/6SUgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh9qWNI/dAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x105b68950>"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now suppose we didn't know the cluster labels. That is, we don't know which of our 4 MVN distributions an observation comes from. We can visualize this with a plot without the colours - just the raw data:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/FromOnline1.png\" alt=\"\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Model####\n",
      "\n",
      "For clarity, let us denote the number of the data vectors with N"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 200"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and the dimensionality of the data vectors with D:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we don't know how many clusters our dataset has, we'll start with some large number of clusters, and the algorithm will pare this number down to a 'best guess' as to the true number of clusters. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember when we have more than 2 possible clusters (we're assuming 10 here to be safe), we use a Categorical distribution. So the cluster assignments Z will have a Categorical distribution, and the prior for the cluster assignment probabilities $P(\\pi)$ will have a Dirichlet distribution:\n",
      "\n",
      "$$P(\\pi) = Dir(\\pi \\mid \\alpha)$$\n",
      "\n",
      "(More about Dirichlet distributions [here](http://en.wikipedia.org/wiki/Dirichlet_distribution), but for this example you only need to know that it's the conjugate prior of the Categorical distribution)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "\n",
      "from bayespy.nodes import Dirichlet, Categorical\n",
      "\n",
      "#Create a Dirichlet process prior for the cluster assignments our P(\\pi) \n",
      "alpha = Dirichlet(1e-5*npones(K), name = 'alpha') \n",
      "\n",
      "#The cluster assignments themselves follow a Categorical distribution\n",
      "#Since there are N observations, there will be N cluster assignments\n",
      "Z = Categorical(alpha, plates=(N,), name='z')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also want to find the mean vectors and precision matrices of the clusters themselves (the precision matrix is the inverse of the covariance matrix, and is arguably nicer to work with when deriving the math behind this process). The conjugate prior of the mean of a Gaussian distribution is another Gaussian, and we will have exactly K elements in our mean vector, $\\mu$ (each cluster will have its own mean). \n",
      "\n",
      "The conjugate prior of the precision matrix $\\Lambda$ (inverse of the covariance matrix) is a Wishart distribution. (More about [Wishart distributions](http://en.wikipedia.org/wiki/Wishart_distribution)).\n",
      "\n",
      "(Side note: Some people like to put all of this together and talk about a Normal-inverse-Wishart distribution as a conjugate prior for a multivariate Normal distribution with unknown mean and covariance matrix. If you're interested, here's the [Wikipedia article](http://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution).)\n",
      "\n",
      "So we have:\n",
      "\n",
      "$$P(\\Lambda) = \\prod_{k = 1}^{K} \\mathcal{W}(\\lambda_k \\mid W_0, \\eta_0)$$\n",
      "\n",
      "$$P(\\mu \\mid \\Lambda) = \\prod_{k = 1}^{K} \\mathcal{N}(\\mu_k \\mid m_0, (\\beta_0 \\lambda_k)^{-1})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "\n",
      "from bayespy.nodes import Gaussian, Wishart\n",
      "\n",
      "mu = Gaussian(np.zeros(D), 1e-5*np.identity(D), plates=(K,), name='mu')\n",
      "\n",
      "Lambda = Wishart(D, 1e-5*np.identity(D), plates=(K,), name='Lambda')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that our data come from a Gaussian mixture distribution, but we don't know anything else. We create a Gaussian mixture distribution object:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "from bayespy.nodes import Mixture\n",
      "\n",
      " Y = Mixture(Z, Gaussian, mu, Lambda, name='Y') #This is the object\n",
      " Y.observe(obs) #and these are the model observations - the data."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we randomly initialize our cluster assignments $Z$ to start everything off:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "Z.initialize_from_random()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're ready to perform inference (get distributional estimates) on the hidden values, given the data:\n",
      "\n",
      "Z (cluster assignments), \n",
      "\n",
      "$\\Lambda$ (the precision matrix), \n",
      "\n",
      "$\\mu$ (the vector of means),\n",
      "\n",
      "$K$ (the number of clusters), and\n",
      "\n",
      "$\\pi$ (the probability of being in a cluster)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Inference####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We create a Variational Bayes object (same as variational inference in this case). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#This part will only work with Python 3.0\n",
      "from bayespy.inference import VB\n",
      "\n",
      "Q = VB(Y, mu, Lambda, Z, alpha)\n",
      "\n",
      "Q.update(repeat=1000)\n",
      "\n",
      "#This is the output:\n",
      "#Iteration 1: loglike=-1.402345e+03 (... seconds)\n",
      "#...\n",
      "#Iteration 61: loglike=-8.888464e+02 (... seconds)\n",
      "#Converged at iteration 61.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of the results can be visualized in the following plot:\n",
      "    \n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/FromOnline2.png\" alt=\"\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the algorithm has found all four clusters, and has figured out the cluster means and covariances, visualized by the ellipsoids. It has also figured out which observations belong to each cluster, with a few expections. It's clear that it would be hard to figure out the cluster assignment for the data point on the far right, for example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###4. The EM Algorithm and how it relates to variational inference###\n",
      "\n",
      "EM arises as a limiting case of VB. In particular, EM and VB are equivalent when the approximate posterior distribution is constrained to be a point mass (Bayesian Data Analysis, 3rd Edition, Gelman et al).\n",
      "\n",
      "In the EM algorithm, the number of clusters must be specified a priori, unless of course we want to use a separate algorithm to decide the number of clusters based on the data before running EM.\n",
      "\n",
      "Keeping the example of four bivariate Gaussian clusters, the algorithm for EM in the case of a (Bayesian) Gaussian mixture model can be paraphrased from Bishop (2006):\n",
      "\n",
      "Until convergence:\n",
      "\n",
      "1. Draw from the conjugate priors for the mean $\\mu$, and precision $\\Lambda$ of each component, and for the mixture probabilities $\\pi$. Evaluate the log likelihood $\\ell$.\n",
      "\n",
      "2. Using these simulated values, calculate the responsibilities $\\gamma_k$ for each component k. This is the E-step, as it involves an expectation:\n",
      "\n",
      "3. Using the responsibilities, estimate $\\mu$, $\\Lambda$, and $\\pi$ using the maximum likelihood estimates:\n",
      "\n",
      "4. Evaluate the log likelihood and check for convergence. That is, if $\\ell_t - \\ell_{t-1} \\lt \\epsilon$, stop iterating.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html\n",
      "\n",
      "import itertools\n",
      "\n",
      "import numpy as np\n",
      "from scipy import linalg\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "%matplotlib inline  \n",
      "from sklearn import mixture\n",
      "\n",
      "# Number of samples per component\n",
      "n_samples = 400\n",
      "\n",
      "# Generate random sample, two components\n",
      "np.random.seed(0)\n",
      "C = np.array([[0., -0.1], [1.7, .4]])\n",
      "#X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n",
      "#          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n",
      "#\n",
      "#print X\n",
      "\n",
      "y0 = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], size=100)\n",
      "y1 = np.random.multivariate_normal([10, 10], [[1, 0], [0, 1]], size=100)\n",
      "y2 = np.random.multivariate_normal([5, 5], [[1, 0], [0, 1]], size=100)\n",
      "y3 = np.random.multivariate_normal([-2, 8], [[1, 0], [0, 1]], size=100)\n",
      "y = np.vstack([y0, y1, y2, y3])\n",
      "\n",
      "## Fit a mixture of Gaussians with EM using four components\n",
      "gmm = mixture.GMM(n_components=4, covariance_type='full')\n",
      "gmm.fit(y)\n",
      "#\n",
      "## Fit a Dirichlet process mixture of Gaussians using four components\n",
      "dpgmm = mixture.DPGMM(n_components=5, covariance_type='full')\n",
      "dpgmm.fit(y)\n",
      "#\n",
      "color_iter = itertools.cycle(['r', 'g', 'b', 'c'])\n",
      "#\n",
      "for i, (clf, title) in enumerate([(gmm, 'GMM'),\n",
      "                                  (dpgmm, 'Dirichlet Process GMM')]):\n",
      "    splot = plt.subplot(2, 1, 1 + i)\n",
      "    Y_ = clf.predict(y) #predicts cluster label\n",
      "\n",
      "    for i, (mean, covar, color) in enumerate(zip(\n",
      "            clf.means_, clf._get_covars(), color_iter)):\n",
      "        v, w = linalg.eigh(covar)\n",
      "        u = w[0] / linalg.norm(w[0])\n",
      "    # as the DP will not use every component it has access to\n",
      "        # unless it needs it, we shouldn't plot the redundant\n",
      "        # components.\n",
      "        if not np.any(Y_ == i):\n",
      "            continue\n",
      "        plt.scatter(y[Y_ == i, 0], y[Y_ == i, 1], .8, color=color)\n",
      "\n",
      "        # Plot an ellipse to show the Gaussian component\n",
      "        angle = np.arctan(u[1] / u[0])\n",
      "        angle = 180 * angle / np.pi  # convert to degrees\n",
      "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)\n",
      "        ell.set_clip_box(splot.bbox)\n",
      "        ell.set_alpha(0.5)\n",
      "        splot.add_artist(ell)\n",
      "\n",
      "    plt.xlim(min(y[:,0]), max(y[:,0]))\n",
      "    plt.ylim(min(y[:,1]), max(y[:,1]))\n",
      "    plt.xticks(())\n",
      "    plt.yticks(())\n",
      "    plt.title(title)\n",
      "#\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x108174610>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Expectation Maximization as a limiting case of Variational inference ###\n",
      "\n",
      "The EM algorithm is usually used to find maximum likelihood estimates of the model parameters. In a Bayesian setting, however, the EM algorithm can be used for maximum a posteriori (MAP) estimation. One advantage of Bayesian EM is that the singularities that arise in GMM estimation disappear due to the influence of the prior distribution (Bishop 2006). As the number of samples approaches infinity, by the Central Limit Theorem the posterior of each model parameter becomes strongly peaked about its mean (this makes sense - the variance decreases as the sample size increases) (Attias, 1999). The algortihm for MAP estimation is slightly different than that for maximum likelihood, purely because of the prior distribution. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using notation from Bishop (2006), say we have a joint distribution over the hidden (Z) and visible (X) variables, and model parameters $\\theta$. If we wanted to do maximum likelihood estimation analytically, we would define the log likelihood function for the observed data given the model parameters and take derivatives and solve in the usual way. That log likelihood would have this form:\n",
      "\n",
      "$${\\rm{ln\\ }} P(X \\mid \\theta) = {\\rm{ln}}\\left\\{ \\sum_Z p(X, Z \\mid \\theta) \\right\\}$$\n",
      "\n",
      "We can't evaluate this likelihood analytically because of that annoying sum over the hidden states that appears inside the logarithm. As stated in Bishop (2006), if the model distribution is part of the exponential family (which would be nice), this log of a sum operation means that the posterior distribution ${\\rm{ln\\ }} P(X \\mid \\theta)$ doesn't necessarily belong to the exponential family. The maximum likelihood solutions will not have very nice forms.\n",
      "\n",
      "Suppose instead that we're told the latent state Z that corresponds to each value X. Then we have an expression for the complete data log likelihood (over the visible and hidden states): ${\\rm{ln\\ }} p(X, Z \\mid \\theta)$. The problem is that in practice, we don't know the values of Z. We can, however, quantify our knowledge of Z via the posterior distribution $p(Z \\mid X, \\theta)$. Instead of using the complete data log likelihood, which we don't know, we approximate it using the posterior for Z. \n",
      "\n",
      "We initialize the parameters $\\theta$ - call this $\\theta_c$. Then in the E-step, we take the expectation of the complete data log likelihood under the posterior distribution of Z, using the current setting of the model parameters:\n",
      "\n",
      "$Q(\\theta, \\theta_c) = \\sum\\limits_Z p(Z \\mid X, \\theta_c) {\\rm{ln\\ }}p(X, Z \\mid \\theta)$\n",
      "\n",
      "Then in the M-step, we maximize this function $Q$ with respect to $\\theta$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Algorithm for General EM ####\n",
      "\n",
      "\n",
      "1. Initialize model parameters $\\theta_c$\n",
      "2. E-step: evaluate $p(Z \\mid X, \\theta_c)$\n",
      "3. M-step: evaluate $\\theta_{new}$:\n",
      "\n",
      "    $$\\theta_{new} = {\\rm{argmax}}_{\\theta} Q(\\theta, \\theta_c)$$\n",
      "\n",
      "    where\n",
      "\n",
      "    $$Q(\\theta, \\theta_c) = \\sum\\limits_Z p(Z \\mid X, \\theta_c) {\\rm{\\ ln\\ }} p(X, Z \\mid \\theta)$$\n",
      "4. Check for convergence: \n",
      "\n",
      "    If ${\\rm{ln\\ }}p(X \\mid \\theta_{new}) -  {\\rm{ln\\ }}p(X \\mid \\theta_{c}) < \\epsilon_1$ \n",
      "\n",
      "    or $\\theta_{new} - \\theta_{c} < \\epsilon_2$\n",
      "\n",
      "    then stop. \n",
      "    \n",
      "    Else let $\\theta_c = \\theta_{new}$ and go back to step 2.\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In MAP estimation, we define a prior distribution over the model parameters $\\theta$. So the function that we need to maximize in the M-step becomes:\n",
      "    \n",
      "\n",
      "$$Q(\\theta, \\theta_c) = \\sum\\limits_Z p(Z \\mid X, \\theta_c) {\\rm{\\ ln\\ }} p(X, Z \\mid \\theta) + {\\rm{ln\\ }}p(\\theta)$$\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Moving from EM to VB ###\n",
      "\n",
      "Recall the (usual) goal of EM is to maximize the log likelihood of the data given the model parameters. The log likelihood ${\\rm{ln\\ }} p(x \\mid \\theta)$ can be decomposed in the following way:\n",
      "\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(X \\mid \\theta) &= {\\rm{ln}} \\sum_Z P(X, Z \\mid \\theta)  {\\hspace{0.5in} \\rm {\\ by\\ conditional\\ probability}} \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\sum_Z \\frac{P(X, Z \\mid \\theta)}{Q(Z)}Q(Z) \\\\[0.5em]\n",
      "&= {\\rm{ln\\ }} E_Q \\left[\\frac{P(X, Z \\mid \\theta)}{Q(Z)} \\right] {\\hspace{0.5in} \\rm{\\, since\\ E_X(g(X)) = \\sum_x g(x)f(x) dx,\\ by\\ definition,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(Z)}}\\\\[0.5em]\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[\\frac{P(X, Z \\mid \\theta)}{Q(Z)} \\right] {\\hspace{0.5in} \\rm{\\, by\\ Jensen's\\ Inequality}} \\\\[0.5em]\n",
      "&= E_Q \\left[{\\rm{ln\\ }} P(X, Z \\mid \\theta)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(Z)}\\right] \\\\[0.5em]\n",
      "&= {\\mathcal{L}}(Q , \\theta) \\\\\n",
      "\\end{align}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also notice the difference between the log likelihood and the lower bound is the KL divergence between the approximating (variational) distribution $Q$ and the true posterior distribution $p(Z\\mid X)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Proof:\n",
      "\n",
      "Rewrite $\\mathcal{L}(Q , \\theta)$ as \n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q , \\theta) &= E_Q \\left[{\\rm{ln\\ }} P(X, Z \\mid \\theta)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(Z)}\\right] \\\\[0.5em]\n",
      "&= \\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }} P(X, Z \\mid \\theta) - \\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }} Q(Z) \\\\[0.5em]\n",
      "&= \\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }} \\frac{P(X, Z \\mid \\theta)}{Q(Z)} \\\\[0.5em]\n",
      "&= \\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }} \\frac{P(Z \\mid X, \\theta)P(X \\mid \\theta)}{Q(Z)} \\\\[0.5em]\n",
      "& = \\sum\\limits_Z Q(Z) \\left\\{{\\rm{\\ ln\\ }} P(Z \\mid X, \\theta) + {\\rm{\\ ln\\ }} P(X \\mid \\theta) - {\\rm{\\ ln\\ }}{Q(Z)}\\right\\} \\\\[0.5em]\n",
      "\\end{align}\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and notice that the KL divergence is defined as:\n",
      "    \n",
      "$${\\rm{KL}}(Q || P) = -\\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }}\\frac{P(Z \\mid X, \\theta)}{Q(Z)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So:\n",
      "\\begin{align}    \n",
      "\\mathcal{L}(Q , \\theta) + {\\rm{KL}}(Q || P) &= \\sum\\limits_Z Q(Z) \\left\\{{\\rm{\\ ln\\ }} P(Z \\mid X, \\theta) + {\\rm{\\ ln\\ }} P(X \\mid \\theta) - {\\rm{\\ ln\\ }}{Q(Z)}\\right\\} -\\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }}\\frac{P(Z \\mid X, \\theta)}{Q(Z)} \\\\[0.5em]\n",
      "& = \\sum\\limits_Z Q(Z) \\left\\{{\\rm{\\ ln\\ }} P(Z \\mid X, \\theta) + {\\rm{\\ ln\\ }} P(X \\mid \\theta) - {\\rm{\\ ln\\ }}{Q(Z)}\\right\\} -\\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }}P(Z \\mid X, \\theta) + \\sum\\limits_ZQ(Z) {\\rm{\\ ln\\ }}{Q(Z)} \\\\[0.5em]\n",
      "&= \\sum\\limits_Z Q(Z){\\rm{\\ ln\\ }} P(X \\mid \\theta) \\\\[0.5em]\n",
      "& = {\\rm{\\ ln\\ }} P(X \\mid \\theta)\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "since ${\\rm{\\ ln\\ }} P(X \\mid \\theta)$ is independent of Z, and since $\\sum\\limits_Z Q(Z) = 1$, since $Q(Z)$ is a probability mass function. QED"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have $${\\rm{\\ ln\\ }} P(X \\mid \\theta) = \\mathcal{L}(Q , \\theta) + {\\rm{KL}}(Q || P)$$\n",
      "\n",
      "To maximize the log likelihood, ${\\rm{\\ ln\\ }} P(X \\mid \\theta)$, we need to maximize the lower bound or minimize the KL divergence between the distribution $Q(Z)$ and $P(Z| X, \\theta)$. Remember the KL divergence is never less than 0, so the log likelihood will always be at least $\\mathcal{L}(Q , \\theta)$ (so $\\mathcal{L}(Q , \\theta)$ is a lower bound on the log likelihood). For later, note:\n",
      "\n",
      "$$\\mathcal{L}(Q , \\theta) =  {\\rm{\\ ln\\ }} P(X \\mid \\theta) - {\\rm{KL}}(Q || P)$$\n",
      "\n",
      "so $\\mathcal{L}(Q , \\theta)$ is maximized when ${\\rm{KL}}(Q || P)$ is 0."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use this decomposition to define the EM algorithm:\n",
      "\n",
      "#### Algorithm for General EM using the log likelihood decomposition####\n",
      "\n",
      "\n",
      "1. Initialize model parameters $\\theta_c$\n",
      "2. E-step: holding $\\theta_c$ constant, maximize $\\mathcal{L}(Q, \\theta_c)$ with respect to $Q(Z)$. \n",
      "\n",
      "Note this is the same as the old E-step of evaluating $p(Z \\mid X, \\theta_c)$, since $\\mathcal{L}(Q, \\theta_c)$ is maximized when $Q(Z)$ is equal to $p(Z \\mid X, \\theta_c)$ (since this makes ${\\rm{KL}}(Q || P)$ = 0). The lower bound is then equal to the log likelihood.\n",
      "\n",
      "\n",
      "3. M-step: hold Q(Z) fixed and maximize $\\mathcal{L}(Q, \\theta)$ with respect to $\\theta$. \n",
      "\n",
      "Note this is the same as the old M-step of maximizing the expected complete data log likelihood, since (starting from the definition of the lower bound):\n",
      "\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q, \\theta) &= \\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }} P(X, Z \\mid \\theta) - \\sum\\limits_Z Q(Z) {\\rm{\\ ln\\ }} Q(Z) \\\\[0.5em]\n",
      "&= \\sum\\limits_Z P(Z \\mid X, \\theta_c) {\\rm{\\ ln\\ }} P(X, Z \\mid \\theta) - \\sum\\limits_Z P(Z \\mid X, \\theta_c) {\\rm{\\ ln\\ }} P(Z \\mid X, \\theta_c){\\rm{\\ substituting\\ after\\ the\\ E-step}} \\\\[0.5em]\n",
      "&= Q(\\theta, \\theta_c) + {\\rm{constant}}\n",
      "\\end{align}\n",
      "\n",
      "where the constant is the entropy of the Q(Z) distribution, which is independent of $\\theta$.\n",
      "\n",
      "<!--\n",
      "We get a value $\\theta_{new}$, which means $KL(Q || P)$ is now nonzero because Q is being held fixed under $\\theta_c$, and the new posterior distribution will be $P(Z \\mid X, \\theta_{new})$. So the log likelihood ${\\rm{\\ ln\\ }} P(X \\mid \\theta)$ will increase\n",
      "-->\n",
      "\n",
      "evaluate $\\theta_{new}$:\n",
      "\n",
      "   $$\\theta_{new} = {\\rm{argmax}}_{\\theta} Q(\\theta, \\theta_c)$$\n",
      "\n",
      "   where\n",
      "\n",
      "   $$Q(\\theta, \\theta_c) = \\sum\\limits_Z p(Z \\mid X, \\theta_c) {\\rm{\\ ln\\ }} p(X, Z \\mid \\theta)$$\n",
      "4. Check for convergence: \n",
      "\n",
      "    If ${\\rm{ln\\ }}p(X \\mid \\theta_{new}) -  {\\rm{ln\\ }}p(X \\mid \\theta_{c}) < \\epsilon_1$ \n",
      "\n",
      "    or $\\theta_{new} - \\theta_{c} < \\epsilon_2$\n",
      "\n",
      "    then stop. \n",
      "    \n",
      "    Else let $\\theta_c = \\theta_{new}$ and go back to step 2.\n",
      "\\end{enumerate} "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### But what happens if the posterior distribution is intractable? ####\n",
      "\n",
      "We relied on letting $Q(Z) = P(Z \\mid X, \\theta_c)$ in the E-step. If $P(Z \\mid X, \\theta_c)$ is intractable, then this step breaks down.\n",
      "\n",
      "So instead of trying to work with the intractable posterior, we use a tractable \"variational posterior\" $Q(Z)$ that is close (in terms of KL divergence) to the true posterior. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variational posterior is chosen such that maximization becomes easy. Using the  mean field assumption, we say that all of the hidden variables are independent of one another and construct:\n",
      "\n",
      "$$Q(Z) = \\prod_j Q(Z_j)$$\n",
      "\n",
      "We can extend this one step further and allow each variational posterior to have its own (variational) parameters. \n",
      "\n",
      "$$Q(Z \\mid \\lambda) = \\prod_j Q(Z_j \\mid \\lambda_j)$$\n",
      "\n",
      "We then optimize the variational parameters to enforce closeness between the variational posterior and the true posterior. The steps remain similar. In fact, only the M step is changed:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exponential family models ###\n",
      "\n",
      "Stochastic variational inference is currently done using exponential family distributions. These distributions are useful for a number of reasons, not least of which is the property of conjugacy. Recall from Bayes Theorem:\n",
      "\n",
      "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)} = \\frac{P(B \\mid A) P(A)}{P(B)}$$\n",
      "\n",
      "If the prior, $P(A)$ is of the same family as the posterior, $P(A \\mid B)$, then the prior and the posterior are conjugate distributions. Exponential family distributions will always have a conjugate distribution. This is not true for all non-exponential family models.\n",
      "\n",
      "There are several general forms of exponential family models. Following the notation in Hoffman et al (2013), we'll use the following exponential family form:\n",
      "\n",
      "$$f_x (x \\mid \\theta) = h(x) {\\rm {exp}}\\left[ \\eta(\\theta) T(x) - a(\\eta)\\right]$$\n",
      "\n",
      "where \n",
      "\n",
      "$h(x)$ is the \"base measure\" - a function of the data alone, completely independent of the hidden variables.\n",
      "\n",
      "$\\eta$ is the \"natural parameter\" (aka the \"canonical parameter\") of the distribution\n",
      "\n",
      "$T(x)$ is a vector of sufficient statistics\n",
      "\n",
      "$a$ is the \"log normalizer\", which assumes a form that forces the distribution to be normalized.\n",
      "\n",
      "All exponential family models (for example, the Gaussian distribution, the Binomial distribution, the exponential distribution, etc) can be written in this form. \n",
      "\n",
      "Anyone who would like to look more into this might want to consult [this lecture](http://www.genome.duke.edu/labs/engelhardt/courses/scribe/lec_09_02_2013.pdf). \n",
      "\n",
      "In the mean-field framework, we've introduced the assumption that all hidden variables are independent. We can also introduce another assumption, namely that the complete conditional distributions come from the exponential family.\n",
      "\n",
      "A complete conditional distribution is the conditional distribution of a particular hidden variable given all other hidden variables and all observed variables. So we will have two complete conditional distribution forms: one for the global hidden variable $\\beta$ and one for the local hidden variable $z_{i}$:\n",
      "\n",
      "$$P(\\beta \\mid x, z) = h(\\beta) {\\rm{\\ exp}} \\left[\\eta_g (x, z)^T\\ \\beta - a(\\eta_g(x, z))\\right]$$\n",
      "\n",
      "$$P(z_{i} \\mid x_i, \\beta) = h(z_{i}) {\\rm{\\ exp}} \\left[\\eta_\\ell (x_i, \\beta)^T\\ z_{i} - a(\\eta_\\ell(x_i, \\beta))\\right]$$\n",
      "\n",
      "Since both $P(\\beta \\mid x, z)$ and $P(z_{i} \\mid x_i, \\beta)$ are assumed to come from exponential family distributions, there is a conjugacy relationship between the global hidden variables $\\beta$ and the local variables $x_i$ and $z_i$:\n",
      "\n",
      "$$P(x_i, z_i \\mid \\beta) = h(x_i, z_i) exp \\left[ \\beta^T T(x_i, z_i) - a_{\\ell} (\\beta)\\right]$$\n",
      "\n",
      "We need these distributions so we can take the required gradients later on. Now that we have the complete conditional of $\\beta$, we can define a distribution over its variational family, knowing that it will also have an exponential family distribution (since there's one variational parameter for each hidden variable):\n",
      "\n",
      "$$Q(\\beta \\mid \\lambda) = h(\\beta) {\\rm {exp}} \\left[ \\lambda^T \\beta - a(\\lambda) \\right]$$\n",
      "\n",
      "Similarly, we can also define a distribution over the variational family of $z_i$ $\\phi$:\n",
      "    \n",
      "$$Q(z_i \\mid \\phi_i) = h(z_i) {\\rm {exp}} \\left[ \\phi_i^T z_i - a(\\phi_i) \\right]$$\n",
      "\n",
      "Again, we need these distributions so we can construct $ Q(\\beta, z)$ and take the gradient later on.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Maximizing the ELBO with respect to the variational parameters ####\n",
      "\n",
      "Here is our objective function, now that everything has (finally!) been defined:\n",
      "\n",
      "$$\\mathcal{L}(\\lambda, \\phi_{1:n}) =  E_Q \\left[{\\rm{ln\\ }} P(\\beta, z, x)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(\\beta, z)}\\right]$$\n",
      "\n",
      "remember that maximizing this lower bound is the same as finding the distribution $Q(\\beta, z)$ that is closest in KL divergence to the true posterior distribution.\n",
      "\n",
      "To maximize this function we need to resort to numerical methods (big surprise), and use coordinate ascent. To do this, we hold all parameters fixed but one, then perform coordinate ascent on that parameter, and move on to the next parameter and so on. Note this could all be parallelized, because all of the hidden variables were assumed indepedent.\n",
      "\n",
      "This means we have to take the gradient of $\\mathcal{L}(\\lambda, \\phi_{1:n})$ with respect to each variational parameter. Without going into derivations, here are the gradients we need:\n",
      "\n",
      "$$\\nabla_\\lambda \\mathcal{L} = a''(\\lambda) \\left\\{E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda \\right\\}$$\n",
      "\n",
      "which leads to this update:\n",
      "\n",
      "$$\\lambda^{*} = E_\\phi \\left[\\eta_g (x, z) \\right]$$\n",
      "\n",
      "Again, we're taking the expectation with respect to all of the other variational parameters, $\\phi$ (we're holding them constant, but they still have a distribution, so we can still take this expectation). The local variational parameters are similar, and the update is shown in the algorithm below.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Algorithm for mean-field variational inference ####\n",
      "\n",
      "Randomly initialize $\\lambda$\n",
      "\n",
      "Until ELBO convergence:\n",
      "\n",
      "1) For each observation, update the local variational parameters according to $\\phi_i^{(t)} = E_{\\lambda^{(t-1)}}\\left[\\eta_\\ell (x_i, \\beta) \\right]$ for $i \\in 1, \\ldots, n$\n",
      "\n",
      "2) Update the global variational parameters according to $\\lambda^{(t)} = E_\\phi^{(t)} \\left[\\eta_g (x_{1:n}, z_{1:n}) \\right]$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: you need to analyze the entire dataset before completing one iteration of this algorithm. That makes it very inefficient."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----------------------------\n",
      "### The Natural Gradient ###\n",
      "\n",
      "To go from vanilla variational inference to stochastic variational inference, we need to jump through some hoops. One of those hoops is the natural gradient. I'll only go over some intuition, but anyone who's interested in details can have a look at Amari (1998). The takeaway message is, we need to take derivatives in a Riemannian space instead of a Euclidean space, and to do that we need some fancy math. Then we apply the fancy math and get a nicer version of the gradient for the update.\n",
      "\n",
      "\n",
      "#### The Riemannian Metric ####\n",
      "\n",
      "Suppose we have a probability function space that is indexed by a parameter $p$ (so if you pick different values of $p$ in this space, you get different probability distributions), and we pick a starting value for the parameter, $p_0$. That value $p_0$ corresponds to a probability distribution with parameter value $p_0$. If we were to move in any direction in that space, we would get a probability distribution corresponding to a different value of $p$. Suppose the space is only 3 dimensional, so a unit change in $p$ would mean that we were somewhere on the unit ball with $p_0$ in the centre. Suppose our move takes us in a direction $d_1$. Then we have a new probability distribution parameterized by $p_{d_1}$. This new probability distribution has a KL divergence with the initial distribution. \n",
      "\n",
      "Now say instead we went in a different direction on the unit ball with $p_0$ in the centre. Call this new direction $d_2$. Then we get another probability distribution that has parameter $p_{d2}$. \n",
      "\n",
      "Even though the distance between $p_0$ and $p_{d1}$ is equal to the distance between $p_0$ and $p_2$ in Euclidean space, their KL divergences will not necessarily be equal.\n",
      "\n",
      "The example given in Hoffman et al (2013) illustrates this problem nicely. Suppose we're interested in the univariate normal distribution. The distributions N(0, 10000) and N(10, 10000) are almost indistinguishable, and the Euclidean distance between their parameter vectors is 10; however, the distributions N(0, 0.01) and N(0.1, 0.01) barely overlap, but they have a much smaller Euclidean distance between their parameter vectors (0.1). \n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/NormalVar10000-1.png\" alt=\"\" style=\"width:400px\"> | <img src=\"https://github.com/caugusta/variational-inference/raw/master/Normalvar001-1.png\" alt=\"\" style=\"width:400px\"> \n",
      "\n",
      "\n",
      "So Euclidean distance is a poor measure of difference between distributions, and we need a way to convert this Euclidean space where distances are measured based on an $L_2$ norm to a Riemannian space, where we can measure differences using a symmetrized KL divergence. We need a symmetrized KL divergence because otherwise the 'distance' from p to q would not be the same as the distance from q to p, if p and q are probability distributions in some probability space.\n",
      "\n",
      "$$D_{KL}^{sym} (\\lambda, \\lambda') = E_{\\lambda} \\left[ log \\displaystyle{\\frac{Q(\\beta \\mid \\lambda)}{Q(\\beta \\mid \\lambda')}} \\right] + E_{\\lambda'} \\left[ log \\displaystyle{\\frac{Q(\\beta \\mid \\lambda')}{Q(\\beta \\mid \\lambda)}} \\right]$$\n",
      "\n",
      "In natural gradient ascent, we premultiply the gradient that we already have by the inverse of the Riemannian metric. In probability distributions (so in exponential family distributions), the Riemannian metric is just the Fisher Information."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### What the update looks like now####\n",
      "\n",
      "For exponential family distributions, the Fisher Information is the second derivative of the log normalizer (the \"a\" function from the exponential family distribution definition). I'm not going to prove that here, because it's shown explicitly in Hoffman et al (2013).\n",
      "\n",
      "So for the update to the global hidden variable, which is based on its variational parameter $\\lambda$, the Fisher Information metric is defined as:\n",
      "$G(p) = a''(\\lambda)$\n",
      "\n",
      "Recall the original gradient with respect to $\\lambda$ was:\n",
      "\n",
      "$$\\nabla_\\lambda \\mathcal{L} = a''(\\lambda) E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda$$\n",
      "\n",
      "So when we premultiply by $a''^{-1}(\\lambda)$, we get:\n",
      "\n",
      "$$a''^{-1}(\\lambda)\\nabla_\\lambda \\mathcal{L} = E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda$$\n",
      "\n",
      "Or, using the notation in Hoffman et al (2013):\n",
      "\n",
      "$$\\hat{\\nabla}_\\lambda \\mathcal{L} = E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda$$\n",
      "\n",
      "<!---\n",
      "(I'm not sure why we didn't have to worry about $a''(\\lambda)$ being multiplied by $\\lambda$)\n",
      "-->\n",
      "\n",
      "And analogously:\n",
      "\n",
      "$$\\hat{\\nabla}_{\\phi_i} \\mathcal{L} = E_{\\lambda} \\left[\\eta_\\ell (x_i, z_i, \\beta) \\right] - \\phi_i$$\n",
      "\n",
      "<!---\n",
      "Hoffman et al (2013) alleges you can compute the natural gradient by \"computing the coordinate updates in parallel and subtracting the current setting of the parameters\". I don't see it yet. I understand the \"subtracting the current setting of the parameters\" part.\n",
      "-->\n",
      "\n",
      "So all you have to do to get these updates is to compute the coordinate updates (those expectations), which can be done in parallel, and subtract the current setting of the parameters. These natural gradients are very easy to compute.\n",
      "\n",
      "The other advantage is that now, of course, we don't have to worry about computing the Fisher Information matrix, like in traditional variational inference. That's pretty costly, so that's now saved.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stochastic optimization ###\n",
      "\n",
      "That coordinate ascent algorithm is still inefficient for large datasets, because we haven't eliminated the need to see all the observations before making an update ($\\hat{\\nabla}_\\lambda \\mathcal{L}$ needs to see all the x values before making an update). Here's where that problem gets solved. We're going to use stochastic optimization on the global hidden variables (because finding the local ones is computationally cheap - it's the global ones that cause the problem).\n",
      "\n",
      "The way stochastic optimization works is to get a noisy estimate of the gradient instead of working with the true gradient. The noise in this case is exclusively sampling noise. You start with a big step size in the direction of the initial noisy gradient based on only one observation from the dataset. Then, as you see more and more observations, you decrease the step size.\n",
      "\n",
      "Since the gradient is composed of additive terms (one for each observation), it's easy to compute a noisy approximation of the gradient by sampling (you just don't have a lot of those terms at the beginning, and get more of them as you sample). As long as certain conditions on the step-size schedule are met, this will converge to a global optimum (if the function is convex) and a local optimum otherwise.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Stochastic natural gradients####\n",
      "\n",
      "Still following the notation in Hoffman et al (2013), suppose we have an objective function $f(\\lambda)$ and a random function $B(\\lambda)$ such that $E_q \\left[ B(\\lambda) \\right] = \\nabla_{\\lambda} f(\\lambda)$. Then we can optimize $f(\\lambda)$ by following samples $b_t$ from the function $B(\\lambda)$. The update for $\\lambda$ is then:\n",
      "\n",
      "$$\\lambda^{(t)} = \\lambda^{(t-1)} + \\rho_t b_t(\\lambda^{(t-1)})$$\n",
      "\n",
      "The conditions the step-size schedule needs to satisfy are:\n",
      "\n",
      "$$\\sum \\rho_t = \\infty; \\sum \\rho_t^2 < \\infty$$\n",
      "\n",
      "Now if we use natural gradients instead of Euclidean gradients, then we have:\n",
      "    \n",
      "$$\\lambda^{(t)} = \\lambda^{(t-1)} + \\rho_t G_t^{-1} b_t(\\lambda^{(t-1)})$$\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stochastic Variational Inference ###\n",
      "\n",
      "So what we're worried about is maximizing the ELBO with respect to the global variational parameter $\\lambda$. Again skipping a lot of derivation, we can write the ELBO with respect to $\\lambda$ in the following form:\n",
      "\n",
      "$$\\mathcal{L} (\\lambda) = E_Q \\left[ {\\rm {ln\\ }} P(\\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(\\beta \\mid \\lambda)\\right] + \\sum_{i=1}^n \\left(E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i \\mid \\phi_i) \\right]\\right)$$\n",
      "\n",
      "Now if we had only one observation instead of all $n$ (that is, if we sampled 1 data point from our dataset according to a discrete Uniform distribution), and pretended that this one sampled data point is the only data point we'll ever see (so we replicate it n times to match the dimension we need for $z$ and $\\phi$) then we would have the following form instead:\n",
      "\n",
      "$$\\mathcal{L}_i (\\lambda) = E_Q \\left[ {\\rm {ln\\ }} P(\\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(\\beta \\mid \\lambda)\\right] + n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i \\mid \\phi_i) \\right]\\right)$$\n",
      "\n",
      "The stochastic gradient algorithm works because this version of the ELBO with respect to 1 observation is an unbiased estimator of the true ELBO with respect to all of the observations. In other words, the expected value of $\\mathcal{L}_i \\lambda$ with respect to the sampled observation is equal to the actual ELBO with respect to all n observations, $\\mathcal{L} \\lambda$. \n",
      "\n",
      "You can see this because if we took the expectation with respect to the sampled observation, and we had sampled independently with probability 1/n, the only term with a $x_i$ (the observation) in $\\mathcal{L}_i \\lambda$ is \n",
      "\n",
      "$$n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)$$\n",
      "\n",
      "The rest will be constants with respect to $x_i$, so we don't care about them when computing this expectation.\n",
      "\n",
      "The sampling procedure will have a discrete Uniform distribution. recall the definition of expectation for a discrete random variable:\n",
      "    \n",
      "$$E(f(X)) = \\sum_{x} f(x) p(x)$$\n",
      "\n",
      "But $p(x) = 1/n$, so :\n",
      "\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "E(f(X)) &= \\sum_{x} f(x) p(x) \\\\[0.5em]\n",
      "&=\\sum_{i=1}^n\\left[n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\frac{1}{n} \\\\[0.5em]\n",
      "&= \\sum_{i=1}^n\\left[\\frac{n}{n}\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\\\[0.5em]\n",
      "&= \\sum_{i=1}^n \\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right) \\\\[0.5em]\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "E(f(X)) &= \\sum_{x} f(x) p(x) \\\\[0.5em]\n",
        "&=\\sum_{i=1}^n\\left[n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\frac{1}{n} \\\\[0.5em]\n",
        "&= \\sum_{i=1}^n\\left[\\frac{n}{n}\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\\\[0.5em]\n",
        "&= \\sum_{i=1}^n \\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right) \\\\[0.5em]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x101f97050>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the expected value of the sampled ELBO is the true ELBO, which makes this sampled ELBO an unbiased estimate of the true ELBO. \n",
      "\n",
      "If we take the natural gradient with respect to $\\lambda$ of the sampled ELBO, then the natural gradient of the sampled ELBO will be an unbiased estimate of the natural gradient of the true ELBO (proof omitted). So the gradient of the sampled ELBO is a noisy gradient of the true ELBO.\n",
      "\n",
      "The sampled ELBO is essentially an ELBO where we saw the same observation $n$ times. \n",
      "\n",
      "Again skipping over a bunch of derivation, the formula for the natural gradient of the sampled ELBO is:\n",
      "    \n",
      "$$\\nabla_\\lambda \\hat{\\mathcal{L}}_i = E_{\\phi_i} \\left[ \\eta_i(z_i, x_i) \\right] - \\lambda$$\n",
      "\n",
      "So we only need the local variational parameters for one observation, instead of having to calculate all $n$ of them. If we were to calcluate the full natural gradient according to that $\\mathcal{L} (\\lambda)$, then we would have to use all of the local parameters $\\phi_{1:n}$.  \n",
      "\n",
      "That result leads, at last, to the algorithm for stochastic variational inference.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Algorithm for Stochastic Variational Inference ####\n",
      "\n",
      "Randomly initialize global parameter(s) $\\lambda$\n",
      "Set a step size schedule $\\epsilon_t$\n",
      "Repeat\n",
      "\n",
      "1) Sample an observation $x_i$ from your dataset, without replacement, according to a DU[1, n] distribution.\n",
      "\n",
      "2) Compute the local variational parameter $\\phi_i$ for that particular observation, according to:\n",
      "\n",
      "  $$\\phi = E_{\\lambda^{(t-1)}}\\left[\\nabla_\\ell (x_i, \\beta) \\right]$$\n",
      "  \n",
      "3) Make believe that $x_i$ is the only observation in the dataset, and calculate\n",
      "\n",
      "  $$\\hat{\\lambda} = E_{\\phi} \\left[ \\eta_i (x_i, z_i)\\right]$$\n",
      "  \n",
      "4) Update the current estimate of the global variational parameter:\n",
      "\n",
      "  $$\\lambda^{t} = (1 - \\epsilon_t) \\lambda^{(t-1)} + \\epsilon_t\\hat{\\lambda}$$\n",
      "    \n",
      "remember the step size decreases as a function of $t$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Example of stochastic variational inference for Gaussian mixture model ####\n",
      "\n",
      "Recall a Gaussian mixture model is a submodel of the more general Dirichlet Process mixture model (DPMM). \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Markov chain Monte Carlo ###\n",
      "\n",
      "I've seen this compared to random walk metropolis hastings MCMC, and to reversible-jump mcmc.\n",
      "\n",
      "RWMHMCMC\n",
      "\n",
      "Reversible-jump MCMC\n",
      "\n",
      "GET REFERENCES FOR STOCHASTIC VERISON"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Conclusion ###\n",
      "\n",
      "Variational inference is useful when the posterior distribution is intractable. It is similar to several methods, like the EM (expectation-maximization) algorithm, especially for mixtures of Gaussians. According to Bishop (2006), there are some advantages to variational inference over the EM algorithm:\n",
      "\n",
      "1. The singularities that we see in the (frequentist) EM algorithm do not arise in this Bayesian treatment.\n",
      "\n",
      "2. If we choose a large number of initial clusters (here we chose K = 10), we don't have to worry about overfitting. \n",
      "\n",
      "3. Finding the optimal value of K did not require the extra step of cross-validation.\n",
      "\n",
      "Variational methods have been extended beyond the mean-field assumption, and have been extended to stochastic versions (see Hoffman et al 2013). Future work lies in moving away from exponential family distributions, finding theoretical guarantees in the stochastic algorithm, and more."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### References ###\n",
      "\n",
      "Bishop, C.M. (2006) Pattern Recognition and Machine Learning. Springer.\n",
      "\n",
      "Bishop, C.M. (1998) Variational Learning in Graphical Models and Neural Networks.  Proceedings 8th International Conference on Artificial Neural Networks, ICANN'98, L.Niklasson et al. (eds), Springer (1998) 1, 13-22.\n",
      "\n",
      "Brodersen, K.H. Variational Bayesian Inference. [Online] http://people.inf.ethz.ch/bkay/talks/Brodersen_2013_03_22.pdf\n",
      "\n",
      "Feng, Z. Assignment 3, STAT 6841: Statistical Inference. Winter 2015. University of Guelph. Guelph, ON, Canada.\n",
      "\n",
      "Hoffman et al (2013). Stochastic Variational Inference. Journal of Machine Learning Research (14), 1303-1347.\n",
      "\n",
      "Jaakko Luttinen and Hannu Hartikainen. BayesPy. http://www.bayespy.org/examples/gmm.html\n",
      "\n",
      "Munroe, R. xkcd. Frequentists vs Bayesians. http://xkcd.com/1132/\n",
      "\n",
      "R dataset (Faithful). R Core Team (2013). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL http://www.R-project.org/.\n",
      "\n",
      "Sridharan, R. Gaussian Mixture Models and the EM Algorithm. http://people.csail.mit.edu/rameshvs/content/gmm-em.pdf"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}