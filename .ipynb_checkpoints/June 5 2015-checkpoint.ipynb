{
 "metadata": {
  "name": "",
  "signature": "sha256:d5887d689c57f9c6e3866a678fbe3a4d7045ce989105445058cecd1a5cb4fe45"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#The General idea of posterior inference#\n",
      "\n",
      "The posterior distribution $P(\\beta, z \\mid x)$ is intractable. There are two general approaches we can take to resolve this:\n",
      "\n",
      "a) Deterministic approximate inference (classical variational inference)\n",
      "\n",
      "b) Stochastic approximate inference (MCMC, stochastic variational inference).\n",
      "\n",
      "##Classical variational inference##\n",
      "\n",
      "While there are other algorithms available, the most popular method for variational inference uses the mean-field approximation. The crux of the approximation is this: we assume the hidden variables are independent. Of course, this does not work for many models. However, it provides a quick and easy approximation. For this reason, many researchers have cited variational methods as being faster than MCMC (but this is a controversial statement). We construct a function that is close to the true posterior in terms of KL divergence, then perform inference on this nice, known function. Results are approximate.\n",
      "\n",
      "##Markov chain Monte Carlo (MCMC)##\n",
      "\n",
      "Posterior inference is carried out using samples from the posterior. We construct a Markov chain that has a stationary distribution equal to the posterior distribution, and run the chain to \"convergence\" (which is a blurry term - convergence diagnostics are still under development). Then we sample from the stationary distribution of the Markov chain, which gives us posterior samples. MCMC will yield exact results if the process is allowed to run forever. These chains, though, often exhibit slow mixing, which means that MCMC can take a long time to yield good results.\n",
      "\n",
      "##Stochastic variational inference##\n",
      "\n",
      "Classical variational inference has come under fire for using the entire dataset in each iteration of the algorithm. Thus, classical variational methods do not scale to large datasets. Stochastic variational inference solves this problem, but adds others. Instead of looking at the entire dataset to perform a parameter update, we sample one datapoint uniformly from the set of observations and use just that observation to inform parameter changes. The stochasticity here comes entirely from sampling noise. While the algorithm as described is entirely online, extensions to minibatches have been implemented. Full batch stochastic variational inference is equivalent to classical variational inference. One new problem introduced by this method is the requirement of a step size schedule, to be described later (one solution has been provided by Ranganath et al, ICML 2013). Future work includes extending the methodology to time series models (started by Johnson and Willsky, ICML 2014), and to non-exponential family models.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Deriving classical (mean field) variational inference from the EM algorithm ##\n",
      "\n",
      "The Expectation-Maximization (EM) algorithm is a method for maximum likelihood estimation when the parameters are coupled. An extension of the ML EM algorithm is the MAP-EM algorithm for Bayesian maximum a posteriori inference (IEEE Signal Processing magazine). The construction of and algorithms for EM and variational inference are very similar. \n",
      "\n",
      "In fact, EM arises as a limiting case of VB. The main difference between the two methods is that in EM, a point estimate for the parameter(s) of interest is produced, whereas in VB, a distribution for the parameter(s) is produced.\n",
      "\n",
      "### The setup ###\n",
      "\n",
      "Suppose we have data $X = \\left\\{x_1, \\ldots, x_n \\right\\}$, global hidden variables $\\beta$ and local hidden variables $Z = \\left\\{z_1, \\ldots, z_n\\right\\}$. Notice we have one local hidden variable for each observation $x_i$ and the global hidden variables do not necessarily have dimension $n$. This setup can be shown using a plate diagram (below)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/PlateModel.png\" alt=\"A plate model with N observations\" style=\"width:250px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plate model (above) is a schematic for a model distribution, showing the conditional dependencies. In a plate model, a rectangle with a total (n) in the bottom right indicates that all variables or parameters inside the rectangle (the \"plate\") are indexed from 1 to n. So we have a pictoral representation of $z_i : i = 1, \\ldots, n$ and $x_i : i = 1, \\ldots, n$. The notation for unobserved variables (hidden units, in this case $z_i$ and $\\beta$) in a plate diagram is an unfilled circle. Observed variables (visible units, in this case $x_i$) are filled circles. Outside the plate are variables or parameters that are not repeated n times. That is, $\\beta$ is a vector of global hidden variables, which means the length of the vector is not necessarily $n$. The arrows between components in the plate model show how each variable or parameter is related. For example, the arrow from $\\beta$ to $z_i$ indicates that the value of $z_i$ dependes on the value of $\\beta$. Taken as a whole, the plate model shows that the joint (model) distribution $P(x, z, \\beta)$, and illustrates the dependencies in the model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For example, in a Gaussian mixture model, the global parameters $\\beta$ would be the means and precision matrices of the components, and the local parameters $z$ would be the identity of the component that the observation belongs to. (http://www-users.cs.york.ac.uk/adrian/Papers/Journals/TSMC-B06.pdf)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model then looks like this:\n",
      "    \n",
      "    \n",
      "\\begin{align}\n",
      "P(x, z, \\beta) &= P(\\beta) \\prod_{i=1}^n P(z_i \\mid \\beta) P(x_i \\mid z_i, \\beta) %P(x_i, z_i \\mid \\beta)\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that given the global hidden parameters, $\\beta$, the local hidden variables $z$ are conditionally independent. While the assumption of total independence (the mean field assumption) doesn't really hold, there is some independence structure here that we are exploiting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal, as usual, is to infer the posterior distribution over the hidden variables, given the observed data:\n",
      "\n",
      "$$P(\\beta, z \\mid x)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, the problem is that the posterior distribution is intractable, because the normalization constant $P(x)$ (aka the partition function) is intractable. Recall the relationship from Bayesian statistics:\n",
      "\n",
      "$$P(A, B \\mid C) = \\frac{P(A, B, C)}{P(C)}$$\n",
      "\n",
      "Using this relationship, we can show the formula needed to calculate the posterior distribution:\n",
      "\n",
      "$$P(\\beta, z \\mid x) = \\frac{P(x, z, \\beta)}{\\int_z \\int_\\beta P(x, z, \\beta)dzd\\beta}$$\n",
      "\n",
      "\n",
      "\n",
      "where the denominator comes from marginalizing over the global hidden variables $\\beta$ and the local hidden variables $z$, which means the denominator becomes a function of the data alone (hence calling it the marginal likelihood)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have the joint distribution from our model, and what we're lacking is the marginal likelihood, $P(x)$.\n",
      "\n",
      "Denote the incomplete data log-likelihood by $P(x \\mid \\beta)$ and the complete data log likelihood by $P(x, z \\mid \\beta)$. Assuming $z$ to be given is equivalent to assuming the cluster assignment is given for each observation in the case of a Gaussian mixture model.\n",
      "\n",
      "Often we do not have the cluster labels, so they are considered a nuisance parameter. We integrate over them to obtain the incomplete data log-likelihood:\n",
      "\n",
      "$${\\rm{ln\\ }} P(x \\mid \\beta) = {\\rm{ln\\ }} \\int_{z} P(x, z \\mid \\beta) dz$$\n",
      "\n",
      "We can derive a lower bound for the incomplete data log likelihood:\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(x \\mid \\beta) &= {\\rm{ln}} \\int_z P(x, z \\mid \\beta)\\ dZ  {\\hspace{0.5in} \\rm {\\ by\\ conditional\\ probability}} \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\int_z \\frac{P(x, z \\mid \\beta)}{Q(z)}Q(z)\\ dz \\\\[0.5em]\n",
      "&= {\\rm{ln\\ }} E_Q \\left[\\frac{P(x, z \\mid \\beta)}{Q(z)} \\right] {\\hspace{0.5in} \\rm{\\, since\\ E_X(g(X)) = \\int_x g(x)f(x) dx,\\ by\\ definition,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(z)}}\\\\[0.5em]\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[\\frac{P(x, z \\mid \\beta)}{Q(z)} \\right] {\\hspace{0.5in} \\rm{\\, by\\ Jensen's\\ Inequality}} \\\\[0.5em]\n",
      "&= E_Q \\left[{\\rm{ln\\ }} P(x, z \\mid \\beta)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(z)}\\right] \\\\[0.5em]\n",
      "&= {\\mathcal{L}}(Q , \\beta) \\\\\n",
      "\\end{align}\n",
      "\n",
      "<!--\n",
      "Suppose we have the complete data (that is, we have the cluster assignment $z_i$ for each data point $x_i$, and we have the observations $x_i$). Since the likelihood is a function of $x$ and $z$, denote it $P(x, z \\mid \\beta)$. We want to maximize the likelihood given the model parameters.\n",
      "\n",
      "$$P(\\beta \\mid x, z) = \\frac{P(x, z \\mid \\beta) P(\\beta)}{ \\int_\\beta P(x, z \\mid \\beta) P(\\beta)d\\beta}$$\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The posterior distribution is intractable because the normalization constant (marginal likelihood) is intractable. \n",
      "\n",
      ": HERE! This is weird because my model distribution would change, right? \n",
      "\n",
      "The likelihood function for the observed data and the hidden variables \n",
      "\n",
      "$$P(x \\mid \\beta, z) = \\int_z \\int_\\beta P(x, z, \\beta)dzd\\beta$$\n",
      "\n",
      "\n",
      "$$P(x \\mid \\beta, z) = \\int_z \\int_\\beta P(x, z \\mid \\beta)dzd\\beta$$\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have $${\\rm{\\ ln\\ }} P(x \\mid \\beta) = \\mathcal{L}(Q , \\beta) + {\\rm{KL}}(Q || P)$$ (I can send you the proof of this one).\n",
      "\n",
      "To maximize the log likelihood, ${\\rm{\\ ln\\ }} P(x \\mid \\beta)$, we need to maximize the lower bound or minimize the KL divergence between the distribution $Q(z)$ and $P(z \\mid x, \\beta)$. Remember the KL divergence is never less than 0, so the log likelihood will always be at least $\\mathcal{L}(Q , \\beta)$ (so $\\mathcal{L}(Q , \\beta)$ is a lower bound on the log likelihood). For later, note:\n",
      "\n",
      "$$\\mathcal{L}(Q , \\beta) =  {\\rm{\\ ln\\ }} P(x \\mid \\beta) - {\\rm{KL}}(Q || P)$$\n",
      "\n",
      "so $\\mathcal{L}(Q , \\beta)$ is maximized when ${\\rm{KL}}(Q || P)$ is 0."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "The distribution $Q$ is important here and in the variational inference version of the EM algorithm.\n",
      "\n",
      "In the EM algorithm, we have to evaluate the expectation of the complete data log likelihood with respec to $Q$, since\n",
      "\n",
      "$${\\mathcal{L}}(Q , \\beta) = E_Q \\left[{\\rm{ln\\ }} P(x, z \\mid \\beta)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(z)}\\right]$$\n",
      "\n",
      "The problem is that we often don't have the complete data log likelihood. We usually don't know the values of the $z_i$ variables. All of the information we have about them, though, can be quantified in the posterior for $z$:\n",
      "\n",
      "$$P(z \\mid x, \\beta)$$\n",
      "-->\n",
      "\n",
      "The classical EM algorithm, using this decomposition for the log likelihood, is:\n",
      "\n",
      "#### Algorithm for General EM using the log likelihood decomposition####\n",
      "\n",
      "\n",
      "1. Initialize model parameters $\\beta_c$\n",
      "2. E-step: holding $\\beta_c$ constant, maximize $\\mathcal{L}(Q, \\beta_c)$ with respect to $Q(Z)$ (note because of the decomposition, the maximizer will be exactly the posterior distribution $P(z \\mid x, \\beta)$.\n",
      "3. M-step: holding Q(Z) constant, maximize $\\mathcal{L}(Q, \\beta)$ with respect to $\\beta$. \n",
      "4. Check for convergence: \n",
      "\n",
      "    If ${\\rm{ln\\ }}P(x \\mid \\beta_{new}) -  {\\rm{ln\\ }}P(x \\mid \\beta_{c}) < \\epsilon_1$ \n",
      "\n",
      "    or $\\beta_{new} - \\beta_{c} < \\epsilon_2$\n",
      "\n",
      "    then stop. \n",
      "    \n",
      "    Else let $\\beta_c = \\beta_{new}$ and go back to step 2.\n",
      "\n",
      "\n",
      "\n",
      "<!--\n",
      "We get a value $\\theta_{new}$, which means $KL(Q || P)$ is now nonzero because Q is being held fixed under $\\theta_c$, and the new posterior distribution will be $P(Z \\mid X, \\theta_{new})$. So the log likelihood ${\\rm{\\ ln\\ }} P(X \\mid \\theta)$ will increase\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### What happens when the posterior distribution is intractable? ####\n",
      "\n",
      "In the E-step of the EM algorithm, the distribution $Q$ that maximized the lower bound on the log likelihood was the posterior distribution $P(z \\mid x, \\beta)$. \n",
      "\n",
      "Often, the posterior distribution is intractable.\n",
      "\n",
      "In variational inference, we introduce an approximating distribution $Q$ with its own set of (variational) parameters $\\lambda$. Then we minimize the KL divergence between $Q(z \\mid \\lambda)$ and the true posterior distribution $P(z \\mid x, \\beta$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The mean field assumption comes into play here. Since we are free to choose any distribution we like for $Q$, we will restrict the optimization problem by choosing a tractable family of distributions: factorized distributions.\n",
      "\n",
      "We assume $Q(z \\mid \\lambda) = \\prod\\limits_z Q(z_i \\mid \\lambda_i)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to optimize the variational parameteres $\\lambda$ to yield the closest possible distribution, within this factorized family, to the true posterior distribution. Surprisingly, this only changes the M step of the EM algorithm (Bishop 2006):\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}