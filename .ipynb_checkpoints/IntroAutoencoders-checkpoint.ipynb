{
 "metadata": {
  "name": "",
  "signature": "sha256:6174b2a884abd0ecf5c8f3b17788283552076f946398ee3c3d59f6fb03c82e8e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Autoencoders#\n",
      "\n",
      "Autoencoders are a class of feedforward neural networks that learns to model the input X with as little distortion as possible (Baldi, 2012). A traditional autoencoder consists of an input layer, a hidden layer, and an output layer, with directed connnections from input to hidden and from hidden to output (Figure 1, Baldi 2012). While this sounds a lot like a multilayer perceptron (MLP), an autoencoder has the same number of output units as input units, whereas an MLP has fewer output units. The point of an autoencoder is to reconstruct the input as faithfully as possible, whereas a multilayer perceptron is used, for example, for classification (so the output layer would have k nodes, where k is the number of classes). \n",
      "\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/AutoEncoderBaldi2012.png\" alt=\"\" style=\"width:300px\">\n",
      "\n",
      "<!--\n",
      "![alt text](https://github.com/caugusta/variational-inference/raw/master/AutoEncoderBaldi2012.png \"A typical autoencoder, with n inputs, p hidden units, and n outputs\"{width=100px)\n",
      "-->\n",
      "\n",
      "\n",
      "Autoencoders are used for unsupervised learning tasks such as feature extraction (which are encoded in the hidden layer), generating more unlabelled data that closely resembles the original data, etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The transformation from input to hidden units is denoted B, and the transformation from hidden to output is A. In the literature, it's common for the output layer to also be called the input layer (which is confusing and should be discouraged), since, in a perfect reconstruction, it would be exactly the same as the input layer.\n",
      "\n",
      "More formally (Baldi, 2012):\n",
      "\n",
      "An n/p/n autoencoder is defined by a tuple \n",
      "\n",
      "\n",
      "So an autoencoder consists of an encoder, taking the data and learning a more compact (or more sparse, depending on the structure) representation of the input, and a decoder, which goes from the representation back to the input. \n",
      "Another important aspect to autoencoders is that their learning paradigm is entirely unsupervised: that is, there is no target (per se)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Common autoencoders ###\n",
      "\n",
      "Linear autoencoders\n",
      "\n",
      "Relationship between Principal Components Analysis (PCA) and linear autoencoders\n",
      "\n",
      "Binary autoencoders\n",
      "\n",
      "Other types of input"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Denoising autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Contractive autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Sparse autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Stacked autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}