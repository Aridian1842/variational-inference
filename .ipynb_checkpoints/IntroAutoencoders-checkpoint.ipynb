{
 "metadata": {
  "name": "",
  "signature": "sha256:ffff2caf3241384ba91a6a47d0968fe1922d94e49a76faed76ff64aff9916fee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Autoencoders#\n",
      "\n",
      "Autoencoders are a class of feedforward neural networks that learns to model the input X with as little distortion as possible (Baldi, 2012). A traditional autoencoder consists of an input layer, a hidden layer, and an output layer, with directed connnections from input to hidden and from hidden to output (Figure 1). While this sounds a lot like a multilayer perceptron (MLP), an autoencoder has the same number of output units as input units, whereas an MLP has fewer output units. The point of an autoencoder is to reconstruct the input as faithfully as possible, whereas a multilayer perceptron is used, for example, for classification (so the output layer would have k nodes, where k is the number of classes). Munch of the notation in this iPython notebook follows Hugo Larochelle's videos. \n",
      "\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/AutoEncoderBaldi2012.png\" alt=\"\" style=\"width:300px\">\n",
      "\n",
      "<!--\n",
      "![alt text](https://github.com/caugusta/variational-inference/raw/master/AutoEncoderBaldi2012.png \"A typical autoencoder, with n inputs, p hidden units, and n outputs\"{width=100px)\n",
      "-->\n",
      "\n",
      "\n",
      "Autoencoders are used for unsupervised learning tasks such as feature extraction (which are encoded in the hidden layer), generating more unlabelled data that closely resembles the original data, etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The transformation from input to hidden units is denoted B, and the transformation from hidden to output is A. The transformation B is a weight matrix that takes the input layer and multiplies each input node by a constant. \n",
      "\n",
      "We often choose to tie the weights, although we don't have to. That is, we often make the weights from the input layer to the hidden layer equal to the weights from the hidden layer to the output layer (but transposed, of course). \n",
      "\n",
      "An autoencoder consists of an encoder, taking the data and learning a more compact (or more sparse, depending on the structure) representation of the input, and a decoder, which goes from the representation back to the input. \n",
      "Another important aspect to autoencoders is that their learning paradigm is entirely unsupervised: that is, there is no target (per se - you could also say the input is the target).\n",
      "\n",
      "An autoencoder consists of two main parts: an encoder and a decoder. The encoder is a mapping from input units to hidden units; the decoder goes from hidden units to \"output\" units. The encoder function is often a sigmoid nonlinearity; that is:\n",
      "\n",
      "$$h = \\frac{1}{1 + {\\rm{exp}}(-(b + Wx))}$$\n",
      "\n",
      "where $b$ is the bias vector from the input layer to the hidden layer, $W$ is a weight matrix, and $x$ is a $k$-dimensional input vector. \n",
      "\n",
      "The decoder function is also a sigmoid:\n",
      "\n",
      "$$\\bar{x} = \\frac{1}{1 + {\\rm{exp}}(-(c + W^{T}h))}$$\n",
      "\n",
      "where $c$ is the bias vector for the connections between the hidden layer and the output layer, and $W^{T}$ is the transpose of the weight matrix connecting the input layer and the hidden layer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Common autoencoders ###\n",
      "\n",
      "Linear autoencoders\n",
      "\n",
      "\n",
      "\n",
      "Relationship between Principal Components Analysis (PCA) and linear autoencoders\n",
      "\n",
      "Binary autoencoders\n",
      "\n",
      "Other types of input"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Denoising autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Contractive autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Sparse autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Stacked autoencoders ###"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}