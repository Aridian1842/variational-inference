{
 "metadata": {
  "name": "",
  "signature": "sha256:4bd0ffc97a673b766e66d85627e3f18898c90a24fed67ec34481659ea87b6c20"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#An Introduction to Mean-Field Variational Inference#"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Carolyn Augusta, caugusta@uoguelph.ca"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "------------\n",
      "## Outline ##\n",
      "\n",
      "1. A refresher on Bayesian statistical inference/ linear regression\n",
      "2. Why we need variational methods\n",
      "3. Mean field variational inference - algorithm and theory\n",
      "4. Mean field variational inference - example: Variational linear regression\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------\n",
      "### Bayesian Framework and Multiple Linear Regression ###\n",
      "\n",
      "Frequentist statisticians say there is one fixed value of a parameter, and all we have to do is find it via estimation methods like maximum likelihood and least squares. There may be some variation about our estimate (quantified in a confidence interval), but there is a single best value for each parameter. For example, in simple linear regression:\n",
      "    \n",
      "$$y_n = \\beta_0 + \\beta_1 x_{n, 1} + \\beta_2 x_{n, 2} + \\ldots + \\beta_p x_{n, N} + \\epsilon_n, \\hspace{0.2in} \\epsilon_n  \\stackrel{iid}{\\sim} N(0, \\sigma^2)$$\n",
      "\n",
      "$$\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}{\\bar{x}}$$\n",
      "\n",
      "$$\\hat{\\beta_1} = \\displaystyle{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}}$$\n",
      "\n",
      "Because $\\epsilon_i$ has a Normal distribution, so does $y_i$:\n",
      "\n",
      "$$Y \\sim N(\\beta_0 + \\beta_1X, \\sigma^2)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bayesian statisticians, by contrast, say that there is a distribution associated with each parameter. So now, in addition to Y having a Normal distribution, $\\beta_0$ and $\\beta_1$ have their own distributions. We can get a mean value for those distributions, and give a credible interval (a Bayesian version of a confidence interval). \n",
      "\n",
      "The distributions for $\\beta_0$ and $\\beta_1$ can be decided in a variety of ways, but here we'll assume, for simplicity, that they both come from Normal distributions. You'll see why later on. We'll also let $\\vec{\\beta} = (\\beta_0, \\beta_1)$ and talk about a distribution over $\\vec{\\beta}$\n",
      "\n",
      "$$\\vec{\\beta} \\sim N(\\mu_0, \\lambda)$$ \n",
      "\n",
      "where $\\lambda$ is the precision (remember the precision is the inverse of the variance)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "---------------\n",
      "### Conditional Probability and Bayes' Theorem ###\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The value of $y_i$ in the simple linear regression example is going to be dependent on the values of $\\beta_0$, $\\beta_1$, $x_i$, and $\\epsilon_i$. \n",
      "\n",
      "So we can express the probability that the variable $Y_i$ takes on value $y_i$ as:\n",
      "\n",
      "$$P(Y = y_i \\mid \\vec{\\beta}, x_i, \\epsilon_i)$$\n",
      "\n",
      "We often shorten this to:\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$P(Y \\mid \\beta)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember the rules of conditional probability:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$P(Y \\mid \\beta) = \\displaystyle{\\frac{P(Y, \\beta)}{P(\\beta)}}$$\n",
      "\n",
      "$$P(\\beta \\mid Y) = \\displaystyle{\\frac{P(Y, \\beta)}{P(Y)}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rearranging gives: $$P(Y, \\beta) = P(Y \\mid \\beta)P(\\beta)$$\n",
      "\n",
      "and similarly,\n",
      "\n",
      "\\begin{equation}\n",
      "P(Y, \\beta) = P(\\beta \\mid Y)P(Y)\n",
      "\\end{equation}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we can substitute to get Bayes' Theorem:\n",
      "\n",
      "\\begin{equation}\n",
      "P(\\beta \\mid Y) = \\displaystyle{\\frac{P(Y \\mid \\beta)P(\\beta)}{P(Y)}}\n",
      "\\end{equation}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where\n",
      "\n",
      "$P(\\beta \\mid Y)$ is the posterior distribution of $\\beta$ given the data Y\n",
      "\n",
      "$P(Y \\mid \\beta)$ is the likelihood of the data under model parameter values $\\beta$\n",
      "\n",
      "$P(\\beta)$ is the prior distribution of $\\beta$ (the distribution we assume $\\beta$ has before we see any data)\n",
      "\n",
      "$P(Y)$ is the normalization constant, and is the probability of the data regardless of the parameter values."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We get the normalization constant, $P(Y)$, by integrating the joint distribution of the data and the parameters over all values the parameters can take (marginalizing):\n",
      "\n",
      "$$P(Y) = \\int_\\beta P(Y \\mid \\beta)P(\\beta) d\\beta$$\n",
      "\n",
      "Bayesian inference is based on the posterior distribution, $P(\\beta \\mid Y)$. We're trying to find the best distribution for the parameters, after we've seen the data.\n",
      "\n",
      "####So the general idea of Bayesian inference is:####\n",
      "\n",
      "1) Start with some previously-known information about your parameters $\\beta$. Use this to construct the prior, $P(\\beta)$\n",
      "\n",
      "2) Posit a model for your data, and calculate the likelihood $P(Y \\mid \\beta)$\n",
      "\n",
      "3) Integrate (or sum, if your parameter distributions are discrete) the joint distribution $P(Y \\mid \\beta)P(\\beta)$ over all values $\\beta$ can take.\n",
      "\n",
      "4) Posterior = likelihood*prior/(normalization constant)\n",
      "\n",
      "5) Use the posterior to calculate, for example, the posterior mean and variance of $\\beta$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------\n",
      "###Why we need variational methods###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Have another look at the normalization constant:\n",
      "    \n",
      "$$P(Y) = \\int_\\beta P(Y \\mid \\beta)P(\\beta) d\\beta$$\n",
      "\n",
      "If the parameter space for $\\beta$ were very large - that is, if we had $\\beta = (\\beta_0, \\beta_1, \\ldots \\beta_{1,000,000}, \\ldots \\beta_n)$, then we would need to integrate over that very large space. This quickly becomes computationally intractable. \n",
      "\n",
      "If the normalization constant is intractable, then the posterior distribution $P(\\beta \\mid Y)$ is also intractable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two main methods right now to do Bayesian inference with \"big data\" models:\n",
      "\n",
      "1) Markov chain Monte Carlo (sampling from the intractable posterior distribution) - stochastic algorithm\n",
      "\n",
      "2) Variational Bayes (approximating the intractable posterior distribution with a known, nice function) - deterministic algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------------------------------------\n",
      "### Mean-field variational inference ###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In statistical physics, mean field theory has long been used to model complex systems using relatively simple ones. A huge assumption is made, though, that all of the variables are mutually independent. This is often a bad assumption, because in reality there are a lot of ways for the model to have dependencies. However, the independence assumption has some nice properties, so that'll be the focus of today's presentation.\n",
      "\n",
      "A hidden or latent variable in a statistical model is a variable that is not directly observed in the process of interest, but whose value or structure is inferred based on observed variables. The classic example comes from a mixture distribution. \n",
      "\n",
      "Briefly, a mixture distribution results when a random variable has more than one parent population. To take an example from R, say we're looking at the Old Faithful dataset. One column gives the duration of each eruption, and another gives the waiting time to an eruption (both in minutes). Using the example plot from R, we can see that shorter eruptions have shorter waiting time between eruptions, and the waiting time between larger eruptions is longer:\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/BishopVarLinReg.png\" alt=\"\" style=\"width:500px\">\n",
      "\n",
      "<!-- The dependence problem is even true in a multiple linear regression model: note there are are k coefficients and n observations. X is a nxk matrix, and beta is a k-vector\n",
      "-->\n",
      "<!--\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/BishopVarLinReg.png\" alt=\"\" style=\"width:500px\">\n",
      "-->\n",
      "<!--Above is a plate diagram of Bayesian simple linear regression. A plate diagram is a schematic for a model distribution, showing the conditional dependencies. In a plate model, a rectangle with a total (N) in the bottom right indicates that all variables or parameters inside the rectangle (the \"plate\") are indexed from 1 to N. So we have a pictoral representation of $t_n : n = 1, \\ldots, N$ and $\\phi_n : n = 1, \\ldots, N$. The notation for unobserved variables ($\\alpha$, $w$) in a plate diagram is an unfilled circle. Observed variables, in this case $t_n$, are filled circles. Outside the plate are variables or parameters that are not repeated n times. That is, $\\beta$ and $\\tau$ are outside the plate, which means their length is not necessarily $n$ (in fact, $\\beta = (\\beta_0, \\beta_1)$. \n",
      "-->\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "A plate model (above) is a schematic for a model distribution, showing the conditional dependencies. In a plate model, a rectangle with a total (n) in the bottom right indicates that all variables or parameters inside the rectangle (the \"plate\") are indexed from 1 to n. So we have a pictoral representation of $y_i : i = 1, \\ldots, n$ and $x_i : i = 1, \\ldots, n$. The notation for unobserved variables ($\\tau$, $\\beta$) in a plate diagram is an unfilled circle. Observed variables, in this case $y_i$, are filled circles. Outside the plate are variables or parameters that are not repeated n times. That is, $\\beta$ and $\\tau$ are outside the plate, which means their length is not necessarily $n$ (in fact, $\\beta = (\\beta_0, \\beta_1)$. \n",
      "\n",
      "-->\n",
      "\n",
      "The arrows between components in the plate model show how each variable or parameter is related. For example, the arrow from $\\tau$ to $\\beta$ indicates that the value of $\\beta$ dependes on the value of $\\tau$. Taken as a whole, the plate model shows that the joint (model) distribution $P(y, \\beta, \\tau)$, and illustrates the dependencies in the model:\n",
      "\n",
      "$$P(y, \\beta, \\tau) = P(\\tau)P(\\beta \\mid \\tau) \\prod_{i=1}^{n} P(y_i \\mid \\beta)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In our linear regression example, we want to know the joint posterior distribution $P(\\beta, \\tau \\mid y)$. From conditional probability:\n",
      "\n",
      "$$P(\\beta, \\tau \\mid y)  = \\displaystyle{\\frac{P(y, \\beta, \\tau)}{P(y)}}$$\n",
      "                                       \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And so:\n",
      "    \n",
      "$$P(\\beta, \\tau \\mid y) = \\displaystyle{\\frac{P(\\tau)P(\\beta \\mid \\tau) \\prod_{i=1}^{n} P(y_i \\mid \\beta)}{P(y)}}$$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to specify a prior distribution for $\\tau$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}