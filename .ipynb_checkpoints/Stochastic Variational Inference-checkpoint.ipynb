{
 "metadata": {
  "name": "",
  "signature": "sha256:c081d79ea83ff23f23feed1eed983d4a71140263456bf2aebe68e2749ea75034"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Variational and Stochastic Variational Inference\n",
      "\n",
      "This tutorial-style introduction is geared toward researchers with an introductory undergraduate knowledge of statistics and a working knowledge of neural networks in particular. Since I am a statistician, I will gloss over some of the more computer science-related topics. However, I would gladly accept any suggestions for improvement, especially from a computer science/engineering perspective. Please send constructive criticism to caugusta at uoguelph dot ca\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Outline**\n",
      "\n",
      "Motivating variational inference\n",
      "\n",
      "Discussing vanilla variational inference, with examples\n",
      "\n",
      "Extension to stochastic variational inference, with examples\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " --------------------------------------\n",
      "\n",
      " ## Motivating variational inference ##\n",
      "\n",
      "Following the notation from Bishop 1998, suppose we have a neural network composed of hidden units H and visible units V. There will also be model parameters w.\n",
      "\n",
      "![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\")\n",
      "\n",
      " Using the notation from Bishop 1998:\n",
      " We have visible units V, hidden units H, model parameters w.\n",
      " The model is defined by the distribution P(H, V|w), that is, \n",
      " the joint distribution of the hidden and visible units given the model parameters.\n",
      "\n",
      " The likelihood of the data given the model parameters is the probability of seeing a particular configuration\n",
      " of visible units under ANY setting of the hidden units, given a particular set of model parameters\n",
      " (sum over all H) P(H, V| w) = P(V|w) [eq. 1]\n",
      "  This process of summing over unwanted variables is called marginalization. \n",
      " If the variables were instead continuous, this would be an integration instead of a sum.\n",
      " We use the model distribution and the likelihood to construct the posterior distribution:\n",
      " P(H|V, w) = P(H, V|w)/P(V|w)\n",
      " That is, the probability of a particular setting of the hidden units, given fixed visible units and model parameters,\n",
      " is the joint probability of the hidden and visible units given the model parameters, divided by the probability that \n",
      " we see a particular setting of the visible units given the model parameters.\n",
      " A simpler version of the previous equation is P(B|A) = P(B, A)/P(A), which is exactly the formula for conditional probability, \n",
      " as seen in many second-year statistics courses.\n",
      " In a lot of practical problems, the posterior distribution is intractable \n",
      " (the sum over all H that we need to do to get the denominator in equation ,\n",
      " but we need to make inferences based on the posterior.\n",
      " If we want to know, for example, the expected value of a particular hidden variable, we have to marginalize \n",
      " over the others in the posterior\n",
      " One solution: MCMC - sampling from the posterior\n",
      " Another solution: Variational methods - approximating the posterior by some known distribution\n",
      "\n",
      " Variational inference\n",
      "\n",
      " Mean-field variational inference\n",
      "  Assume the hidden units are independent of one another (big assumption, often untrue)\n",
      "\n",
      "\n",
      "Building up stochastic variational inference from vanilla variational inference\n",
      "Start from Bishop 1998.\n",
      "Add from Bishop 2006, with my proofs.\n",
      "Add from Hoffman et al 2013, applied to Bishop 1998 examples, and compared with vanilla variational inference.\n",
      "How can Neural Variational Inference be added to this?\n",
      "Apply to a dynamic model, structured similarly to Epidemic Thresholds on Dynamic Contact Networks work.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " **References**: \n",
      " \n",
      " Bishop, C. Pattern Recognition and Machine Learning. Springer, 2006 (Chapter 10)\n",
      " \n",
      " Hoffman, M. D. et al. Stochastic Variational Inference. Journal of Machine Learning Research, vol. 14. (2013), pp 1303-1347.\n",
      " \n",
      " Bishop, C. Variational Learning in Graphical Models and Neural Networks. 1998\n",
      " \n",
      " Mnih, A. and Gregor, K. Neural Variational Inference and Learning in Belief Networks. 31st ICML, 2014; JMLR vol 32.\n",
      " \n",
      " Volz, E. and Meyers L.A. Epidemic thresholds in dynamic contact networks. J.R. Soc. Interface (2009) vol 6, 233-241."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}