{
 "metadata": {
  "name": "",
  "signature": "sha256:b839db25516789ad849570377f8562de79e70a6e8e1ac1f423127d9f1cf18a0d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#An Introduction to Mean-Field Variational Inference#"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Carolyn Augusta, caugusta@uoguelph.ca"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "------------\n",
      "## Outline ##\n",
      "\n",
      "1. Frequentist vs Bayesian inference\n",
      "2. Conditional probability and Bayes' Theorem\n",
      "3. Why we need variational methods\n",
      "4. Mean field variational inference - theory\n",
      "5. Mean field variational inference - example: Variational clustering\n",
      "6. Conclusion\n",
      "7. References\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "###1. Frequentist vs Bayesian philosophy###\n",
      "\n",
      "Frequentist and Bayesian statisticians vary in philosophy, but are both trying to do basically the same thing: assign a probability to a chance event. This xkcd comic is a nice illustration of the concept:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://imgs.xkcd.com/comics/frequentists_vs_bayesians.png\" alt=\"\" style=\"width:500px\">\n",
      "\n",
      "(Note: there is a lot of debate about this comic, [particularly here](http://stats.stackexchange.com/questions/43339/whats-wrong-with-xkcds-frequentists-vs-bayesians-comic), but it's early and coffee and comics go together!) Also, my summary of frequentist vs Bayesian is missing some details, but I just want to communicate the general idea.\n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Frequentist interpretation:####\n",
      "\n",
      "If I were to repeat the sun going nova many times, then I could get a probability of the sun going nova now. I could compare that probability with the probability that the machine is lying, and conclude, based on a p-value, which is the more likely scenario. Because my p-value (0.027) is less than my significance level (0.05), I think it's really unlikely that the machine is lying. I reject the null hypothesis that the sun has not gone nova. (Note one of the arguments about this comic: it's not clear what the null hypothesis is here - is it that the machine is not lying, or that the sun has not gone nova?).\n",
      "\n",
      "Frequentists rely on frequency of events over long periods of time to calculate probabilities. They assume parameters are fixed, and data are random, from some population.\n",
      "\n",
      "####Bayesian interpretation:####\n",
      "\n",
      "The sun going nova is not a repeatable event, so I need a different way to assign a probability to the sun going nova. I'll use common sense (prior knowledge that the sun won't die for another 5 billion years or so), and I'll look at the data I have (that the machine said yes) and since my prior knowledge is more trustworthy than my data, I'll conclude that the sun has not gone nova.\n",
      "\n",
      "Bayesians rely on prior knowledge to quantify the probability of an event before seeing any data. They assume parameters are random variables (with probability distributions) and that data are fixed, from some population.\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2. Conditional probability and Bayes' Theorem###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The rest of this talk will be on Bayesian inference, which is based on Bayes' Theorem. As a reminder of what's going on, have a look at the derivation of Bayes' Theorem from conditional probability:\n",
      "\n",
      "Recall:\n",
      "\n",
      "$$P(Z \\mid X) = \\displaystyle{\\frac{P(X, Z)}{P(X)}}$$\n",
      "\n",
      "$$P(X \\mid Z) = \\displaystyle{\\frac{P(X, Z)}{P(Z)}}$$\n",
      "\n",
      "Rearranging gives: $$P(X, Z) = P(X \\mid Z)P(Z)$$\n",
      "\n",
      "and similarly,\n",
      "\n",
      "\\begin{equation}\n",
      "P(X, Z) = P(Z \\mid X)P(X)\n",
      "\\end{equation}\n",
      "\n",
      "So we can substitute to get Bayes' Theorem:\n",
      "\n",
      "\\begin{equation}\n",
      "P(Z \\mid X) = \\displaystyle{\\frac{P(X \\mid Z)P(Z)}{P(X)}}\n",
      "\\end{equation}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where\n",
      "\n",
      "$P(Z \\mid X)$ is the posterior distribution of the hidden variables $Z$ given the data $X$ (more on hidden variables later)\n",
      "\n",
      "$P(X \\mid Z)$ is the likelihood of the data under hidden variable values $Z$\n",
      "\n",
      "$P(Z)$ is the prior distribution of $Z$ (the distribution we assume the hidden variables $Z$ have before we see any data)\n",
      "\n",
      "$P(X)$ is the normalization constant, and is the probability of the data regardless of the hidden variables' values.\n",
      "\n",
      "We get the normalization constant, $P(X)$, by integrating the joint distribution of the data and the parameters over all values the parameters can take (marginalizing):\n",
      "\n",
      "$$P(X) = \\int_Z P(X \\mid Z)P(Z) dZ$$\n",
      "\n",
      "Bayesian inference is based on the posterior distribution, $P(Z \\mid X)$. We're trying to find the best distributions for the hidden variables, after we observe the data. \n",
      "\n",
      "An important concept we're going to use is that of a conjugate prior. The posterior and prior distributions are said to be conjugate when they come from the same family of distributions. That's going to be useful in the example later on.\n",
      "\n",
      "####So the general idea of Bayesian inference is:####\n",
      "\n",
      "1) Start with some previously-known information about your hidden variables $Z$. Use this to construct the prior, $P(Z)$\n",
      "\n",
      "2) Posit a model for your data, and calculate the likelihood $P(X \\mid Z)$\n",
      "\n",
      "3) Integrate (or sum, if your hidden variable distributions are discrete) the joint distribution $P(X, Z) = P(X \\mid Z)P(Z)$ over all values each $Z$ can take. This is the normalization constant.\n",
      "\n",
      "4) Posterior = likelihood*prior/(normalization constant)\n",
      "\n",
      "5) Use the posterior to make inferences about the hidden variables, $Z$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------\n",
      "###3. Why we need variational methods###\n",
      "\n",
      "Have another look at the normalization constant:\n",
      "    \n",
      "$$P(X) = \\int_Z P(X \\mid Z)P(Z) dZ$$\n",
      "\n",
      "If we have a lot of hidden variables, we would need to integrate over that very large space. This quickly becomes computationally intractable. \n",
      "\n",
      "If the normalization constant is intractable, then the posterior distribution $P(Z \\mid X)$ is also intractable.\n",
      "\n",
      "There are two main classes of methods right now to do Bayesian inference when the normalization constant is intractable:\n",
      "\n",
      "1) Markov chain Monte Carlo (sampling from the intractable posterior distribution) - stochastic algorithm\n",
      "\n",
      "2) Variational Bayes (approximating the intractable posterior distribution with a known, nice function) - deterministic algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---------------------------------------\n",
      "### 4. Mean-field variational inference - theory###\n",
      "\n",
      "\n",
      "#### Hidden (latent) variables and mixture distributions ####\n",
      "\n",
      "A hidden or latent variable in a statistical model is a variable that is not directly observed in the process of interest, but whose value is inferred based on observed variables. The classic example comes from a mixture distribution. \n",
      "\n",
      "A mixture distribution results when a random variable could come from one of several subpopulations, or clusters. Say we have $n$ observations from a random variable $X$ (that is, we have $x_1, \\ldots, x_n$). Then each $x_i$ belongs to some cluster. We want to know which cluster $x_i$ came from, but that information is hidden. We only have the observations themselves, not the cluster labels. \n",
      "\n",
      "To take an example from R, say we're looking at the Old Faithful dataset. We know the duration of each eruption, and the waiting time to an eruption (both in minutes). Using the example plot from R, we can see that the data follow a bimodal distribution (so you could say there are two clusters):\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/OldFaithful.png\" alt=\"\" style=\"width:500px\">\n",
      "\n",
      "We could model this process as a mixture distribution with two distributions (clusters): $g_s(X) = N(\\mu_1, \\sigma_1^2)$ and $g_\\ell(X) = N(\\mu_2, \\sigma_2^2)$, where $s$ is for short wait time and $\\ell$ is for long wait time.\n",
      "\n",
      "Suppose we have a sample of eruptions $\\boldsymbol{x} = \\{x_1, \\ldots, x_n\\}$. \n",
      "\n",
      "<!--\n",
      "and we observe their wait times $\\boldsymbol{y} = \\{y_1, \\ldots y_n\\}$. \n",
      "\n",
      "-->\n",
      "\n",
      "We assume there's an unobserved (hidden) cluster label $Z_i = \\{{\\rm{Short, Long}}\\}$, with one realization per wait time, $\\boldsymbol{z} = \\{z_1, \\ldots z_n\\}$.\n",
      "\n",
      "The distribution of cluster labels is Bernoulli$(\\pi)$ (since it can only be 1 of 2 possible clusters - if we had more clusters, we would use a Categorical distribution).\n",
      "\n",
      "\n",
      "$$P(Z) = \\prod_{i=1}^{n} P(Z_i \\mid \\pi) = \\prod_{i=1}^{n} \\pi^{Z_i} (1 - \\pi)^{1 - Z_i}$$\n",
      "\n",
      "where\n",
      "\n",
      "\\begin{equation}\n",
      "    Z_i = \n",
      "    \\begin{cases}\n",
      "      1, & {\\rm{if\\ }} x_i {\\rm{\\ is\\ Short}}\\  \\\\\n",
      "      0, & \\text{otherwise}\n",
      "    \\end{cases}\n",
      "  \\end{equation}\n",
      "\n",
      "and $$P(Z_i = {\\rm{Short}}) = \\pi$$\n",
      "\n",
      "\n",
      "\n",
      "The probability that we'd get a wait time from the first cluster is proportional to the area under the left hand curve. Similar for the right handed curve. The distribution of wait times (regardless of the cluster labels) can be represented as a mixture of normal distributions:\n",
      "\n",
      "$$P(X) = P(X_1=x_1, X_2 = x_2, \\ldots X_n = x_n) = \\prod_{i=1}^{n} \\left( \\pi g_s(x_i) + (1 - \\pi) g_\\ell(x_i) \\right)$$\n",
      "\n",
      "We also know the likelihood, since:\n",
      "\n",
      "$$X_i \\mid (Z_i = 1) \\sim g_s (x_i)$$\n",
      "\n",
      "$$X_i \\mid (Z_i = 0) \\sim g_\\ell (x_i)$$\n",
      "\n",
      "$$P(X \\mid Z) = \\prod_{i=1}^{n} \\left(g_s(x_i)^{z_i} + g_\\ell(x_i)^{1 - z_i} \\right)$$\n",
      "\n",
      "\n",
      "So we have the normalization constant $P(X)$, the likelihood $P(X \\mid Z)$, and the prior $P(Z)$. The cluster label $z_i$ is hidden. If we wanted to infer it based on the data we have, then we could use variational inference (although the posterior distribution $P(Z \\mid X)$ in this case is tractable, so we could just use Bayes' Theorem).\n",
      "\n",
      "\n",
      "\n",
      "<!--\n",
      "The only thing is, these distributions clearly overlap! If we draw $x = 68$ from this distribution, it's not clear whether $x$ came from the first subpopulation or the second. So the identity of the distribution from which $x$ came is a hidden variable.\n",
      "\n",
      "-->\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The Mean Field Assumption ####\n",
      "\n",
      "In statistical physics, mean field theory has long been used to model complex systems using relatively simple ones. A huge assumption is made, though, that all of the hidden variables $Z_1, Z_2, \\ldots, Z_n$ are mutually independent. This is often a bad assumption, because in reality there are a lot of ways for the model to have dependencies. However, the mean field (independence) assumption has some nice properties, so that'll be the focus of today's presentation.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "Remember, the posterior distribution $P(Z \\mid X)$ is intractable because the normalizing constant, $P(X)$ is intractable. We want to approximate this intractable posterior distribution with some known, nice function $Q$ (called the \"variational distribution\"). So:\n",
      "\n",
      "$$P(Z \\mid X) \\approx Q(Z)$$\n",
      "\n",
      "Using the mean field assumption, we'll assume Q is a factorized distribution. That is, we'll assume:\n",
      "\n",
      "$$Q(Z) = \\prod_{i=1}^n Q_i(Z_i)$$\n",
      "\n",
      "We have the likelihood (from the data and the model) and the prior (because we can always specify one). So the only thing to cover now is the normalization constant, $P(X)$. \n",
      "-->\n",
      "\n",
      "####The lower bound on the normalization constant ####\n",
      "\n",
      "\n",
      "To get to the objective function in mean-field variational inference, we need to minimize the distance between an approximating distribution $Q$ (called the \"variational distribution\") and the true posterior distribution $P(Z \\mid X)$. To do this, we need to get an appropriate distance metric. The distance metric we'll use is based on maximizing the log probability of the data regardless of the hidden variables (the log normalization constant, ${\\rm{ln\\ }} P(X)$). Remember any time a statistician says log, they usually mean ln. We can get a lower bound on the normalization constant in the following way:\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(X) &= {\\rm{ln}} \\int_Z P(X \\mid Z)P(Z)\\ dZ \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\int_Z P(X, Z)\\ dZ  {\\hspace{0.5in} \\rm {\\ by\\ conditional\\ probability}} \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\int_Z \\frac{P(X, Z)}{Q(Z \\mid X)}Q(Z \\mid X)\\ dZ \\\\[0.5em]\n",
      "&= {\\rm{ln\\ }} E_Q \\left[\\frac{P(X, Z)}{Q(Z \\mid X)} \\right] {\\hspace{0.5in} \\rm{\\, since\\ E_X(g(X)) = \\int_x g(x)f(x) dx,\\ by\\ definition,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(Z)}}\\\\[0.5em]\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[\\frac{P(X, Z)}{Q(Z \\mid X)} \\right] {\\hspace{0.5in} \\rm{\\, by\\ Jensen's\\ Inequality}} \\\\[0.5em]\n",
      "&= E_Q \\left[{\\rm{ln\\ }} P(X, Z)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(Z \\mid X)}\\right] \\\\[0.5em]\n",
      "&= {\\mathcal{L}}(Q) \\\\\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first expected value term here is the expected value of the log joint distribution, which is our model distribution. The second expected value term is the entropy of $Q$. The two terms together constitute a lower bound on the normalization constant. This lower bound, denoted $\\mathcal{L}(Q)$ is also called the \"expectation lower bound\", or ELBO. Also note, when I write $Q$, or $Q(Z)$, I mean $Q(Z \\mid X)$, it's just nicer notation.\n",
      "\n",
      "Aside: Jensen's Inequality applied here is ${\\rm{ln}}(E(X)) \\ge E({\\rm{ln}} (X))$, and more information is available [here](http://en.wikipedia.org/wiki/Jensen's_inequality). The vast majority of this derivation is given in Bishop (1998) and again in Hoffman (2013)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####The objective function ####\n",
      "\n",
      "In variational inference, we want to approximate that intractable posterior distribution by some known, nice function $Q$. To find the optimal $Q$, we need to define a relationship between the intractable log normalization constant ${\\rm{ln\\ }} P(X)$ and $Q$, and get an objective function based on that relationship.\n",
      "\n",
      "The mean-field assumption leads to a rather nice result, namely that we can use that ELBO $\\mathcal{L} (Q)$ (lower bound on the log probability of the data) in the following way:\n",
      "\n",
      "$${\\rm{ln}}\\ P(X) = {\\rm{\\mathcal{L}}}(Q) + {\\rm{KL}}(Q \\ \\| \\ P)$$\n",
      "\n",
      "where ${\\rm{\\mathcal{L}}}(Q)$ is the ELBO, and ${\\rm{KL}}(Q \\ \\| \\ P)$ is the KL (Kullback-Leibler) divergence between the approximating distribution $Q$ and the true posterior distribution $P(Z \\mid X)$. \n",
      "\n",
      "(Aside: Remember the KL divergence is a measure of the difference between two probability distributions. It's not the same as a distance function, because it isn't symmetric (but you can define it to be - that's done in stochastic variational inference, but not here). It is defined (for continuous distributions) as:\n",
      "\n",
      "$${\\rm{KL}}(Q(Z \\mid X) \\ \\| \\ P(Z\\mid X)) = - \\int_{Z} Q(Z \\mid X) {\\rm\\ ln}\\left[\\frac{P(Z \\mid X)}{Q(Z \\mid X)}\\right] dZ$$\n",
      "\n",
      "Or, in simplified notation:\n",
      "\n",
      "$${\\rm{KL}}(Q \\ \\| \\ P) = - \\int_Z Q {\\rm\\ ln}\\left[\\frac{P}{Q}\\right] dZ$$\n",
      "\n",
      "This relationship can be derived by expanding the KL divergence, and expanding the ELBO, then adding them together, cancelling out some terms and remembering that Q is a probability density function (which means the integral of $Q$ over its domain is equal to 1). I can send you the proof if you like, or you can find most of it in Bishop (2006).)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So maximizing the ELBO $\\mathcal{L}(Q)$, with respect to $Q$ is the same as minimizing the KL divergence between the variational distribution $Q$ and the true posterior distribution $P(Z \\mid X)$. This is more easily seen through a picture (from Bishop 1998):\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/KLpicture.png\" alt=\"Illustration of the KL divergence minimization being equivalent to maximization of the lower bound\" style=\"width:250px\">\n",
      "\n",
      "In this diagram, ${\\rm{ln\\ }} P(V \\mid w)$ is the equivalent of our ${\\rm {ln\\ }} P(X)$. Note that we can't compute the KL divergence here, since we can't compute the posterior $P(Z \\mid X)$. Instead, we focus on maximizing the ELBO:\n",
      "\n",
      "$$\\mathcal{L}(Q) = E_Q \\left[{\\rm{ln\\ }} P(X, Z)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(Z)}\\right]$$\n",
      "\n",
      "#### Choosing Q ####\n",
      "Recall the minimum value of the KL divergence metric is 0. If we could choose any $Q$ we like, we would choose it such that $Q = P(Z \\mid X)$, which would give a KL divergence of 0. That would mean, though, that we knew the form of the posterior distribution, which we don't. So instead, we choose a \"best\" $Q$ out of a particular family of functions. Corresponding to the mean-field assumption, we choose the following factorized form for the variational distribution:\n",
      "\n",
      "$$ Q(Z) = \\prod_{i=1}^n Q_i (Z_{i}) $$\n",
      "\n",
      "So there will be $n$ functions $Q_i$ corresponding to the $n$ different $Z_i$ hidden variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Maximizing the ELBO ####\n",
      "\n",
      "Finally, we have our objective function:\n",
      "    \n",
      "$$\\mathcal{L}(Q) = E_Q \\left[{\\rm{ln\\ }} P(X, Z)\\right] - E_Q \\left[{\\rm{ln\\ }} {\\prod_{i=1}^{n} Q_i(Z_i)}\\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to maximize $\\mathcal{L}(Q)$ with respect to each function $Q_i(Z_i)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final result is:\n",
      "    \n",
      "$$Q^{*}_j(Z_j)= \\displaystyle{\\frac{{\\rm{exp}}\\left\\{ E_{Q_{i \\ne j}} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\}}{\\int_{Z_j} \\rm{exp}\\left\\{ E_{Q_{i \\ne j}} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\} d{Z_j}}}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So to arrive at a best distribution for all $Z_j,\\ j = 1, \\ldots n$, we need to initialize all of the factors $Q_i(Z_i), i \\ne j$, then calculate the expectation of $Z_j$ with respect to the other factors. We then update our $Q_j(Z_j)$, and move to another distribution, holding all the others constant. In this way, we will arrive at an optimal distribution for $Q$, since the bound is convex with respect to each of the $Q_i(Z_i)$ distributions (Bishop, 2006).\n",
      "\n",
      "####The complete algorithm is:####\n",
      "\n",
      "1. Initialize the parameters of all of the variational distributions $Q_j(Z_j),\\ j = 1, \\ldots, n$\n",
      "2. For j in 1:n\n",
      "    $$Q^{new}_j(Z_j) = \\displaystyle{\\frac{{\\rm{exp}}\\left\\{ E_{Q_{i \\ne j}} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\}}{\\int_{Z_j} \\rm{exp}\\left\\{ E_{Q_{i \\ne j}} \\left[ {\\rm{\\ln\\ }} P(X, Z) \\right]\\right\\} d{Z_j}}}$$\n",
      "    \n",
      "3. Iterate until convergence.\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###5. Mean field variational inference - example: Variational Gaussian Mixture Model###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This example is taken directly, verbatim from http://www.bayespy.org/examples/gmm.html. All of the mathematical derivations are available in section 10.2 of Bishop (2006).\n",
      "\n",
      "The Python 3.0 module BayesPy allows variational Bayes procedures to be implemented on linear regression, Gaussian mixture models, Bernoulli mixture models, Hidden Markov models, and more. Unfortunately the authors have not seen fit to provide a Python 2.7 implementation.\n",
      "\n",
      "Remember from the Old Faithful example that we had 2 component distributions in our mixture distribution. Here, we'll instead have 4 component distributions, (called clusters) to make the problem a little more interesting:\n",
      "\n",
      "<!--\n",
      "Recall the parameters of a Normal distribution are $\\mu$ and $\\sigma^2$. Just like the Normal distribution, each variational distribution will have some associated parameters. Arriving at an optimal distribution for each $Q_j(Z_j)$ means finding the best settings of the variational parameters for each distribution. \n",
      "\n",
      "In Bayesian multiple linear regression, we have the following model:\n",
      "\n",
      "$$y_i = \\beta_0 + \\beta_1 x_{i, 1} + \\beta_2x_{i, 2} + \\cdots + \\beta_k x_{i,k} + \\epsilon_i, \\hspace{0.5in} i = \\{1, \\ldots, n\\}$$\n",
      "\n",
      "$$\\epsilon_i \\sim N(0, \\tau),$$ \n",
      "\n",
      "$$\\beta \\sim N(0, \\alpha),$$ \n",
      "\n",
      "$$\\alpha \\sim Gamma(a_0, b_0)$$\n",
      "-->\n",
      "\n",
      "<!--\n",
      "where:\n",
      "\n",
      "the $n$-vector $y$ is the dependent variable (observed variable)\n",
      "\n",
      "the $n x k$ matrix $X$ is the design matrix\n",
      "\n",
      "the $k$-vector $\\beta$ is a vector of regression coefficients (hidden parameter)\n",
      "\n",
      "the $n$ vector $\\epsilon$ is an error (noise) term with distribution $N(0, \\tau)$\n",
      "\n",
      "$\\tau$ is the precision (inverse of the covariance matrix) of the distribution of the error term (hidden parameter)\n",
      "\n",
      "$\\alpha$ is the precision of the distribution on the regression coefficients $\\beta$ (hidden parameter)\n",
      "\n",
      "Remember in a Bayesian treatment, parameters have distributions, and are considered random variables.\n",
      "-->\n",
      "\n",
      "<!--\n",
      "If you have Python 3.0, you can use a very nice new package called BayesPy, which also does Variational inference on a Gaussian mixture model (follow the example [here](http://www.bayespy.org/examples/gmm.html)). Since I have Python 2.7, I'm going to be doing things a little differently.\n",
      "\n",
      "Say we have data from a Gaussian process, and we can plot the data. In this very simple example, we'll assume that the number fo clusters is known a priori, but of course you can always treat the number of clusters as a hidden variable and infer it in a similar way to the ohter hidden variables in the model. In this example, there are 300 observations in each cluster:\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/MyData.png\" alt=\"\" style=\"width:600px\">\n",
      "\n",
      "and we know the true value of the means is [2,2] and [10,10], and that each cluster has the 2x2 identity covariance matrix. Say we didn't know this, though. All we had were the raw data, and we didn't know the means and covariance matrices. We could use mean field variational inference to infer them, as well as the cluster label for each data point.\n",
      "-->"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This will work regardless of your Python version\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#Get 50 samples from each of 4 separate 2D Multivariate Normal distributions\n",
      "#x0 = np.random.multivariate_normal([0, 0], [[2, 0], [0, 0.1]], size=50)\n",
      "#x1 = np.random.multivariate_normal([0, 0], [[0.1, 0], [0, 2]], size=50)\n",
      "#x2 = np.random.multivariate_normal([2, 2], [[2, -1.5], [-1.5, 2]], size=50)\n",
      "#x3 = np.random.multivariate_normal([-2, -2], [[0.5, 0], [0, 0.5]], size=50)\n",
      "\n",
      "\n",
      "#concatenate all of the observations together\n",
      "obs = np.vstack([x0, x1, x2, x3]) #this version is used in the online example\n",
      "obsplot = np.r_[x0, x1, x2, x3]  #this version is for my plot. The only difference is formatting.\n",
      "\n",
      "#and plot them\n",
      "plt.plot(obs[:,0][0:49], obs[:,1][0:49], 'ro', obs[:,0][50:99], obs[:,1][50:99], 'bo', obs[:,0][100:149], obs[:,1][100:149], 'go', obs[:,0][150:199], obs[:,1][150:199], 'mo',)\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('y')\n",
      "plt.axis([-3, 6, -3, 5])\n",
      "plt.title('Example data from four MVN distributions')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEZCAYAAAB/6SUgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXtcVOX2/z8DqKAighdAUdFRv94RMy+VMlkwFVFqVmre\npTqdk6Cn70lTCbx7tDoHSDuV99Tqa7/UYogwDdDKzCNpZFcUFRSviKDceX5/4Axz2Xtmz8ye2Xtm\n1ruXr2b2PHvvNXs2n+fZa61nPQrGGANBEAThUXhJbQBBEAThfEj8CYIgPBASf4IgCA+ExJ8gCMID\nIfEnCILwQEj8CYIgPBASfw9h27ZtGDNmjCjHSklJwfTp00U5lhBmz56NoKAgjBo1ymnn1FJVVYW4\nuDi0b98ezz77rNPPLyVFRUXw8vJCY2MjAOCxxx7DBx98IMqxDx8+jH79+uneh4eH4+DBg6IcGwAG\nDRqEvLw80Y7njpD4i0B4eDhat24Nf39/3b+EhASpzXIYCoVCcNtZs2YhKSnJ5nMdPnwYX331FS5e\nvIijR4/afBxb+eSTT3DlyhXcuHEDH3/8scPPN2vWLHh5eeGzzz4z2L5gwQJ4eXlh+/btOHr0KNq2\nbYvbt2+b7B8ZGYmNGzfqhDs2Ntbg82nTpmHZsmU22ZaZmSmo0/fy8sKZM2fMthkzZgx+/fVX3XuF\nQmHVfaUP1z1WUFCAsWPH2nQ8T4HEXwQUCgUyMjJQUVGh+5eWlia1WW7BuXPnEB4eDl9fX87P6+vr\nHX7+vn37wstL/D+VhoYGk20KhQJ9+/bFjh07dNvq6+vxf//3f+jduzcUCgVGjRqFsLAwfPLJJwb7\nFhQU4JdffsGUKVN0244dO4bvvvvO4Pi2iqw1mJs76ujfjBAGib+DeemllzBp0iTd+4ULF+Lhhx8G\nAJSVleHxxx9H586dERQUhLi4OJSUlOjaqlQqJCUl4f7774e/vz+eeOIJXLt2Dc899xwCAgIwYsQI\nnDt3Ttfey8sL6enpUCqV6NSpE1599VXeP8Jff/0V0dHR6NChA/r164c9e/bwfoezZ88iKioK7dq1\nQ0xMDK5du2bw+dNPP43Q0FC0b98eUVFROH36NADgvffew+7du7Fu3Tr4+/vjySefBACsXbsWvXv3\nRrt27TBw4EDs27eP87ybN2/G888/j++++w7+/v5YtmwZcnJyEBYWhnXr1iE0NBRz585FbW0t5s+f\nj65du6Jr165YsGABamtrAUDXfv369ejcuTO6dOmCffv2ITMzE3379kWHDh2wdu1azvMnJydjxYoV\n+Pjjj+Hv74+tW7eCMYaVK1ciPDwcwcHBmDlzJm7duqU7V7du3QyOER4ejkOHDgFocpdNmjQJ06dP\nR0BAALZv38553ri4OBw5cgQ3b94EAGRlZSEiIgLBwcG6NjNnzjToIABgx44diI2NRWBgoG7bq6++\niiVLlhi047snGhsb8b//+7/o1KkTlEolNBqNwecqlQqbN28GAPz555+IiopC+/bt0alTJ12Hox1t\nR0REwN/fH3v27OH8zbiu1bFjxzBw4EAEBQVhzpw5qKmpAcDtsvTy8kJhYSHvPabvRqqpqbF4f7z1\n1lsIDg5Gly5dsG3bNt15MjMzMXDgQLRr1w5hYWF48803Oa+dS8IIuwkPD2dfffUV52d37txhffv2\nZdu2bWN5eXmsY8eOrKSkhDHG2PXr19mnn37KqqqqWEVFBXv66afZ+PHjdftGRUWxPn36sDNnzrDy\n8nI2YMAA1rt3b3bw4EFWX1/PZsyYwWbPnq1rr1Ao2Lhx41hZWRk7f/4869u3L9u0aRNjjLGtW7ey\nBx54gDHGWGVlJQsLC2Pbtm1jDQ0NLD8/n3Xs2JGdPn2a8zuMGjWKvfLKK6y2tpbl5eUxf39/Nn36\ndN3nW7duZZWVlay2tpbNnz+fDR06VPfZrFmzWFJSksHx9uzZwy5dusQYY+zjjz9mbdq00b03Ztu2\nbTq7GWPs66+/Zj4+PmzRokWstraWVVVVsaSkJDZ69Gh29epVdvXqVXbffffpzqltv2LFClZfX8/e\nf/991qFDBzZ16lRWWVnJfv75Z+bn58eKioo4z5+SkmLwXTdv3sx69+7Nzp49yyorK9nEiRN1n3/9\n9dcsLCzMYP/w8HB28OBBxhhjycnJrEWLFmz//v2MMcaqqqpMzjdr1iy2dOlS9sILL7B33nmHMcbY\n008/zT788EP2wAMPsO3btzPGGDt//jzz8fFhFy5cYIwx1tDQwMLCwnTHPnv2LFMoFKyiooJ17dpV\nd39OmzaNpaSkcH7Xd955h/Xr148VFxezGzduMJVKxby8vFhDQwNjjDGVSsU2b97MGGNs8uTJbPXq\n1Ywxxmpqatg333yjO45CoWCFhYVmfzPja9WjRw82ePBg3bnvv/9+tnTpUsaY4b3LdQ6ue0z/ugu5\nP5KTk1l9fT3LzMxkrVu3Zjdv3mSMMRYSEsKOHDnCGGPs5s2b7MSJE5zXzhUh8ReBHj16sLZt27L2\n7dvr/mlFlzHGvv/+exYYGMh69OjBPvroI97j5Ofns8DAQN17lUql+wNjjLFXXnmFPfbYY7r3n3/+\nuYHQKhQK9uWXX+reb9y4kT300EOMMcM/oI8++oiNGTPG4NwvvPACW7ZsmYlN586dYz4+PuzOnTu6\nbVOnTmXTpk3j/A5lZWVMoVCwW7duMcaaxcwcQ4cO1YmWMcZ/+F9//TVr2bIlq6mp0W1TKpXsiy++\n0L3/8ssvWXh4uK69n58fa2xsZIwxduvWLaZQKNixY8d07e+55x62b98+zvMnJycbfNdx48bpRJkx\nxn777TfWokUL1tDQIEj8o6KizF4L7fU6cuQIGz16NLt58yYLDg5mVVVVBuLPGGMPP/yw7v7Izs5m\nnTp1YvX19YyxZvFvaGhgGzduZKNGjWKMmRf/Bx98kL377ru699nZ2bpjMGYo/jNmzGAvvPACKy4u\nNjkOl/gb/2bG1yo8PNzg3JmZmUypVDLGhIm/8T2mf92F3B/a78gYY507d2bff/89Y4yx7t27s3ff\nfZeVl5dzXjNXhtw+IqBQKLB//36UlZXp/s2dO1f3+YgRI9CrVy8ATS4SLXfu3MGLL76I8PBwBAQE\nICoqCuXl5QaP5fqP+r6+vujcubPB+8rKSgNb9B+lu3fvjosXL5rYe+7cOXz//fcIDAzU/du9ezcu\nX75s0vbixYsIDAyEn5+fbluPHj10rxsaGrBo0SL07t0bAQEB6NmzJwCYuIb02bFjByIjI3XnLigo\nwPXr13nbG9OpUye0bNnSwEZ9m4y/d4cOHXR+bu330L+ufn5+nMFTLi5dumRyrvr6es5rx0VYWJjF\nNgqFAvfffz+uXr2KlStXIi4ujjPmMXPmTF32zQcffIApU6bA29vbpN3cuXNx+fJlZGRkmD3vpUuX\nTO4fPtatWwfGGEaMGIFBgwZh69atZo9t/JtxIeTetQUh94d+TKd169a6v6v/9//+HzIzMxEeHg6V\nSiVJ0oGjIPF3Ahs2bEBtbS26dOmCdevW6ba/+eab+P3333Hs2DGUl5cjNzcXrOlpjPM4QgJ158+f\nN3jdtWtXkzbdu3dHVFSUQWdVUVGBDRs2mLQNDQ1FWVkZ7ty5o9t27tw5nS27d+/GZ599hoMHD6K8\nvBxnz54F0OxXNrb53LlzeOGFF7BhwwbcuHEDZWVlGDRokNkAoTHGx+zSpQuKiooMvneXLl0EH8/e\nc/n4+CA4OBht2rQxuE4NDQ24evWq2eOZY9q0aXjrrbcwY8YMzs8nTJiA4uJifP3119i7dy9mzpzJ\n2a5ly5ZITk5GUlKS2escGhpqcv/wERwcjPfeew8lJSV499138de//tVsho8t9672NzS+rqWlpVYd\n2577Y/jw4di3bx+uXr2K8ePH45lnnhG0nytA4i8SfH9Uv//+O5KSkrBr1y7s2LED69atw8mTJwEA\nlZWV8PPzQ0BAAG7cuMGZgqd/XCEC+cYbb+DmzZu4cOEC0tLSOHPTY2Nj8fvvv2Pnzp2oq6tDXV0d\nfvjhB4PUOy09evTA8OHDkZycjLq6Ohw5csRgBFlZWYlWrVohKCgIt2/fxuLFiw32Dw4ONhCF27dv\nQ6FQoGPHjmhsbMTWrVtRUFBg8XuZY8qUKVi5ciWuXbuGa9euYfny5aLNQzC+5lOmTMG//vUvFBUV\nobKyEosXL8bkyZPh5eWFvn37orq6GpmZmairq8PKlSt1QUtrzqc9Z0JCAr766ive+Rlt2rTBpEmT\nMHv2bISHh2PYsGG8x50+fTqqq6uRlZXFK5bPPPMM0tLSUFJSgrKyMt5AOADs2bMHxcXFAID27dtD\noVDoRs/BwcEoLCwU9H21MMawYcMGlJSU4MaNG1i1ahUmT54MoCl4/PPPP+PkyZOorq5GSkqKwb7G\n95gxtt4fdXV12LVrF8rLy+Ht7Q1/f3/OJytXhcRfJOLi4gzy/J966ik0NDRg+vTpWLRoEQYPHoze\nvXtj9erVmD59Ourq6jB//nxUVVWhY8eOuO+++/Doo4+a/GHqv+dK0zN+/+STT+Kee+5BZGQkHn/8\ncZ37SX9ff39/ZGdn46OPPkLXrl0RGhqK1157TZcBYczu3bvx/fffIygoCMuXLzcYYc6YMQM9evRA\n165dMWjQIIwePdrAprlz5+L06dMIDAzExIkTMWDAALzyyisYPXo0QkJCUFBQgAceeID3ugr5zkuX\nLsXw4cMxZMgQDBkyBMOHD8fSpUt521sz+jY+/5w5czB9+nSMHTsWvXr1QuvWrZGeng4ACAgIwMaN\nGxEfH4+wsDC0bdvWwJUhJM1Sv01gYCAefPBBs+1nzpyJ8+fPcz4d6J/Ly8sLy5cvR1lZGe+xnn/+\neajVakRERGD48OF46qmneO09fvw4Ro0apcuwSUtLQ3h4OICmrKaZM2ciMDAQn3zyCe/3Nr63n3vu\nOcTExECpVKJPnz6637Bv3754/fXX8fDDD+N//ud/MGbMGLP3mDHW3h/67Ny5Ez179kRAQADee+89\n7Nq1i7etq6Fg1jxvi0x4eDjatWsHb29vtGjRAseOHZPKFLfAy8sLf/75py6+QBAEwYePlCdXKBTI\nyclBUFCQlGYQBEF4HJK7fSR88HA7nDFzkyAI90BSt0+vXr0QEBAAb29vvPjii3j++eelMoUgCMKj\nkNTt88033yA0NBRXr15FdHQ0+vXrJ1rlSYIgCIIfScU/NDQUQNMEkAkTJuDYsWMG4t+7d2+rU8YI\ngiA8HaVSiT///NNsG8l8/nfu3EFFRQWAptzv7OxsDB482KBNYWGhLu9ZLv+Sk5Mlt8EVbJLCrozs\nDMTMikHUzCjEzIpBRnaG5DbJ8TqRTe5vl5BBs2Qj/8uXL2PChAkAmkq8anN8CcIWNAc0SNyQiMLI\n5pu+cEPT69joWL7dCMJjkUz8e/bsiR9//FGq0xNuRtruNAPhB4DCyEKkf5hO4k8QHEie6ulqqFQq\nqU0wQY42Ac61q4Zxl1Gobqw2eC/Ha0U2CUOONgHytcsSkqZ6WkKhUEDG5hEyQj1bjezwbNPt59TI\n2pIlgUUEIR1CtJNG/oRbkDA1Acp8pcE25Qkl5k2ZJ5FFBCFvaORPuA2aAxqkf5iO6sZq+Hr5Yt6U\neeTvJzwSIdpJ4k8QBOFmkNuHIAiC4ITEnyAIwgMh8ScIgvBASPwJgiA8EBJ/giAID4TEnyAIwgMh\n8ScIgvBAJK3nTxCaAxqk7U5DDatBK0UrJExNoIlZBOEESPwJyaAyzAQhHTTD101xhRE1FWMjCMcg\nRDtp5O+GuMqIWmgZZoIgxIcCvm6IuYVN5EQrRSvO7b5evk62hCA8DxJ/N8RVRtRUhpkgpIPcPm6I\nq4yotS4ogzLML1MZZoJwBpIHfBsaGjB8+HCEhYXh888/N/iMAr62weXzV55QIvXlVBJWgvAAXCLg\nm5qaigEDBqCiokJqU9wGGlETBGEJSUf+xcXFmDVrFpYsWYK33nqLRv4EQRAiIPvFXBYsWID169fD\ny4vizgRBEM5EMtXNyMhA586dERkZSaN7giAIJyOZz//bb7/FZ599hszMTFRXV+PWrVuYMWMGduzY\nYdAuJSVF91qlUkGlUjnXUIIgCJmTk5ODnJwcq/aRPNsHAHJzc/HGG2+Qz58gCEIEZO/z10ehUEht\nAkEQhMcgi5E/HzTyJwiCsB6XGvkTBEEQzoPEnyAIwgMh8ScIgvBAJC/vQBCE7bjCoj2EPCHxJwgX\nxVUW7SHkCWX7EB6Hu4yWaRlMgg+XqOpJEJYQU6zdabTsKov2EPKExJ+QNWKLtbklLqUSf1s7N1dZ\ntIeQJyT+hKwRW6zlNlq2p3NLmJqAwg2FJov2zHuZlsEkLEPiT8gascVabqNlezo3WrSHsAcSf0KW\naF0hp34+BfQ0/dxWsZbbaJmzcysCjhUcg2qWyqIbKDY6lsSesAkSf0J2mLhCDgJ4qPlze8RabqNl\nkyeRIgCFQNnjZchFLgDXDUgT8oZSPQnZYZLCWATgDBBYE4gRA0Zg3hT3cW1Y6ui0UPomYQ2U6km4\nJCaukPCmf0PODsG8KfOQtjsN63etd+kcfS3GTyKnak+hDGUm7Sh9kxAbEn9CdvAFZW9dveU2Ofr6\n6Pvt1bPVyIbpxC1K3yTEhgq7EbIjYWoClPlKg23KE0rAG7yZMe4C33efN4XSNwlxoZE/ITv4grLr\nd63nbO9OLhG5BaQJ94UCvoTL4IxaNu5S94fwbCjgS7gVjs7Rd6e6PwRhCclG/tXV1YiKikJNTQ1q\na2vx5JNPYs2aNYbG0cifMEJzQGPoEhEx7ZOqZBLugqxH/r6+vvj666/RunVr1NfX44EHHsCRI0fw\nwAMPSGUS4SQ0mjykpWWjpsYHrVrVIyEhBrGxYwXta2lGqz1uG7nV/SEIRyKp26d169YAgNraWjQ0\nNCAoKEhKcwgnoNHkITHxSxQWrgKQByAbeXmb0L//R1ixYrLgToDz2Ha6beRW94cgHImkqZ6NjY0Y\nOnQogoOD8eCDD2LAgAFSmkM4gbS0bD3h/xLASlRX70B+/kYkJn4JjSbP9mObKZImBK40S8VnChw5\ndgTDYodBc0Bjs23OQnNAA/VsNVSzVFDPVruEzYQ0SDry9/Lywo8//ojy8nKo1Wrk5ORApVIZtElJ\nSdG9VqlUJp8TrkVNjfaWywawyuCzwsJVSE9Psnn0b6/bRvt08Prbr+On8z+hrl0d2BCG2+G3kX8w\nH/Er47EJm2Qb/KWAteeSk5ODnJwcq/aRRbZPQEAAYmNjcfz4cbPiT7g+rVrV333FfetVV3vbfmwR\n3Dax0bFI252GumF1hh88BJQeKjUotSy3tFA5LlRDOAfjgfGyZcss7iOZ+F+7dg0+Pj5o3749qqqq\ncODAASQnJ0tlDuEkEhJiUFi4BIWFCs7PfX0bbD+2SKmgfE8QUDQ/RchxlE0Ba8IaJBP/S5cuYebM\nmWhsbERjYyOmT5+Ohx7iKGdIuBVal05S0g788stLqK5+R/eZUrkY8+Y9YvuxRZody/cEAdb8FCHH\nUTYFrAlrkEz8Bw8ejBMnTkh1ekJCYmPHIjZ2LDSaPKSnJ6G62hu+vg2YN+8Ru7J9AHEWN0mYmoBT\n60+h9P7S5o1fASGKEF2NHTmOsuW2UA0hb2Th8yc8E20n4Chs9cnHRsdiEzbh9bdfx9nLZ4EGILxz\nOFbMX6HbX46jbKoLRFgD1fYh3BIun7wyX4nUv6WKIoacxz+hROrL4hyfIOxBiHaS+BNuibOKwDmq\n1ARB2IOsyzsQhL2Yc+s4wydPi6cTrgyJP+GSWEq1lKNPniDkBK3kRbgklko50IpYBGEeGvkTLokl\ntw5lvhCEeUj8CZdEiFuHfPIEwQ+5fQiXQlu18uL1i/Db6wcUNX9Gbh2CEA6lehKywdIiL1xBXr8s\nP/QK7IWwkDBKtSSIu1CqJ+EyGC7y0kRh4RIAzfWAuIK8VY9UIexcGC2zSBBWQm4fQhY0L/LSTFN9\n/wO693xB3uLSYo9fwIQWcSGshUb+hCxoXuTFEP36/pxB3iLgTNkZ/DzyZ90mqUsrOxs5lpcm5A+N\n/AlR0GjyoFYvhUqVArV6qdXLMTYv8mKIfn1/rtx9v3w/VD1SZbDNmqUb3QF7l68kPBMa+RN2I8Rf\nr9+WK6jbvMhL8zGM6/tz5e6XdC9BAQpMbPKkBUzkWF6akD8k/oTd8PvrDdfjFdJJWKrvb5y7r56t\n5hR/TyrjQKUsCFsg8SfsRoi/HrDcSdhS39/WBUzktv6uPThiERd3uj4ENyT+hN0I8dcDwjsJa7Cl\njIO7BUjFLmXhbteH4IYmeRF2w+XOUSoXIzXV0G2jVi9FdvZKk/3V6iRkZa1wiq2Ac2r9uzJ0fVwf\nWU/yunDhAmbMmIErV65AoVDghRdeQEJCglTmEHYg1F8vJKjrDC5evwiEm26nAGkTFED2DCQT/xYt\nWuBf//oXhg4disrKStxzzz2Ijo5G//79pTKJsAMh/nqhnYQj0RzQoPB8IXCP6WcUIG2CAsiegWTi\nHxISgpCQEABA27Zt0b9/f1y8eJHE381x9KLtlkjbnYaqyCrgIICHmrf7Zflh3nIqCgc4JoBMyA9Z\nBHyLioqQn5+PkSNHSm0KIUPEzDypYTVAz7tvDgFQAGBAr8BeFMy8C62F4BlILv6VlZWYNGkSUlNT\n0bZtW5PPU1JSdK9VKhVUKpXzjCMkR+zME51LIxwGfv+wc2G2G+mG0FoIrkVOTg5ycnKs2kfSbJ+6\nujo8/vjjePTRRzF//nyTzynbhxA784SrM1GeUCL15VQSO8JtkHW2D2MMc+fOxYABAziFnyAA8TNP\nHOnSkMvEKLnYQcgbycT/m2++wc6dOzFkyBBERkYCANasWYNHHnFu2h8hbxyReeIIl4ZcJkbJxQ5C\n/tAkL0JWGBd+Gx3lj53H3pe9m8ZW95TYo3SaoEUAMnf7EK6FpSUWxToHV+G3aXOex9E/v5Z15okt\n7il7Rul8nQZN0CKEQuJPWMSaks32wFf47WheErKy5D1qtcU9Za4Ov621ieQ6QYviEPKDFnMhLCJk\niUUxcEThN2fBtdCM8oQS86bwT4yydZRurtOwxQ5Ho+2sssOzkdszF9nh2UjckEhLTUoMjfwJizhL\nlIVWB5UjtmQR2TpKN9dpyHGClq1POIRjIfEnLOIsUZZL4TdbsTaLyNYyCpY6DXuymYS6Z6xx41Ac\nQp6Q+BMWcZYoy6HwmzOxdZTuqNo7QgPQ1gaq5RqH8HQo1ZMQhEaTh/T0A3qiHO22ouwKaA5oDDuN\nKfa7doSmiVqbTkqzqp0PpXoSoiF1NU7CEEdMVBPqnrHWjSPHOARB4k8QxF2EumdscePY0llReqhj\nIfEnCAKA8FiCM+r9U5kKx0M+f4IgdAiNJTgi5qAPlamwD/L5E7LGGSUjCPNwuVaEiKuj6/1Teqjj\nIfEnJMFZJSOcgav6puXsWqH0UMdD5R0ISXBWyQhH48qlC8zNvJUaOZapcDdo5E9IgivX8dHHlUsX\nyNm1QumhjofEn5AEV67jo4+cBdQSznCt2OMSo3WEHQuJPyEJrl7HR4sr+6YdnbIp55gCQamehIS4\nQ8kIVy9d4MiUTUrXlA4h2kniT4iKJ6ZvOjrn3VVRzVIht2euyfaos1HI2ZbjfIM8CNnn+c+ZMwca\njQadO3fGTz/9JKUphAi4U/qmNZBvmhtXdol5ApKmes6ePVv2y/MRwnGX9E1CHChdU95YHPmnpaVh\n+vTpCAwMFP3kY8aMQVFRkejHJZyDsYvn4sVKznaulr4pBFed2OVMKF1T3lgU/8uXL+Pee+/FsGHD\nMGfOHKjVaigUCmfYRsgYLhePn9+znG1dLX3TEpTFIhxyickXQQHfxsZGZGdnY9u2bTh+/DieeeYZ\nzJ07F0ql0tKuFikqKkJcXBynz1+hUCA5OVn3XqVSQaVS2X1Own7U6qXIzl5ptDUPfn67UVX1H90W\npXIxUlPdazUuymIh5EZOTg5ycnJ075ctWyZOwNfLywshISEIDg6Gt7c3ysrKMGnSJDz88MNYv369\nXUZbIiUlxaHHJ2yDe4buWCgUbyIwcAoYa4levdpi+fJn3Ur4Adee2EW4J8YD42XLllncx6L4p6am\nYseOHejQoQPi4+PxxhtvoEWLFmhsbESfPn0cLv6EPOGboXvnzhDcubMCAFBevsSZJjkNT8liobiG\ne2NR/G/cuIFPP/0UPXr0MNju5eWFzz//3K6TT5kyBbm5ubh+/Tq6deuG5cuXY/bs2XYdk3AOXDN0\ngcUAmmfoNmX6JFk98pf7XAFnLGZiC3kaDbLT0uBTU4P6Vq0Qk5CAsbG2ibXmgAbxK+NRykqbcgIb\ngVMrT2ETNonaAVAHIx0Wxd/c48OAAQPsOvmHH35o1/6EdGjFOD09CdXV3jh16jeUlb0EwFCkrc30\ncYW5AnLMYsnTaPBlYiJWFTZ3SEvuvralA0j6d1KT8D/UvK30YCmS/p0k2vc0FzgHQJ2Cg6EZvoQo\ncAeAAbU6CVlZK5x+HE9jqVqNldmmQegktRorbJhLE3RvEMoeLzPZHpgRiBs/3LDJRmP4AufDTgxD\nuaLc8MkqX4nUv7lGyQw5IEQ7qZ4/IQoJCTFQKg19/E2F2qKh0eRBrV4KlSoFavVSaDR5vMdxl1LP\nzsanhjsI7V1tWxCaefMIh4g/A1/g/MzlM4LWGdAc0EA9Ww3VLBXUs9UusYaCnKCqnoQoGLuBmgq1\nNfn/Lblx9H38BQW/AMiDsfvI3eYKiE19K+4gdIOvr01+9Z6deyIf+abbg3uKYi/AHzhXNHDPI9LP\npqK5FvZD4k+IRmzsWBO/vFq9lKfkQ1MgmMvH7+PzF9TXA9oOwBVLPTubmIQELCksNPD5L1YqETB2\nlE0iuWLeCsSvj0fp/aW6bSFHQrD81eWi2cwXOG/XuR3KYOpy0s+mcuVFdOQCiT/hUCy5cUzrAeWh\nvr4jfHzehr//RoSHt8WKFTNkE+yVK9qgblJ6Oryrq9Hg64tH5s3Dqk9sE8nY6FhswibDoPar4ga1\n+QLnADjhoKnxAAAgAElEQVTLZOtnU9FcC/sh8SdsQmg6pqUVuww7hzwAXwJYhfp6oKwMCApyz7kC\njmBsbKxJZs/re7jn4QgRSWeUZjB3DnPZVJ4y18KRkPgTVmMpHVO/Y7h1qxQhIX9Haelburb6bhzD\nziEbAL+LiLAeVxVJSx2PXOdauBIk/oTV8JduTgJgGuANCZmLYcP+Bn//TrpAsFbMDSeLUaaPMfZO\n3HJXkZTjXAtXg8SfsBpzfnyujqG0dDMiIpKQlZViso9+ltCxY3+gzDTO57GZPmJM3HJnkaSKofZB\nk7wIQRinY16//jcYp2Oq1U1pnrm5KXpb8wBkIyCgGCNHhpkt1cDlTnLHqqBCEXvilhygcg7OQfbL\nOLoiYtZPcRWsScdMS9MXq+YAbnk5kJ0NHDs8C4tePYaFKf9rch6+uQKeKPyA+BO3pIZy8+UFib8V\niF0/xVXgcuXU1/8Hfj6PonObt+DtU4Pnpz2kE+lmH75pAPdm1Ta8ty4Ko+/tz3nNuOYKeCrmJm65\nIpSbLy+ovIMVZKelGQg/AKwqLMSB9HSePdwDPh//iPoqFJXvR+H1LNza+R/kaTSIjR2L1FQ11Ook\nBAQUc+7XrUrh1tcsT6PBUrUaKSoVlqrVyNPYVnYgJiEBS4wWTFqsVCJ6nmOCtY4ul0C5+fKCRv5W\nIIfHcCncTry5+rite72qsBBJ6ekYGxurG703FWnj3s9bxEvmzGti6VxiPh3yTdxyxHdzhkvGVdNO\n3RUSfyuQ+jFcKrcTV+1+JZ7BPPxq0M64E0xIiMHRnBm4VbvDZL+jvvfbbI9+8PnOrUsIv/Q1/q/0\nD93njromQq4/39OhtmO0Fq6JW47AGS4ZV0g79aSANIm/FfDVT3nEQY/hxogtLEIxDsReKPgeqdeP\nIBaVBu2MO0F/VGBE6xycqb0X3dAGvriNefgVX4W0xQS9a2bNyJ0r+HwDz0KDSzp7HHVNhFx/OTwd\n2oIzXDJyTzv1tIA0ib9AtAJ1zdcXz3bogNDQUPh37eqwx3BjQewyejT+/OEHpACoBxCD5kRLZwiL\nfiA2T6PB2/GFSCsNQAl8UIrWULRoiZ5XumOUJk/XLjstDQduXkAeLuAAmqoBHwVQEdpHd82sfZpp\nDj43pZACPihEHyShD2L1qlA64poIEXa+p8PiW7ewVK2WbZaYs1wycs7N97SAtFuJv6N8v5wC1b49\noh0o/PrnywOw+9AhfFTf7HvXVrwZC+dnf1TAH9/gMVzERGhTOVEHXMsH4uPnIjT0I7Rr1xkXTjGM\nRlvEotJgRkBKu3a619Y+zTQFn5tTSLX8ggvQ4A/d6N/SNbHlXhHi9uN6OpwTEoL2ly5hZX5z5yS3\nLDFXcMk4Gk8LSLuN+DvSH+5sd4vx+bIB/KfeMOi6CkASgCylEmGjRjl1VJmWlo2Lpf8GsBSGqZx5\nKC0NQWlp87ZEPAsg08BFpC+WlRcvcp6joqSE8zs1BZ9NU0irsR3pOI1YHLfoiuO6V+aeOoWPQkPR\nuV073msoxO3HFaT1vXIFb+Ub1sZ3hrvOGuTuknEGnhaQllT8s7KyMH/+fDQ0NCA+Ph4LFy60+ViO\nFGgx/bhCRpzG5+P7kc4HBmLktGko2blTtE5PiH2XL5bzWMZRmA0fIx33IhbHARiKZZ5Gg0tGv5mW\nS4WF+LigwOQ7JSTEIC9vE7gu/Y/eAXi2fQdETZtm9rsb3yt5AEJKS7GqtLl2Pdc1FJp9YxykTVGp\nOO2QWxxAzi4ZZ+BxTz9MIurr65lSqWRnz55ltbW1LCIigp0+fdqgjTXmJUdFMQaY/EuOirLb1iUx\nMZzHXqpWW3Wc3IwMtlipNDjGYqWS5WZkmD3fEo5za88vlm189r3o58deGDiQLYmJ0dnZs4P67sdL\njE6bzGUK6x44gSVHRbGlarXBd10SE8NyAbbYaIfZXl4s18x3iox8ifM8agznvab6GN8r5q6vGIj5\nG+mTm5HBlsTEsOSoKIPfh7CdjOwMpp6tZlEzo5h6tpplZLvmNRWinZKJ/7fffsvUejf/mjVr2Jo1\nawzaWCP+jvoDY4xbFF+zIDD22Gh8vlyAvejjw3l+MTs9XvuMOqopA0cxJZ5hQC4DFus1Ne4M7oqy\neinn+bS25949R/Ld/z/ZurXZ75SRkcuUysUGHyvxNMtAW0G/u/H3TOYRfzEGDoyJd/9YOqalTo/w\nHIRop2Run5KSEnTr1k33PiwsDN9//73Nx3NkGqZYk22Euo+4zjdk1CgkHT0K7+pqXK2oQA1jOLR+\nPX4pKDBY8VabA3Ph1CksVaut8v/z2nf3/6sKCxGflISG0vNIRQHScQbF8EEpHkIogKttG6Boy1+7\nX588jQa/FBRwZi896+cH3Lljso82VqCfevrr0Z/Qr7wE8/CrQVyBy6WidWlVXryIZ/388LeqKoy9\ne34ufrPhGnLhiMlaUqX9Eu6DZOKvUHAv0mxMSkqK7rVKpYKKx3/q6NmQYky2sWaSGN/5uIKVf/Hx\nwd0qa805MGVlQHa2Vf5/Xvu05wbQ4pdf8LfqanwJIOuuHx+429GmpqIC/hYLs2m/w8fXr+u2abOX\nspRKRE2bhiVGcQzjjlybetpU+bLZDp3NRteU87r5+WFXr15oaNkSf790CW/p+fwXA3iprAxjrbyG\nfIg9WctV5xMQjiEnJwc5OTnW7eSEJxBOvvvuOwO3z+rVq9natWsN2khonkPQPqrn3vUzJwPsGT8/\ntiE5WfAx+Fwzz3bowJ4NDBTkVsrIyGUxMUtYVFQyi4lZwjIycg3sM3BP3HXLGPvG9V01z3bowHIz\nMsz6oLXnjBj0Mgv2GW3gotH/Dtp9cjMy2FK1miVHRbG5kZHspchIzuMKdalYcrlpzzejfXu2VO87\n2+M+dKRP3pFuTsL1EaKdkqlrXV0d69WrFzt79iyrqamxO+DrKmxITmYv+vnZ7Ks15+MX4v/n9Jcr\nFxt0AEvVapY4aBB7xs/PQASn+/ryHt+cD5rbR/+MSQfA5WMX4tvW7yiMg8pCrpst7SzhCJ+8fmfy\nUmQkmxMSImocgXAfZC3+jDGWmZnJ+vbty5RKJVu9erXJ5+4o/vaO2MztL+TYMTHCg7LGovpSZKRN\n5+Y9593sHHPXQKwRrtDjCHlCEDKaF3tkztWZLAgJYXPvPhHxdXqEZyJEOyXN83/00Ufx6KOPSmmC\n0xHqq+XLt7cU2LYU9Da3BKMxxn7qPI0GS4z85trjH1q/nvd71fCtzYs2vHZqEcu3LTQhwFw7ayYS\niu2T5wrwvlVaiqSICKS46KpehLS4zQxfKbGmVICQoK8QkTEX2Db3GW95ZgHr5Jo7d3ZaGu/3asW4\nz3kM3ghHFKp9arFg2kTOayZWJVUh1037O97288OzHTqgfUgIOoeF6dotVasFZ9iIXQGWAryE6Djh\nCcRmZG4eY8x6366QAKWtLgMhLglun/9rOp+/mNfhNTM+fx/MZk3zBJreB/rOYM9EjjWx2RE58kLt\nN/4drYkHiG03BXgJaxCinTTytxNr862FjEBtGeUJdUk4ap1cc98rT6PBPe2OAIFq3K5tgbLqOtQ0\nLIH+AvBl1dtRnn8vvkxMNDiesxY0EfI7WpuqK6bdUpcTJ9wQJ3RCNiNz8xhjjikrYcsoT64jQ64R\ncDffRzgDwFGIksxmIb+js55C+BCS1UQQjNHI3yk4YnUvW0Z5cvUJc42o+1dfwwWOttplIaWwWcjv\n6KinEKExI2et6kV4BiT+duKIx3FbREbqJSb54OqUEvArfvCdibLq7bpt+stCSmGz0N9RbAGWamlO\ngiDxtxNHjQatFRm5+oS5OqVYVCKmfxHOKl7EpdPnMaD6mq42j1Q2O3OxdH2oRg8hFYq7/iFZolAo\nIGPzZEeeRoMDeuIl5kpj+oumt2pVj4SEGEFBYq6RrbYOkDYY7CibXYEUlQopubmm26OikGJtrRaC\nuIsQ7fSokf8hzSHsS9sHRY0CrBXD+ITxGBc7TmqzRMNRPmGuRdMLC5tKsWk7AL7OwdKI2tP92HJ1\n1xHuj8eI/yHNIXyY+CGeK3xOt21X4S4AcKsOwBE0L5reTGGhGjNnbsCgQYdw61YxLl1qZ1DKWb9z\n8HSBN4dc3XWE++Mxbp8EdQImZk802b5XvRepWaminMNdUalSkJuborfFeAH1pQBWmuw3bNjf0LFj\noNWuIk/D011fhPiQ20cPRQ3P+gE0O94ipiUhjNfq5b6NTp+uQHX1Bt17Y1cR0YTYT0aaAxqk7U5D\nDatBK0UrJExN8Oi1eQluPEb8WSueXpBcqxZJSIhBYeESPdeP8W3DXbunurqHwfvCwlVIT08i8Xcg\nmgMaJG5INFiEvHBD02vqAAh9PEb8xyeMx67CXQY+/53KnZg6b6pVx3H3oDEXxiUhCgp+gd4iXGha\nhHEJ9J8GfH3/gupq02vLVT2UEI+03WkGwg8AhZGFSP8wncSfMMBjxF8r0HvT9za5enyBqfOmWiXc\n7h40NpfOqV02UdsuMVH/SWAsQkK2oUuXv8HfvxN8fRtw5Uo98vNNR/gVFVcxbFg8iooqwVgr9OzZ\nBitWTKanAZGoYdwzvasbyb9JGOIx4g80CbQ9Ir0vbZ+B8APAc4XPYW/6XpcXfyHpnFq4i8PNMmhn\n2kEAISELcObMJdy82R/AJgBAfj4QH/93bNpEsQAxaKXgTh319SL/JmGIR4m/vbhz0Jg7nZPfR6//\nJMAFVwdx5UoF8vMHwDgzqLT0LYoFiETC1AQUbig0cP0oTygx72VKHSUMIfG3AncOGluzwpdQjDsI\nlSqFty3FAsRB69dP/zAd1Y3V8PXyxbyX55G/nzCBxN8KxAoayxF7Vviy9xxc57G1nATR1AGQ2BOW\nkET89+zZg5SUFPz666/44YcfMGzYMCnMsBoxgsZyxTSdE1AqF2PevEdEPcepU9tRWmqYGdS+/TRc\nudIOKlUKWrWqx+jRXbBzZ4mg+ANBELYhyQzfX3/9FV5eXnjxxRfx5ptv8oq/JxZ2kzKVVKPJQ3r6\nAb0gbrToYqvR5CEpaQeKim4DaInAwBrcudPFoDSEn99fUFU1FforfQGAWp2ErKwVotpDEO6IbGf4\n9uvXz6b93C3H3vj79BrdCz/v/FmyVFJLQVxHnEOtXorsbMMAcFXVfwAkwVj8KS5AEOLhMj5/d8ux\n5/o+qYdToapSGbRzl1RSPvgCzYCp0IsZfyAIT8dh4h8dHY3S0lKT7atXr0ZcXJzg46SkpAAAvvjg\nC0w5M8XgM1cWRq45A4lVidiCLYhAhGFjmaWSpqRsxNtv56K+3g8+PlV4+eUopKT81aZj8QWB/fx+\nQVVV83ux4w8E4U7k5OQgx8r1Hxwm/gcOHBDlOFrxL8spw9AzQ00b8Agjl4sIgGzcRnxzBrzgZbpR\nRqmkKSkbsWrVKdTXf6zbtmLFM/jgg6Po1q2X1Zk5fIHmadOicPSo/iSyRyjYSxA8qFQqqFQq3ftl\ny5ZZ3Edyt4/QgK41OfZcLpVNpzbhNm4jsTRRt81at5GYMQe+71PsVwzojXjllkraNOL/WG9LHhob\n++DMmVU4c6ZpizWZOdyzhUnoCcLRSJLts3fvXiQkJODatWsICAhAZGQkvvjiC1Pj9CLWXIK+U7kT\nU1NNUy35avdvwRbMwRxDWwTW8+eMOSh3YUrqFJs6AL7vM2jaIJw9elaXSvrkvCcFHd9ZwfD27Weh\nvHyb3hbuWv6UmUMQ0iHbbJ8JEyZgwoQJVu1jTY69VS4Vgf50ser66Iv0rXa3sHnYZgT5B9k1Z8CZ\nwXAfnyrjLZztKDOH0ELrC8gTyd0+lpg2bBo6tOugG80KGaXzuVQa0Wi6kcefbjySvlJyhbthNf8+\nxqNvvqeHJ5cLG93z4cyCcy+/HIVVq/6C+vr/3N3i+JnBhOtC6wvIF9mLf3x+vO610NEsVxmG90Pe\nxx3cAfQSkPj86ZxpmH6pOImTppk4vvz7GNvrKJF2ZsG5pqyejXj77cmor/dFQ8NVtGjxMsrK3ta1\nocwcQgutLyBfZC/++ggVSi4X0dx5c0228blZ+NIwV/qtBKqA4zgOb3jjgt8FxI2K493H2F5HibSY\nBeeE1NRJSfmrQWpn08xgCtgSptD6AvLFpcQfgGCh5KvdL2SEzSfSvp19kXMlB4lVdzOGqoBdO3fh\n0L2HBAm7o6qCilVwzpqa/vo4Y2Yw4ZrQ+gLyhSMCKnOccM/wiXRtZW2z8N/lucLnsD99vyBhH58w\nHruUuww+3qnciSfnPWmXveNix2FK6hTsVe/F3qi92Kvey5kFZQn+mv7izNkgPI+EqQlQ5isNtilP\nKDFvCq0vIDUuNfJ3Vs4730g61C8UuM6xQzUw/h+WR9+OrApq7yplgGNq+hOeDa0vIF9kL/571c4v\nn8wn0vvS9gEFHDv4Chd2MUTaVixlIzmjpj/hedD6AvJE9uIvJLXTUTDGoEDzZAlLvnUphd0SQrKR\nnFHTnyAIeSDJDF+hSFXP39xsXgDYn77f6hm4UsM369l4hrMzavoTBOFYZDvD11GIVeLAXNpmalaq\nS4i9MULTTK3J3OG63lXwMUkVBUBLMhKEzHAb8T+kOYQt8VsQX9o8KWzTqU3AJutLHDhr0pQzF6cR\nO82U6+novVOb8D18cKZ0m27bqVNzAQQYrNRFSzIShPS4jfhvSTIUfgCIL43H5tc3WxRUYxG+fOsy\nd0MR00ydvTiN2IvPcz0dvVAaj/9ir8G20tJQADFoKgDnA6AehYVqpKcfIPEnCAlxG/EvO1vGuf3G\n2Rtm9+Ms/xyyCakhqQbln8VOMxWz1IOQJwix00z5no78TLZcBfAl9BdsB5aguPiaTeclCEIc3Eb8\n6xR1nNvreQqPaeES4fjSeGwethl7IxyXZiqWa8maJwgxs5H43EjGNT+BmwDeNdq2CqWlk0WxgyAI\n23Ab8Q8MD8Smsk2IR7Pr5328j8DwQLP78YlwkH+QQ9NMxfLBO7Oipz5cbqR3Q95HPVoYFM9r0UKB\nOo5+OTQ0xGG2EQRhGbcR/xdXvIj0+HRsKd0CL3ihEY24HXIbCSsSzO7nqHo7lhDLB+/Mip76cLmR\nnp83F9PgY1Dk7cqVIOTnm+7ftau/Yw0kCMIsshf/RFWioEyYcbHjgE2GOfiz5s2yqfyzvf59Z/rg\nxe68hFT11MLnRtJv31QsjiaOEYTckL34T8htWvFLSCaMrT7tW+1uYU3gGvgwHwT1CsLs5bNtdpk4\n2wcvZudla1VPc9AavQQhTySZ4fuPf/wDGRkZaNmyJZRKJbZu3YqAgABT4xQK/Bv/1tXPv9jhIhZt\nXySaL1vsdXkB4TNpxeSQ5pAos47V6qXIzrZ9PV5rnhoIgnAcsp3hGxMTg3/+85/w8vLCokWLsGbN\nGqxdu5az7Q/4oTmIex3YlWj+CcCaiVOOCJZK4YMXK4vHnqqejnhqIAjCcUhSzz86OhpeXk2nHjly\nJIqLi3nb6mfvAM3187nQjuQnZk/EhNwJmJg9ER8mfohDmkOc7R0h1FIFkMXAnqqetBYAQbgWki/m\nsmXLFjz22GPW7cQjznwjeb7OwhFC7agFW5xBQkIMlMolBtuagrPRFveltQAIwrVwmNsnOjoapaWl\nJttXr16NuLimdW9XrVqFli1bYupUK4OTPOJs7UjeEZk+jlywxdHYE5yltQAIwrVwmPgfOGD+cX/b\ntm3IzMzEwYMHzbb7e+DfMaRsCABgKIaiQFnAK87WjuQdJdTGPvhDmkNIUCcYxCEAOK2omzXYuh4v\nrQVAENKRk5ODnJwcq/aRJNsnKysLr7zyCnJzc9GxY0fedgqFAgczDgrOZOHK3tmp3IlB0wbhzHdn\nJBFaLptSQ1LRBm0MCtHZm2UkB2gtAIKQB0KyfSQR/z59+qC2thZBQUEAgNGjR2Pjxo2mxtmwmItx\n2mPPUT3x886fRU3ntAbj1M+TOIl92IdkJJu0dWQ6qDOhlE+CkBbZpnr+8ccfDju2scslQZ0gSe0b\nLfpxiJM4iR/wA3qgB3djB6SDOnPNAIBSPgnCVZD9DF97kar2jRb9OMRxHEc84rEZm7kbi5wO6uw1\nAwBzKZ9JJP4EISMkT/V0NFLn3eunfnqjKe1xOIZjEzYZtHNEOqi1qa9iQCmfBOEauP3I35Z0TjFd\nJdr9/pP0H1z46QK21W9DAxrQER2xBU0VSC91uISFqQtFH41L8dRjTconxQYIQjrcXvytTed0lKvE\n/5Y/VtU3u0M2YRPuxb34SfmTQ4QfkOapR2jKJ8UGCEJaJMn2EYot2T724ojCbHzHXNNhDRZud4zw\nA/ypr1NTHTvpTEjKp71F5AiC4Ee22T5yxhGuEr5j9hvUz6EiLNVsYyETxSg2QBDSQuJvhCNcJfYe\n054YhJjr9ooJlYMgCGlx+2wfa3FEYTZ7jmltpVJXwZ4icgRB2I/H+fyFjKLFWhxFjGNKsTiMs6By\nEAThGMjnb4TQTB5zrhJbXTC2ul+knqRmLdakb9paRI4gCPvxKPG3d+UuS52HI0opSD1JzRoofZMg\nXAePEn++UfTNkpu61+YE3FznAcAh8wMcseaAo6DSDgThOniU+PONoksKS3QBVHMCbs4F44j1gLXn\nBVxjcRhK3yQI18GjxH98wnikHk5FYlWibtv7eB9xVXHYn74fjDGzAs7XedyouIEbJ29wn1QE37xc\n0zWNofRNgnAd3CrVU7tiVqIqEQnqBJN0yHGx49CqVytswRZswzZswRaMwAhEIAKothxc5UrZfD/k\nfVRerETQzSDufWXom3cUlL5JEK6D24z8hWbydO7aGRN/Nk2dhC/4U6PuCjiXC6b+Sj0S8xNxEiex\nCZsQj+bVueTqm3cU9qwBTBCEc5F9nv/BjIOi5sObq3cDmPr8LdXCSVQlYkLuBABNi7X8F/9tqtQZ\neAkLP3Bc3R6CIAg+3CLPP31SOt7t/y5eXPGiWSEVmg8vJIBqTXBVPw4Qcfc/ANg7gjvQ6+yVtQiC\nILiQvfgnVidiS/4WfJj4IQD+tElr8uHNBVCtDa5ak4opxcpaBEEQXEgS8E1KSkJERASGDh2Khx56\nCBcuXDDb3gteFlegckRNHiGMix2HKalTsFe9F3uj9mKvei+vm0iKlbUIgiC4kET8X331VZw8eRI/\n/vgjxo8fj2XLlplt34jGphdm0iatEWF7yMnJ4Tx3alYqUnNSkZqVyntOR5Vq4LJJDsjRLrJJGGST\ncORqlyUkEX9/f3/d68rKSnTs2JG37TIsQxDuplFaSJsUKsL2YM8P7ahSDXK9+eRoF9kkDLJJOHK1\nyxKS5fkvWbIE3bt3x/bt27Fo0SLedslIxjVcw79D/u1wF46jkco1RRAEYYzDAr7R0dEoLS012b56\n9WrExcVh1apVWLVqFdauXYsFCxZg69atvMeKRzw2d9ns8kFRVyrVQBCEeyN5nv/58+fx2GOPoaCg\nwOSzroquuIiLElhFEAThuiiVSvz5559m20iS6vnHH3+gT58+AID9+/cjMjKSs10JK3GmWQRBEB6D\nJCP/SZMm4bfffoO3tzeUSiXeeecddO7c2dlmEARBeCySu30IgiAI5yP7qp7WTghzBv/4xz/Qv39/\nREREYOLEiSgvL5faJOzZswcDBw6Et7c3Tpw4IaktWVlZ6NevH/r06YN//vOfktqiZc6cOQgODsbg\nwYOlNkXHhQsX8OCDD2LgwIEYNGgQ0tLSpDYJ1dXVGDlyJIYOHYoBAwbgtddek9okHQ0NDYiMjERc\nXJzUpgAAwsPDMWTIEERGRmLEiBFSmwMAuHnzJiZNmoT+/ftjwIABOHr0KH9jJnNu3bqle52Wlsbm\nzp0roTVNZGdns4aGBsYYYwsXLmQLFy6U2CLGfvnlF/bbb78xlUrF/vvf/0pmR319PVMqlezs2bOs\ntraWRUREsNOnT0tmj5a8vDx24sQJNmjQIKlN0XHp0iWWn5/PGGOsoqKC9e3bVxbX6vbt24wxxurq\n6tjIkSPZ4cOHJbaoiTfffJNNnTqVxcXFSW0KY4yx8PBwdv36danNMGDGjBls8+bNjLGm3+/mzZu8\nbWU/8rdmQpiziI6OhpdX06UbOXIkiouLJbYI6NevH/r27Su1GTh27Bh69+6N8PBwtGjRApMnT8b+\n/dKXrxgzZgwCAwOlNsOAkJAQDB06FADQtm1b9O/fHxcvSp/d1rp1awBAbW0tGhoaEBTEs1aFEyku\nLkZmZibi4+MtVqt0JnKypby8HIcPH8acOXMAAD4+PggICOBtL3vxB4RPCJOCLVu24LHHHpPaDNlQ\nUlKCbt266d6HhYWhpISytixRVFSE/Px8jBw5UmpT0NjYiKFDhyI4OBgPPvggBgwYILVJWLBgAdav\nX68bdMkBhUKBhx9+GMOHD8f7778vtTk4e/YsOnXqhNmzZ2PYsGF4/vnncefOHd72sriS0dHRGDx4\nsMm/zz//HACwatUqnD9/HrNmzcKCBQtkYZPWrpYtW2LqVOcs2CLEJqlRKHjqFxG8VFZWYtKkSUhN\nTUXbtm2lNgdeXl748ccfUVxcjLy8PMnLF2RkZKBz586IjIyU1Uj7m2++QX5+Pr744gts2LABhw8f\nltSe+vp6nDhxAn/9619x4sQJtGnTBmvXruVtL4uSzgcOHBDUburUqU4bZVuyadu2bcjMzMTBgwed\nYg8g/DpJSdeuXQ2C8hcuXEBYWJiEFsmburo6PPXUU5g2bRrGjx8vtTkGBAQEIDY2FsePH4dKpZLM\njm+//RafffYZMjMzUV1djVu3bmHGjBnYsWOHZDYBQGhoKACgU6dOmDBhAo4dO4YxY8ZIZk9YWBjC\nwsJw7733AmhKqTcn/rIY+Zvjjz/+0L02NyHMmWRlZWH9+vXYv38/fH3lt0ivlKOj4cOH448//kBR\nURFqa2vx8ccf44knnpDMHjnDGMPcuXMxYMAAzJ8/X2pzAADXrl3DzZs3AQBVVVU4cOCA5H9zq1ev\nxjCpjmIAAAIvSURBVIULF3D27Fl89NFHGDdunOTCf+fOHVRUVAAAbt++jezsbMkzyUJCQtCtWzf8\n/vvvAICvvvoKAwcO5N/BCQFou3jqqafYoEGDWEREBJs4cSK7fPmy1Cax3r17s+7du7OhQ4eyoUOH\nspdeeklqk9inn37KwsLCmK+vLwsODmaPPPKIZLZkZmayvn37MqVSyVavXi2ZHfpMnjyZhYaGspYt\nW7KwsDC2ZcsWqU1ihw8fZgqFgkVEROjupS+++EJSm06dOsUiIyNZREQEGzx4MFu3bp2k9hiTk5Mj\ni2yfM2fOsIiICBYREcEGDhwom/v8xx9/ZMOHD2dDhgxhEyZMMJvtQ5O8CIIgPBDZu30IgiAI8SHx\nJwiC8EBI/AmCIDwQEn+CIAgPhMSfIAjCAyHxJwiC8EBI/AmCIDwQEn+CIAgPhMSfIKzghx9+QERE\nBGpqanD79m0MGjQIp0+fltosgrAamuFLEFaSlJSE6upqVFVVoVu3bli4cKHUJhGE1ZD4E4SV1NXV\nYfjw4fDz88N3331HZawJl4TcPgRhJdeuXcPt27dRWVmJqqoqqc0hCJugkT9BWMkTTzyBqVOn4syZ\nM7h06RLS09OlNokgrEYWi7kQhKuwY8cOtGrVCpMnT0ZjYyPuu+8+5OTkSLrYCUHYAo38CYIgPBDy\n+RMEQXggJP4EQRAeCIk/QRCEB0LiTxAE4YGQ+BMEQXggJP4EQRAeCIk/QRCEB0LiTxAE4YH8f7lI\nh9qWNI/dAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x105b68950>"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now suppose we didn't know the cluster labels. That is, we don't know which of our 4 MVN distributions an observation comes from. We can visualize this with a plot without the colours - just the raw data:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/FromOnline1.png\" alt=\"\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Model####\n",
      "\n",
      "For clarity, let us denote the number of the data vectors with N"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 200"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and the dimensionality of the data vectors with D:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we don't know how many clusters our dataset has, we'll start with some large number of clusters, and the algorithm will pare this number down to a 'best guess' as to the true number of clusters. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember when we have more than 2 possible clusters (we're assuming 10 here to be safe), we use a Categorical distribution. So the cluster assignments Z will have a Categorical distribution, and the prior for the cluster assignmenat probabilities $P(\\pi)$ will have a Dirichlet distribution:\n",
      "\n",
      "$$P(\\pi) = Dir(\\pi \\mid \\alpha)$$\n",
      "\n",
      "(More about Dirichlet distributions [here](http://en.wikipedia.org/wiki/Dirichlet_distribution), but for this example you only need to know that it's the conjugate prior of the Categorical distribution)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "\n",
      "from bayespy.nodes import Dirichlet, Categorical\n",
      "\n",
      "#Create a Dirichlet process prior for the cluster assignments our P(\\pi) \n",
      "alpha = Dirichlet(1e-5*npones(K), name = 'alpha') \n",
      "\n",
      "#The cluster assignments themselves follow a Categorical distribution\n",
      "#Since there are N observations, there will be N cluster assignments\n",
      "Z = Categorical(alpha, plates=(N,), name='z')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also want to find the mean vectors and precision matrices of the clusters themselves (the precision matrix is the inverse of the covariance matrix, and is arguably nicer to work with when deriving the math behind this process). The conjugate prior of the mean of a Gaussian distribution is another Gaussian, and we will have exactly K elements in our mean vector, $\\mu$ (each cluster will have its own mean). \n",
      "\n",
      "The conjugate prior of the precision matrix $\\Lambda$ (inverse of the covariance matrix) is a Wishart distribution. (More about [Wishart distributions](http://en.wikipedia.org/wiki/Wishart_distribution)).\n",
      "\n",
      "(Side note: Some people like to put all of this together and talk about a Normal-inverse-Wishart distribution as a conjugate prior for a multivariate Normal distribution with unknown mean and covariance matrix. If you're interested, here's the [Wikipedia article](http://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution).)\n",
      "\n",
      "So we have:\n",
      "\n",
      "$$P(\\Lambda) = \\prod_{k = 1}^{K} \\mathcal{W}(\\lambda_k \\mid W_0, \\eta_0)$$\n",
      "\n",
      "$$P(\\mu \\mid \\Lambda) = \\prod_{k = 1}^{K} \\mathcal{N}(\\mu_k \\mid m_0, (\\beta_0 \\lambda_k)^{-1})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "\n",
      "from bayespy.nodes import Gaussian, Wishart\n",
      "\n",
      "mu = Gaussian(np.zeros(D), 1e-5*np.identity(D), plates=(K,), name='mu')\n",
      "\n",
      "Lambda = Wishart(D, 1e-5*np.identity(D), plates=(K,), name='Lambda')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know that our data come from a Gaussian mixture distribution, but we don't know anything else. We create a Gaussian mixture distribution object:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "from bayespy.nodes import Mixture\n",
      "\n",
      " Y = Mixture(Z, Gaussian, mu, Lambda, name='Y') #This is the object\n",
      " Y.observe(obs) #and these are the model observations - the data."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we randomly initialize our cluster assignments $Z$ to start everything off:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#This part will only work with Python 3.0\n",
      "Z.initialize_from_random()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're ready to perform inference (get distributional estimates) on the hidden values, given the data:\n",
      "\n",
      "Z (cluster assignments), \n",
      "\n",
      "$\\Lambda$ (the precision matrix), \n",
      "\n",
      "$\\mu$ (the vector of means),\n",
      "\n",
      "$K$ (the number of clusters), and\n",
      "\n",
      "$\\pi$ (the probability of being in a cluster)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Inference####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We create a Variational Bayes object (same as variational inference in this case). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#This part will only work with Python 3.0\n",
      "from bayespy.inference import VB\n",
      "\n",
      "Q = VB(Y, mu, Lambda, Z, alpha)\n",
      "\n",
      "Q.update(repeat=1000)\n",
      "\n",
      "#This is the output:\n",
      "#Iteration 1: loglike=-1.402345e+03 (... seconds)\n",
      "#...\n",
      "#Iteration 61: loglike=-8.888464e+02 (... seconds)\n",
      "#Converged at iteration 61.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of the results can be visualized in the following plot:\n",
      "    \n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/FromOnline2.png\" alt=\"\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the algorithm has found all four clusters, and has figured out the cluster means and covariances, visualized by the ellipsoids. It has also figured out which observations belong to each cluster, with a few expections. It's clear that it would be hard to figure out the cluster assignment for the data point on the far right, for example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Conclusion ###\n",
      "\n",
      "Variational inference is useful when the posterior distribution is intractable. It is similar to several methods, like the EM (expectation-maximization) algorithm, especially for mixtures of Gaussians. According to Bishop (2006), there are some advantages to variational inference over the EM algorithm:\n",
      "\n",
      "1. The singularities that we see in the (frequentist) EM algorithm do not arise in this Bayesian treatment.\n",
      "\n",
      "2. If we choose a large number of initial clusters (here we chose K = 10), we don't have to worry about overfitting. \n",
      "\n",
      "3. Finding the optimal value of K did not require the extra step of cross-validation.\n",
      "\n",
      "Variational methods have been extended beyond the mean-field assumption, and have been extended to stochastic versions (see Hoffman et al 2013). Future work lies in moving away from exponential family distributions, finding theoretical guarantees in the stochastic algorithm, and more."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### References ###\n",
      "\n",
      "Bishop, C.M. (2006) Pattern Recognition and Machine Learning. Springer.\n",
      "\n",
      "Bishop, C.M. (1998) Variational Learning in Graphical Models and Neural Networks.  Proceedings 8th International Conference on Artificial Neural Networks, ICANN'98, L.Niklasson et al. (eds), Springer (1998) 1, 13-22.\n",
      "\n",
      "Brodersen, K.H. Variational Bayesian Inference. [Online] http://people.inf.ethz.ch/bkay/talks/Brodersen_2013_03_22.pdf\n",
      "\n",
      "Feng, Z. Assignment 3, STAT 6841: Statistical Inference. Winter 2015. University of Guelph. Guelph, ON, Canada.\n",
      "\n",
      "Hoffman et al (2013). Stochastic Variational Inference. Journal of Machine Learning Research (14), 1303-1347.\n",
      "\n",
      "Jaakko Luttinen and Hannu Hartikainen. BayesPy. http://www.bayespy.org/examples/gmm.html\n",
      "\n",
      "Munroe, R. xkcd. Frequentists vs Bayesians. http://xkcd.com/1132/\n",
      "\n",
      "R dataset (Faithful). R Core Team (2013). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL http://www.R-project.org/.\n",
      "\n",
      "Sridharan, R. Gaussian Mixture Models and the EM Algorithm. http://people.csail.mit.edu/rameshvs/content/gmm-em.pdf"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}