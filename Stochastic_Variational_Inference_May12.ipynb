{
 "metadata": {
  "name": "Presentation-StochVarInf.ipynb",
  "signature": "sha256:bd3c59223b424971103aeae524c4360c1f18cbd5e1065c7376426621ea2aeb68"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction to Stochastic Variational Inference #\n",
      "\n",
      "This presentation is based on the paper Stochastic Variational Inference by Hoffman et al 2013, with a lot of content also from David Blei's 2012 presentation at an event called Big Learning: Algorithms, Systems, and Tools. \n",
      "\n",
      "In Bayesian statistics, inference is carried out through the posterior distribution of the model parameters and/or hidden variables given visible (input, observed, data) variables. For high-dimensional inference problems, the posterior distribution is intractable, due to an inability to calculate the normalization constant. One method of solving this problem is to sample from the posterior using a Markov chain Monte Carlo (MCMC) method. Another currently popular approach is variational Bayesian (VB) methods, which create an approximation of the posterior and conduct posterior inference based on the approximation.\n",
      "\n",
      "In this introduction, classical (non-stochastic) mean-field variational inference will be discussed as a stepping stone to the more complicated stochastic version of the algorithm. \n",
      "\n",
      "##Outline ##\n",
      "\n",
      "Mean-field variational inference\n",
      "\n",
      "Natural gradients \n",
      "\n",
      "Stochastic optimization\n",
      "\n",
      "Stochastic variational inference\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------------------\n",
      "###Mean-field variational inference###\n",
      "\n",
      "#### The setup ####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we have a model with the following general setup:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$N$ observations $x_n, n = 1, \\ldots, N$\n",
      "\n",
      "$\\beta$, a vector of global hidden variables\n",
      "\n",
      "$z = z_{1:N}$, local hidden variables. $z_{n, 1:J}$\n",
      "\n",
      "$\\alpha$, a vector of fixed parameters. We choose to allow $\\alpha$ to govern the prior over $\\beta$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model can be visualized using a plate diagram:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/PlateModel.png\" alt=\"A plate model for the model with N observations\" style=\"width:250px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plate model (above) is a schematic for a model distribution, showing the conditional dependencies. In a plate model, a rectangle with a total (n) in the bottom right indicates that all variables or parameters inside the rectangle (the \"plate\") are indexed from 1 to n. So we have a pictoral representation of $z_i : i = 1, \\ldots, n$ and $x_i : i = 1, \\ldots, n$. The notation for unobserved variables (hidden units, in this case $z_i$ and $\\beta$) in a plate diagram is an unfilled circle. Observed variables (visible units, in this case $x_i$) are filled circles. Outside the plate are variables or parameters that are not repeated n times. That is, $\\beta$ is a vector of global hidden variables, which means the length of the vector is not necessarily $n$. The arrows between components in the plate model show how each variable or parameter is related. For example, the arrow from $\\beta$ to $z_i$ indicates that the value of $z_i$ dependes on the value of $\\beta$. Taken as a whole, the plate model shows that the joint (model) distribution $P(x, z, \\beta)$, and illustrates the dependencies in the model:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{align}\n",
      "P(x, z, \\beta) &= P(\\beta) \\prod_{i=1}^n P(z_i \\mid \\beta) P(x_i \\mid z_i, \\beta) %P(x_i, z_i \\mid \\beta)\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal, as usual, is to infer the posterior distribution over the hidden variables, given the observed data:\n",
      "\n",
      "$$P(\\beta, z \\mid x)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, the problem is that the posterior distribution is intractable, because the normalization constant $P(x)$ (aka the marginal likelihood, the model evidence, the partition function) is intractable. Recall the relationship from Bayesian statistics:\n",
      "\n",
      "$$P(A, B \\mid C) = \\frac{P(A, B, C)}{P(C)}$$\n",
      "\n",
      "The relationship here shows that the posterior distribution is given by the ratio of the joint distribution and the model evidence. Using this relationship, we can show the formula needed to calculate the posterior distribution:\n",
      "\n",
      "$$P(\\beta, z \\mid x) = \\frac{P(x, z, \\beta)}{\\int_z \\int_\\beta P(x, z, \\beta)dzd\\beta}$$\n",
      "\n",
      "where the denominator comes from marginalizing over the global hidden variables $\\beta$ and the local hidden variables $z$, which means the denominator becomes a function of the data alone (hence calling it the marginal likelihood)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have the joint distribution from our model, and what we're lacking is the normalization constant. Since the normalization constant is a function of x alone, denote it $P(x)$. That is, let:\n",
      "\n",
      "$$P(x) = \\int_z \\int_\\beta P(x, z, \\beta)dzd\\beta$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since it's often easier to work with the log of probability functions, we'll work with ${\\rm{ln\\ }} P(x)$ here (remember statisticians say \"log\" when we mean \"ln\" - just to mess with everyone!)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The mean-field assumption ####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In statistical physics, mean field theory has long been used to model complex systems using relatively simple ones. A huge assumption is made, though, that all of the hidden units $\\beta$ and $z_1, z_2, \\ldots, z_n$ are mutually independent, and the local hidden variables are also independent of one another within their context (for example, in the first context, $z_{11} \\perp z_{12} \\perp \\cdots \\perp z_{1J}$). This assumption is often false, which leads to alternate methods of variational inference, but it also has some nice properties, so that's where we'll start. To summarize, the mean-field assumption is the assumption that all hidden variables are independent of one another."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####The lower bound on the marginal likelihood ####\n",
      "\n",
      "\n",
      "To get to the objective function we need to minimize the distance between an approximating distribution $Q$ (called the \"variational distribution\") and the true posterior distribution $P(\\beta, z \\mid x)$, we need to get an appropriate distance metric. The distance metric we'll use is based on maximizing the log probability of the data (the marginal likelihood, ${\\rm{ln\\ }} P(x)$). We can get a lower bound on the marginal likelihood in the following way:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(x) &= {\\rm{ln}} \\int_\\beta \\int_z P(\\beta, z, x)\\ d\\beta\\ dz \\\\[0.5em]\n",
      "&= {\\rm{ln}} \\int_\\beta \\int_z Q(\\beta, z) \\frac{P(\\beta, z, x)}{Q(\\beta, z)}\\ d\\beta\\ dz \\\\[0.5em]\n",
      "&= {\\rm{ln\\ }} E_Q \\left[\\frac{P(\\beta, z, x)}{Q(\\beta, z)} \\right] {\\rm{\\, since\\ E_X(g(X)) = \\int_x g(x)f(x) dx,\\ by\\ definition\\ of\\ expectation,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(\\beta, z)}}\\\\[0.5em]\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[\\frac{P(\\beta, z, x)}{Q(\\beta, z)} \\right] {\\rm{\\, by\\ Jensen's\\ Inequality}} \\\\[0.5em]\n",
      "&= E_Q \\left[{\\rm{ln\\ }} P(\\beta, z, x)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(\\beta, z)}\\right] \\\\[0.5em]\n",
      "&= {\\mathcal{L}}(Q) \\\\\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "{\\rm{ln}}\\ P(x) &= {\\rm{ln}} \\int_\\beta \\int_z P(\\beta, z, x)\\ d\\beta\\ dz \\\\[0.5em]\n",
        "&= {\\rm{ln}} \\int_\\beta \\int_z Q(\\beta, z) \\frac{P(\\beta, z, x)}{Q(\\beta, z)}\\ d\\beta\\ dz \\\\[0.5em]\n",
        "&= {\\rm{ln\\ }} E_Q \\left[\\frac{P(\\beta, z, x)}{Q(\\beta, z)} \\right] {\\rm{\\, since\\ E_X(g(X)) = \\int_x g(x)f(x) dx,\\ by\\ definition\\ of\\ expectation,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(\\beta, z)}}\\\\[0.5em]\n",
        "&\\ge E_Q\\ {\\rm{ln}} \\left[\\frac{P(\\beta, z, x)}{Q(\\beta, z)} \\right] {\\rm{\\, by\\ Jensen's\\ Inequality}} \\\\[0.5em]\n",
        "&= E_Q \\left[{\\rm{ln\\ }} P(\\beta, z, x)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(\\beta, z)}\\right] \\\\[0.5em]\n",
        "&= {\\mathcal{L}}(Q) \\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x105b35050>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first expected value term here is the expected value of the joint distribution, which is our model distribution, and assumed known. The second expected value term is the entropy of $Q$. The two terms together constitute a lower bound on the marginal likelihood, ${\\mathcal{L}}(Q)$. This lower bound is also called the \"expectation lower bound\", or ELBO. \n",
      "\n",
      "Aside: Jensen's Inequality applied here is ${\\rm{ln}}(E(X)) \\ge E({\\rm{ln}} (X))$, and more information is available [here](http://en.wikipedia.org/wiki/Jensen's_inequality). The vast majority of this derivation is given in Bishop (1998) and again in Hoffman (2013)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####The objective function ####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In variational inference, we want to approximate that intractable posterior distribution by some known, nice function $Q$. To find the optimal $Q$, we need to define a relationship between ${\\rm{ln\\ }} P(x)$ and $Q$, and get an objective function based on that relationship.\n",
      "\n",
      "The mean-field assumption leads to a rather nice result, namely that we can put a lower bound on the log probability of the data:\n",
      "\n",
      "$${\\rm{ln}}\\ P(x) = {\\rm{\\mathcal{L}}}(Q) + {\\rm{KL}}(Q \\ \\| \\ P(\\beta, z \\mid x))$$\n",
      "\n",
      "where ${\\rm{\\mathcal{L}}}(Q)$ is the ELBO, and ${\\rm{KL}}(Q \\ \\| \\ P(\\beta, z \\mid x))$ is the KL divergence between the approximating distribution $Q$ and the true posterior distribution. This relationship can be derived by expanding the KL divergence, and expanding the ELBO, then adding them together, cancelling out some terms and remembering that Q is a probability density function (which means the integral of $Q$ over its domain is equal to 1). Details were given in January, and I can send them to you if you like."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So maximizing the ELBO $\\mathcal{L}(Q)$, with respect to $Q$ is the same as minimizing the KL divergence between the variational distribution $Q$ and the true posterior distribution $P(\\beta, z \\mid x)$. This is more easily seen through a picture (from Bishop 1998):\n",
      "\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/KLpicture.png\" alt=\"Illustration of the KL divergence minimization being equivalent to maximization of the lower bound\" style=\"width:250px\">\n",
      "\n",
      "In this diagram, ${\\rm{ln\\ }} P(V \\mid w)$ is the probabilty of the visible units given the model parameters, which is equivalent to ${\\rm {ln\\ }} P(x)$ here. Note that we can't compute the KL divergence here for the same reason that we couldn't compute the posterior - it requires the marginal distribution. Instead, we focus on maximizing the ELBO:\n",
      "\n",
      "$$\\mathcal{L}(Q) = E_Q \\left[{\\rm{ln\\ }} P(\\beta, z, x)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(\\beta, z)}\\right]$$\n",
      "\n",
      "#### Choosing Q ####\n",
      "Recall the minimum value of the KL divergence metric is 0. If we could choose any $Q$ we like, we would choose it such that $Q = P(\\beta, z \\mid x)$, which would give a KL divergence of 0. That would mean, though, that we knew the form of the posterior distribution, which we don't. So instead, we choose a \"best\" $Q$ out of a particular family of functions. Corresponding to the mean-field assumption, we choose the following factorized form for the variational distribution:\n",
      "\n",
      "$$ Q(\\beta, z) = Q(\\beta \\mid \\lambda) \\prod_{i=1}^n Q_i (z_{i} \\mid \\phi_{i}) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Optimizing $Q(\\beta, z)$ amounts to finding the variational parameters (sometimes called \"free-form parameters\") $\\lambda$ and $\\phi$ that minimize the KL divergence between $Q(\\beta, z)$ and the true posterior distribution. This form gives each hidden variable its own variational parameter. The global hidden variable $\\beta$ has variational parameter $\\lambda$. The notation $z_i$ indexes the i^{th} local hidden variable and each $z_i$ has its own variational (free) parameter, $\\phi_i$\n",
      "\n",
      "<!---\n",
      "The notation $z_{ij}$ indexes the $i^{th}$ context (or sample, or input vector) and the $j^{th}$ attribute from that particular input vector, and each $z_{ij}$ has its own variational parameter $\\phi_{ij}$. \n",
      "--->\n",
      "\n",
      "For example, in Bayesian mixtures of Gaussians, there would be a local hidden variable $z_i$ for each input $x_i$ corresponding to the mixture label for that particular input, and the global hidden variable $\\beta$ would be the vector of mixture proportions, and the means and variances of the mixture components (Hoffman et al 2013). In a more complicated model, there would be multiple local hidden variables per input vector, giving a need for a second subscript. In Hoffman et al (2013), the example is topic modelling.\n",
      "\n",
      "Now we need to get to the distributions of the variational parameters, $\\lambda$ and $\\phi$. To do this, we're going to need to go into some exponential family properties."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exponential family models ####\n",
      "\n",
      "Stochastic variational inference is currently done using exponential family distributions. These distributions are useful for a number of reasons, not least of which is the property of conjugacy. Recall from Bayes Theorem:\n",
      "\n",
      "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)} = \\frac{P(B \\mid A) P(A)}{P(B)}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the prior, $P(A)$ is of the same family as the posterior, $P(A \\mid B)$, then the prior and the posterior are conjugate distributions. Exponential family distributions will always have a conjugate distribution. This is not true for all non-exponential family models.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are several general forms of exponential family models. Following the notation in Hoffman et al (2013), we'll use the following exponential family form:\n",
      "\n",
      "$$f_x (x \\mid \\theta) = h(x) {\\rm {exp}}\\left[ \\eta(\\theta) T(x) - a(\\eta)\\right]$$\n",
      "\n",
      "where \n",
      "\n",
      "$h(x)$ is the \"base measure\" - a function of the data alone, completely independent of the hidden variables.\n",
      "\n",
      "$\\eta$ is the \"natural parameter\" (aka the \"canonical parameter\") of the distribution\n",
      "\n",
      "$T(x)$ is a vector of sufficient statistics\n",
      "\n",
      "$a$ is the \"log normalizer\", which assumes a form that forces the distribution to be normalized.\n",
      "\n",
      "All exponential family models (for example, the Gaussian distribution, the Binomial distribution, the exponential distribution, etc) can be written in this form. \n",
      "\n",
      "Anyone who would like to look more into this might want to consult [this lecture](http://www.genome.duke.edu/labs/engelhardt/courses/scribe/lec_09_02_2013.pdf). \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the mean-field framework, we introduce the assumption that all hidden variables are independent. We can also introduce another assumption, namely that the complete conditional distributions come from the exponential family.\n",
      "\n",
      "A complete conditional distribution is the conditional distribution of a particular hidden variable given all other hidden variables and all observed variables. So we will have two complete conditional distribution forms: one for the global hidden variable $\\beta$ and one for the local hidden variable $z_{ij}$:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$P(\\beta \\mid x, z) = h(\\beta) {\\rm{\\ exp}} \\left[\\eta_g (x, z)^T\\ \\beta - a(\\eta_g(x, z))\\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$P(z_{i} \\mid x_i, \\beta) = h(z_{i}) {\\rm{\\ exp}} \\left[\\eta_\\ell (x_i, \\beta)^T\\ z_{i} - a(\\eta_\\ell(x_i, \\beta))\\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since both $P(\\beta \\mid x, z)$ and $P(z_{ij} \\mid x_i, \\beta)$ are assumed to come from exponential family distributions, there is a conjugacy relationship between the global hidden variables $\\beta$ and the local variables $x_i$ and $z_i$:\n",
      "\n",
      "$$P(x_i, z_i \\mid \\beta) = h(x_i, z_i) exp \\left[ \\beta^T T(x_i, z_i) - a_{\\ell} (\\beta)\\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need these distributions so we can take the required gradients later on. Now that we have the complete conditional of $\\beta$, we can define a distribution over its variational family, knowing that it will also have an exponential family distribution (since we define a one-to-one relationship between the hidden variables and the variational parameters):\n",
      "\n",
      "$$Q(\\beta \\mid \\lambda) = h(\\beta) {\\rm {exp}} \\left[ \\lambda^T \\beta - a(\\lambda) \\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similarly, we can also define a distribution over the variational family of $z_i$ $\\phi$:\n",
      "    \n",
      "$$Q(z_i \\mid \\phi_i) = h(z_i) {\\rm {exp}} \\left[ \\phi_i^T z_i - a(\\phi_i) \\right]$$\n",
      "\n",
      "Again, we need these distributions so we can construct ${\\rm{ln\\ }} Q(\\beta, z)$ and take the gradient later on."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Maximizing the ELBO with respect to the variational parameters ####\n",
      "\n",
      "Here is our objective function, now that everything has (finally!) been defined:\n",
      "\n",
      "$$\\mathcal{L}(\\lambda, \\phi_{1:n}) =  E_Q \\left[{\\rm{ln\\ }} P(\\beta, z, x)\\right] - E_Q \\left[{\\rm{ln\\ }} {Q(\\beta, z)}\\right]$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "remember that maximizing this lower bound is the same as finding the distribution $Q(\\beta, z)$ that is closest in KL divergence to the true posterior distribution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To maximize this function we need to resort to numerical methods (big surprise), and use coordinate ascent. To do this, we hold all parameters fixed but one, then perform coordinate ascent on that parameter, and move on to the next parameter and so on."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This means we have to take the gradient of $\\mathcal{L}(\\lambda, \\phi_{1:n})$ with respect to each variational parameter. Without going into derivations, here are the gradients we need:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\nabla_\\lambda \\mathcal{L} = a''(\\lambda) E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "which leads to this update:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\lambda^{*} = E_\\phi \\left[\\eta_g (x, z) \\right]$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, we're taking the expectation with respect to all of the other variational parameters, $\\phi$ (we're holding them constant, but they still have a distribution, so we can still take this expectation). The local variational parameters are similar, and the update is shown in the algorithm below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Algorithm for mean-field variational inference ####\n",
      "\n",
      "Randomly initialize $\\lambda$\n",
      "\n",
      "Until ELBO convergence:\n",
      "\n",
      "1) For each observation, update the local variational parameters according to $\\phi_i^{(t)} = E_{\\lambda^{(t-1)}}\\left[\\eta_\\ell (x_i, \\beta) \\right]$ for $i \\in 1, \\ldots, n$\n",
      "\n",
      "2) Update the global variational parameters according to $\\lambda^{(t)} = E_\\phi^{(t)} \\left[\\eta_g (x_{1:n}, z_{1:n}) \\right]$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: you need to analyze the entire dataset before completing one iteration of this algorithm. That makes it very inefficient."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----------------------------\n",
      "### The Natural Gradient ###\n",
      "\n",
      "To go from vanilla variational inference to stochastic variational inference, we need to jump through some hoops. One of those hoops is the natural gradient. I'll only go over some intuition, but anyone who's interested in details can have a look at Amari (1998). The takeaway message is, we need to take derivatives in a Riemannian space instead of a Euclidean space, and to do that we need some fancy math. Then we apply the fancy math and get a nicer version of the gradient for the update.\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### The Riemannian Metric ####\n",
      "\n",
      "Suppose we have a function space that is indexed by a parameter $p$ (so if you pick different values of $p$ in this space, you get different probability distributions), and we pick a starting value for the parameter, $p_0$. That value $p_0$ corresponds to a probability distribution with parameter value $p_0$. If we were to move in any direction in that space, we would get a probability distribution corresponding to a different value of $p$. Suppose the space is only 3 dimensional, so a unit change in $p$ would mean that we were somewhere on the unit ball with $p_0$ in the centre. Suppose our move takes us in a direction $d_1$. Then we have a new probability distribution parameterized by $p_{d_1}$. This new probability distribution has a KL divergence with the initial distribution. \n",
      "\n",
      "Now say instead we went in a different direction on the unit ball with $p_0$ in the centre. Call this new direction $d_2$. Then we get another probability distribution that has parameter $p_{d2}$. \n",
      "\n",
      "Even though the distance between $p_0$ and $p_{d1}$ is equal to the distance between $p_0$ and $p_2$ in Euclidean space, their KL divergences will not necessarily be equal.\n",
      "\n",
      "The example given in Hoffman et al (2013) illustrates this problem nicely. Suppose we're interested in the univariate normal distribution. The distributions N(0, 10000) and N(10, 10000) are almost indistinguishable, and the Euclidean distance between their parameter vectors is 10; however, the distributions N(0, 0.01) and N(0.1, 0.01) barely overlap, but they have a much smaller Euclidean distance between their parameter vectors (0.1). \n",
      "\n",
      "\n",
      "\n",
      "So Euclidean distance is a poor measure of difference between distributions, and we need a way to convert this Euclidean space where distances are measured based on an $L_2$ norm to a Riemannian space, where we can measure differences using a symmetrized KL divergence. We need a symmetrized KL divergence because otherwise the 'distance' from p to q would not be the same as the distance from q to p, if p and q are probability distributions in some probability space.\n",
      "\n",
      "$$D_{KL}^{sym} (\\lambda, \\lambda') = E_{\\lambda} \\left[ log \\displaystyle{\\frac{Q(\\beta \\mid \\lambda)}{Q(\\beta \\mid \\lambda')}} \\right] + E_{\\lambda'} \\left[ log \\displaystyle{\\frac{Q(\\beta \\mid \\lambda')}{Q(\\beta \\mid \\lambda)}} \\right]$$\n",
      "\n",
      "In natural gradient ascent, we premultiply the gradient that we already have by the inverse of the Riemannian metric. In probability distributions, the Riemannian metric is just the Fisher Information."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### What the update looks like now####\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For exponential family distributions, the Fisher Information is the second derivative of the log normalizer (the \"a\" function from the exponential family distribution definition). I'm not going to prove that here, because it's shown explicitly in Hoffman et al (2013)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So for the update to the global latent variable, which is based on its variational parameter $\\lambda$, the Fisher Information metric is defined as:\n",
      "$G(p) = a''(\\lambda)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the original gradient with respect to $\\lambda$ was:\n",
      "\n",
      "$$\\nabla_\\lambda \\mathcal{L} = a''(\\lambda) E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So when we premultiply by $a''^{-1}(\\lambda)$, we get:\n",
      "\n",
      "$$a''^{-1}(\\lambda)\\nabla_\\lambda \\mathcal{L} = E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda$$\n",
      "\n",
      "Or, using the notation in Hoffman et al (2013):\n",
      "\n",
      "$$\\hat{\\nabla}_\\lambda \\mathcal{L} = E_\\phi \\left[\\eta_g (x, z) \\right] - \\lambda$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(I'm not sure why we didn't have to worry about $a''(\\lambda)$ being multiplied by $\\lambda$)\n",
      "\n",
      "And analogously:\n",
      "\n",
      "$$\\hat{\\nabla}_{\\phi_i} \\mathcal{L} = E_\\lambda \\phi_{-i} \\left[\\eta_\\ell (x_i, z_i, \\beta) \\right] - \\phi_i$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hoffman et al (2013) alleges you can compute the natural gradient by \"computing the coordinate updates in parallel and subtracting the current setting of the parameters\". I don't see it yet. I understand the \"subtracting the current setting of the parameters\" part."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The advantage is that now, of course, we don't have to worry about computing the Fisher Information matrix, like in traditional variational inference. That's pretty costly, so that's now saved."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stochastic optimization ###\n",
      "\n",
      "That coordinate ascent algorithm is still inefficient for large datasets, because we haven't eliminated the need to see all the observations before making an update. Here's where that problem gets solved. We're going to use stochastic optimization on the global hidden variables (because finding the local ones is computationally cheap - it's the global ones that cause the problem)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The way stochastic optimization works is to get a sample of the thing you want to optimize, then take a gradient based on only that sample. This is called a \"noisy estimate\" of the gradient, where the noise is exclusively sampling noise. You start with a big step size in the direction of the initial gradient. Then, as you see more and more observations, you decrease the step size.\n",
      "\n",
      "Since the gradient is composed of additive terms (one for each observation), it's easy to compute a noisy approximation of the gradient by sampling (you just don't have a lot of those terms at the beginning, and get more of them as you sample). As long as certain conditions on the step-size schedule are met, this will converge to a global optimum (if the function is convex) and a local optimum otherwise.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Stochastic natural gradients####\n",
      "\n",
      "Still following the notation in Hoffman et al (2013), suppose we have an objective function $f(\\lambda)$ and a random function $B(\\lambda)$ such that $E_q \\left[ B(\\lambda) \\right] = \\nabla_{\\lambda} f(\\lambda)$. Then we can optimize $f(\\lambda)$ by following samples $b_t$ from the function $B(\\lambda)$. The update for $\\lambda$ is then:\n",
      "\n",
      "$$\\lambda^{(t)} = \\lambda^{(t-1)} + \\rho_t b_t(\\lambda^{(t-1)})$$\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The conditions the step-size schedule needs to satisfy are:\n",
      "\n",
      "$$\\sum \\rho_t = \\infty; \\sum \\rho_t^2 < \\infty$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now if we use natural gradients instead of Euclidean gradients, then we have:\n",
      "    \n",
      "$$\\lambda^{(t)} = \\lambda^{(t-1)} + \\rho_t G_t^{-1} b_t(\\lambda^{(t-1)})$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stochastic Variational Inference ###\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So what we're worried about is maximizing the ELBO with respect to the global variational parameter $\\lambda$. Again skipping a lot of derivation, we can write the ELBO with respect to $\\lambda$ in the following form:\n",
      "\n",
      "$$\\mathcal{L} (\\lambda) = E_Q \\left[ {\\rm {ln\\ }} P(\\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(\\beta \\mid \\lambda)\\right] + \\sum_{i=1}^n \\left(E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now if we had only one observation instead of all $n$ (that is, if we sampled 1 data point from our dataset according to a Uniform distribution), and pretended that this one sampled data point were the only data point we would ever see (so we replicate it n times) then we would have the following form instead:\n",
      "\n",
      "$$\\mathcal{L}_i (\\lambda) = E_Q \\left[ {\\rm {ln\\ }} P(\\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(\\beta \\mid \\lambda)\\right] + n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The stochastic gradient algorithm works because the this version of the ELBO with respect to 1 observation is an unbiased estimator of the true ELBO with respect to all of the observations. In other words, the expected value of $\\mathcal{L}_i \\lambda$ with respect to the sampled observation is equal to the actual ELBO with respect to all n observations, $\\mathcal{L} \\lambda$. \n",
      "\n",
      "You can see this because if we took the expectation with respect to the sampled observation, and we had sampled independently with probability 1/n, the only term with a $x_i$ (the observation) in $\\mathcal{L}_i \\lambda$ is \n",
      "\n",
      "$$n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)$$\n",
      "\n",
      "The rest will be constants with respect to $x_i$, so we don't care about them when computing this expectation.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The sampling procedure will have a discrete Uniform distribution. recall the definition of expectation for a discrete random variable:\n",
      "    \n",
      "$$E(f(X)) = \\sum_{x} f(x) p(x)$$\n",
      "\n",
      "But $p(x) = 1/n$, so :\n",
      "\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "E(f(X)) &= \\sum_{x} f(x) p(x) \\\\[0.5em]\n",
      "&=\\sum_{i=1}^n\\left[n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\frac{1}{n} \\\\[0.5em]\n",
      "&= \\sum_{i=1}^n\\left[\\frac{n}{n}\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\\\[0.5em]\n",
      "&= \\sum_{i=1}^n \\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right) \\\\[0.5em]\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "E(f(X)) &= \\sum_{x} f(x) p(x) \\\\[0.5em]\n",
        "&=\\sum_{i=1}^n\\left[n\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\frac{1}{n} \\\\[0.5em]\n",
        "&= \\sum_{i=1}^n\\left[\\frac{n}{n}\\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right)\\right] \\\\[0.5em]\n",
        "&= \\sum_{i=1}^n \\left( E_Q \\left[ {\\rm {ln\\ }} P(x_i, z_i \\mid \\beta) \\right] - E_Q \\left[ {\\rm {ln\\ }} Q(z_i) \\right]\\right) \\\\[0.5em]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x105b0f050>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the expected value of the sampled ELBO is the true ELBO, which makes this sampled ELBO an unbiased estimate of the ELBO. \n",
      "\n",
      "If we take the natural gradient with respect to $\\lambda$ of the sampled ELBO, then the natural gradient of the sampled ELBO will be an unbiased estimate of the natural gradient of the true ELBO (proof omitted). So the gradient of the sampled ELBO is a noisy gradient of the true ELBO.\n",
      "\n",
      "The sampled ELBO is an ELBO where we saw the same observation $n$ times. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again skipping over a bunch of derivation, the formula for the natural gradient of the sampled ELBO is:\n",
      "    \n",
      "$$\\nabla_\\lambda \\hat{\\mathcal{L}}_i = E_{\\phi_i} \\left[ \\eta_i(z_i, x_i) \\right] - \\lambda$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we only need the local variational parameters for one observation, instead of having to calculate all $n$ of them. If we were to calcluate the full natural gradient according to that $\\mathcal{L} (\\lambda)$, then we would have to use all of the local parameters $\\phi_{1:n}$.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That result leads, at last, to the algorithm for stochastic variational inference."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Algorithm for Stochastic Variational Inference ####\n",
      "\n",
      "Randomly initialize global parameter(s) $\\lambda$\n",
      "Set a step size schedule $\\epsilon_t$\n",
      "Repeat\n",
      "\n",
      "1) Sample an observation $x_i$ from your dataset, according to a DU[1, n] distribution.\n",
      "2) Compute the local variational parameter $\\phi_i$ for that particular observation, according to:\n",
      "\n",
      "  $$\\phi = E_{\\lambda^{(t-1)}}\\left[\\nabla_\\ell (x_i, \\beta) \\right]$$\n",
      "  \n",
      "3) Make believe that $x_i$ is the only observation in the dataset, and calculate\n",
      "\n",
      "  $$\\hat{\\lambda} = E_{\\phi} \\left[ \\eta_i (x_i, z_i)\\right]$$\n",
      "  \n",
      "4) Update the current estimate of the global variational parameter:\n",
      "\n",
      "  $$\\lambda^{t} = (1 - \\epsilon_t) \\lambda^{(t-1)} + \\epsilon_t\\hat{\\lambda}$$\n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "remember the step size decreases as a function of $t$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}