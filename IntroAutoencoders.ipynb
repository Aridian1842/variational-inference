{
 "metadata": {
  "name": "",
  "signature": "sha256:0b8129390693a76f5ad22b0fccce3b672baef8fd0596ee1ba0002f0bcc3548d7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Autoencoders#\n",
      "\n",
      "Autoencoders are a class of model architecture that learns to model the input X with as little distortion as possible, using a distributed representation (Baldi, 2012). A traditional autoencoder consists of an input layer, a hidden layer, and an output layer, with directed connnections from input to hidden and from hidden to output (Figure 1, Baldi 2012). While this sounds a lot like a multilayer perceptron (MLP), an autoencoder has the same number of output units as input units, whereas an MLP has fewer output units. The point of an autoencoder is to reconstruct the input as faithfully as possible, whereas a multilayer perceptron is used, for example, for classification (so the output layer would have k nodes, where k is the number of classes). \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}