{
 "metadata": {
  "name": "",
  "signature": "sha256:555817abd3280c3ab952311859a4d8a8111cd35b2f68a646e3c912a7c4a60733"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Autoencoders#\n",
      "\n",
      "Autoencoders (or autoassociators) are a class of feedforward neural network that learns to model the input $x$ with as little distortion as possible (Baldi, 2012). A traditional autoencoder consists of an input layer, a hidden layer, and an output layer, with directed connnections from input to hidden and from hidden to output (Figure 1). Deep autoencoders have more hidden layers, but retain a symmetric structure (Michl, 2013).\n",
      "\n",
      "<!--\n",
      "<div style=\"float: right\">\n",
      "    <img src=\"http://placehold.it/85x85\" alt=\"Your alt text\" title=\"Title\"/>\n",
      "</div>\n",
      "-->\n",
      "\n",
      "<div style=\"float: right\">\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/AutoEncoder.png\" alt=\"\" style=\"width:400px\">\n",
      "</div>\n",
      "\n",
      "While this sounds a lot like a multilayer perceptron (MLP), an autoencoder has the same number of output units as input units, whereas an MLP has fewer output units. The point of an autoencoder is to reconstruct the input as faithfully as possible, whereas a multilayer perceptron is used, for example, for classification (so the output layer would have k nodes, where k is the number of classes, and typically k << n, where n is the number of inputs $x$). Autoencoders are used for unsupervised learning tasks such as feature extraction, generating more unlabelled data that closely resembles the original data, and as a starter for a subsequent classification network. Another important aspect to autoencoders is that their learning paradigm is entirely unsupervised: that is, there is no target (per se - you could also say the input is the target).  \n",
      "\n",
      "\n",
      "\n",
      "<!--\n",
      "![alt text](https://github.com/caugusta/variational-inference/raw/master/AutoEncoderBaldi2012.png \"A typical autoencoder, with n inputs, p hidden units, and n outputs\"{width=100px)\n",
      "-->\n",
      "\n",
      "\n",
      "Here, the input layer is denoted $x$, the hidden layer $h$, and the output layer $\\bar{x}$. The weights from the input layer to the hidden layer are often called the recognition weights. The weights from the hidden layer to the output are the generative weights (Hinton and Zemel, 1994). We often choose to tie the weights (that is, we choose the generative weights to be equal to the recognition weights, transposed). There's no necessity to doing so, but it has a regularization effect (forcing the decoder weights to be the same as the encoder weights strictly limits which weights can be chosen for the decoder), and it prevents degenerate solutions (Delallau). The dimensionality $p$ of the hidden layer is a parameter to be tuned during learning. Included in the weight matrix is a vector for the bias on each neuron. Much of the notation in this iPython notebook follows Hugo Larochelle's lecture videos. \n",
      "\n",
      "<!-- \n",
      "Is there a connection between autoencoders and bootstrapping? They can accomplish the same goals, so are they in any way mathematically similar?\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "The transformation from input to hidden units is denoted B, and the transformation from hidden to output is A. The transformation B is a weight matrix that takes the input layer and multiplies each input node by a constant. \n",
      "\n",
      "\n",
      "We often choose to tie the weights, although we don't have to. That is, we often make the weights from the input layer (often called the recognition weights) to the hidden layer equal to the weights from the hidden layer to the output layer (but transposed, of course - these are called the generative weights) (Hinton and Zemel, 1994).\n",
      "\n",
      "-->\n",
      "\n",
      "An autoencoder consists of an encoder, a function taking the data and learning a representation of the input, and a decoder, which maps from the representation back to the input.\n",
      "\n",
      "There's a hidden layer pre-activation function that takes the input layer and multiplies it by the recognition weights:\n",
      "\n",
      "$$ a(x) = Wx $$\n",
      "\n",
      "There's a hidden layer activation function that transforms a(x) (this is the encoder - the result is a 'code', $h$):\n",
      "\n",
      "$$ h = g(a(x))$$\n",
      "\n",
      "There's an output layer pre-activation function that multiplies h by the generative weights:\n",
      "\n",
      "$$ \\hat{a}(x) = W^{T}h $$\n",
      "\n",
      "And there's an output layer activation function that transforms $\\hat{a}(x)$ (this is the decoder - the result is a reconstruction of the input, $\\bar{x}$):\n",
      "\n",
      "$$ \\bar{x} = m(\\hat{a}(x)) $$\n",
      "\n",
      "<!--\n",
      "An autoencoder consists of two main parts: an encoder and a decoder. The encoder is a mapping from input units to hidden units; the decoder goes from hidden units to \"output\" units. The encoder function is often a sigmoid nonlinearity; that is:\n",
      "\n",
      "-->\n",
      "\n",
      "We can choose, for example, a sigmoid activation function (when we use binary units - more on this later) for the encoder (so $g$ is a sigmoid):\n",
      "\n",
      "$$h = \\frac{1}{1 + {\\rm{exp}}(-(Wx))}$$\n",
      "\n",
      "...or a linear function (for real-valued inputs, so $g$ is an identity mapping):\n",
      "\n",
      "$$h = Wx$$\n",
      "\n",
      "where $W$ is the matrix of recognition weights, and $x$ is a $d$-dimensional input vector. \n",
      "\n",
      "The decoder function is also be a sigmoid (so $m$ is a sigmoid):\n",
      "\n",
      "$$\\bar{x} = \\frac{1}{1 + {\\rm{exp}}(-( W^{T}h))}$$\n",
      "\n",
      "...or a linear function (so $m$ is an identity mapping):\n",
      "\n",
      "$$\\bar{x} = W^{T}h $$\n",
      "\n",
      "where $W^{T}$ is the matrix of generative weights connecting the input layer and the output layer. Other types of input and functions are also possible, but real-valued and binary inputs are some of the most common (Larochelle). \n",
      "\n",
      "## The loss function (reconstruction error) ##\n",
      "\n",
      "The loss function for learning autoencoders is a function of the difference between the output $\\hat{x}$ and the input $x$. This reconstruction loss function can take one of several forms depending on the type of data under consideration. \n",
      "\n",
      "### Binary input units ###\n",
      "\n",
      "If the input $x$ are binary units, and we have a sigmoid activation function in the decoder (going from the hidden to the output), we use the cross-entropy error function:\n",
      "\n",
      "$$f(x) = - \\sum_{k=1}^{n}\\ \\left(\\ x_k {\\rm{\\log}}(\\bar{x}_k) + (1 - x_k){\\rm{log}}(1 - \\bar{x}_k)\\ \\right)$$\n",
      "\n",
      "Recall the definition of entropy between two distributions (suppose $X$ is a random variable with realizations $x$, and $P(X = x)$ and $Q(X = x)$: \n",
      "\n",
      "$$- \\sum_{x \\in X} P(X = x) {\\ \\rm{log}}\\ Q(X = x)$$ \n",
      "\n",
      "If $X \\in \\left\\{0, 1\\right\\}$, then this is a sum over the probability that $X = 0$ and that $X = 1$, so this becomes:\n",
      "\n",
      "$$- \\left[P(X = 1)\\ {\\rm{log}\\ } Q(X = 1) + P(X = 0)\\ {\\rm{log}\\ } Q(X = 0)\\right]$$\n",
      "\n",
      "Now if we follow the autoencoder framework, $P(X = 1)$ is our $x_k$, and $Q(x = 1)$ is our $\\bar{x}_k$, and $P(X = 0) = 1 - x_k$, $Q(X = 0) = 1 - \\bar{x}_k$. Considering all data points $x_k$, we arrive at:\n",
      "\n",
      "$$ - \\sum_{k=1}^{n} \\ \\left(\\ x_k {\\rm{\\log}}(\\bar{x}_k) + (1 - x_k){\\rm{log}}(1 - \\bar{x}_k)\\ \\right)$$\n",
      "\n",
      "This error function makes a lot of sense for binary units. To illustrate this, suppose for the moment we're only interested in one unit, $x_k$. If the true value of $x_k$ is 1, then the second term is 0 and the first term becomes $- {\\rm{log}} (\\bar{x_k})$. Since we want to minimize reconstruction error, we want to minimize $- {\\rm{log}} (\\bar{x_k})$. By duality, this is the same as maximizing $ {\\rm{log}} (\\bar{x_k})$. So we're pushing the log of the output to be as close as possible to 1. \n",
      "\n",
      "If the true value of $x_k$ is 0, then the first term is 0 and the second term becomes $ - {\\rm{log}} (1 - \\bar{x}_k)$. Using the same reasoning as above, we're maximizing ${\\rm{log}}(1 - \\bar{x}_k)$, trying to push $\\bar{x}_k$ to 0 to match the input $x_k$.\n",
      "\n",
      "\n",
      "### Real-valued input units ###\n",
      "\n",
      "If the input $x$ are real-valued, it's common to use squared error loss (Euclidean distance).\n",
      "\n",
      "$$f(x) = - \\frac{1}{2} \\sum_{k=1}^{n}\\ (\\bar{x}_k - x_k)^2$$\n",
      "\n",
      "Typically we would also have, instead of sigmoid nonlinearities, just plain linear activation functions.\n",
      "\n",
      "\n",
      "### How learning works ###\n",
      "\n",
      "Learning, as usual, is conducted via gradient descent. Whether we use a cross-entopy error function or a squared error function for the reconstruction loss, the gradient of the pre-activation of the decoder (that is, the gradient of $\\hat{a}(x)$) with respect to the loss is the same:\n",
      "\n",
      "$$\\nabla_{\\hat{a}(x_k)} = \\bar{x}_k - x_k$$ \n",
      "\n",
      "From this point, we can use the backpropagation algorithm to get the gradients with respect to the parameters. One important point is that if we choose to use tied weights as shown here, the gradient with respect to the weight matrix W will be a sum of the gradient given in $W^T$ and the gradient given in $W$.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "<!--\n",
      "### Common traditional autoencoders ###\n",
      "\n",
      "#### Linear autoencoders ####\n",
      "\n",
      "Suppose we have real-valued input $x$. If $m$ and $a$ are linear functions (identity mappings) we get a linear autoencoder. Of course, since this would just yield a sequence of linear transformations, this can be compressed into just one The sigmoid function that we usually use may not be necessary, or even useful, in some cases.\n",
      "\n",
      "Relationship between Principal Components Analysis (PCA) and linear autoencoders\n",
      "\n",
      "Interestingly, if we use a squared error loss term and a linear decoder, the optimal hidden layer (in terms of reconstruction error) is given by a linear transformation.\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Denoising autoencoders ##\n",
      "\n",
      "A traditional autoencoder learns the best set of hidden units that minimize the reconstruction loss, and has fewer hidden nodes than inputs. The point of this classic autoencoder is to learn a more compact representation of the data. The problem with more compact descriptions is that there's less room for error. That is, autoencoders with fewer hidden nodes than input nodes are not robust to the presence of noise in the data. \n",
      "\n",
      "To make the system more robust to noise, we need an overcomplete representation. That is, we need more hidden units than input units. At first glance, this seems like a bad idea. The hidden layer could simply learn the identity function, and reconstruct the inputs exactly, instead of learning a useful representation. Of course this is true, but once noise is injected, a simple copy would not be a faithful representation of the true input. This is best seen through an image:\n",
      "<div style=\"float: right\">\n",
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/DenoisingAutoEncoderProperUse.png\" alt=\"\" style=\"width:500px\">\n",
      "</div>\n",
      "We now have four layers: \n",
      "\n",
      "1) A non-noisy input layer $x$\n",
      "\n",
      "2) Input with injected noise $\\tilde{x}$. Noise is injected through $P(\\tilde{x} \\mid x)$\n",
      "\n",
      "3) A hidden layer $h$\n",
      "\n",
      "4) An output layer $\\bar{x}$\n",
      "\n",
      "The goal of a denoising autoencoder is the same as that of a traditional encoder: to reconstruct the input as faithfully as possible. However, now the data being given to the encoder are corrupted. For example, there might be some missing data (0s in place of some of the $x$ values), or the $x$ values might be perturbed by some noise. The decoder then has to take the code based on the corrupted values $\\tilde{x}$ and reconstruct $x$ as best it can. \n",
      "\n",
      "One way to inject noise is to randomly remove some of the input elements (that is, replace their values with zeros). Since the noisy input layer is connected to the hidden layer, and the original input is not, learning the identity function would not be helpful for reconstructing the true input (because then the hidden units connecting to the noisy $\\tilde{x}$ would only ever be 0). Another way to include noise is to add Gaussian noise, with mean 0 and some covariance matrix, to the original data. The covariance matrix would then be a hyperparameter to be optimized.\n",
      "\n",
      "The reconstruction error still compares the output $\\bar{x}$ to the true data $x$, so there is a signal for learning. Of course, if the input is entirely noise, then the denoising autoencoder should not reproduce a faithful example. That is, if we input an $x_k$ vector that is random noise, the autoencoder should not think that $x_k$ is a true example of our data. In fact, the random noise would look so different from our data (even our corrupted data) that the reconstruction would (and should) be nonsense. So a denoising autoencoder works for 'small' amounts of noise (for example, [this tutorial](http://deeplearning.net/tutorial/dA.html) uses a corruption level of 0.3).\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<!--\n",
      "### Contractive autoencoders ###\n",
      "\n",
      "Another common extension to classical autoencoders is the so-called contractive autoencoder. Instead of injecting noise in the system, this autoencoder (with an overcomplete representation) will penalize the identity as a solution. The goal is to construct a system that will only extract features that reflect variations in the training set. All other variability should be penalized. This necessitates another term in the loss function:\n",
      "\n",
      "new loss function = reconstruction error + hyperparameter*(Jacobian of the encoder)\n",
      "-->"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sparse autoencoders ## \n",
      "\n",
      "To wrap up the discussion on non-deep autoencoders, a variant called a sparse autoencoder has also generated interest recently. In a sparse autoencoder, an overcomplete representation is also used, but sparsity is enforced. The knee-jerk reaction here is to dismiss the structure as being the same as a typical undercomplete autoencoder (where the number of hidden units is less than the number of input units). However, the loss function is slightly different. I'll discuss this purely from a binary unit point of view. So we have sigmoid activation functions.\n",
      "\n",
      "\n",
      "Here, we want to enforce a sparsity constraint. That is, we want only some of the hidden units to be active at any given time. To accomplish this, we restrict the average activation of a hidden unit $p$ to be some small number, and do so for all $p$. So very few hidden units are active at any given time. This sparsity constraint forces all hidden units to learn useful features, so that even when only a few are being used to reconstruct the input, they can still get a pretty good guess at what the input should look like.\n",
      "\n",
      "Using notation from Ng (2011), suppose $g_j(Wx_k)$ is the activation of hidden node $j$ when input $x_k$ is specified. Then the average activation of hidden unit $j$ is given by:\n",
      "\n",
      "$$\\hat{\\rho_j} = \\frac{1}{n} \\sum_{k = 1}^{n} g_j(Wx_k)$$\n",
      "\n",
      "We want to enforce the constraint that $\\hat{\\rho_j} = \\rho$, where $\\rho$ is a sparsity parameter, typically close to 0. So we need to add a penalty term to the original objective function (which is the usual reconstruction error). The term should penalize values of $\\hat{\\rho_j}$ that are far from $\\rho$. \n",
      "\n",
      "A unit is either activated or it is not (so its activation is either 0 or 1). So the activation of the hidden unit $j$ can be said to follow a Bernoulli distribution with mean $\\rho_j$, its average activation. We want to measure how different $\\hat{\\rho_j}$ is from $\\rho$, which can be said to be the mean of another Bernoulli distribution (Ng).\n",
      "\n",
      "The KL divergence is a measure of how two probability distributions differ. In this case, we'll have:\n",
      "\n",
      "$${\\rm{KL}}(\\rho || \\hat{\\rho_j}) = \\sum_{j = 1}^{m} \\left(\\rho {\\rm{log}} \\frac{\\rho}{\\hat{\\rho_j}} + (1 - \\rho) {\\rm{log}} \\frac{1 - \\rho}{1 - \\hat{\\rho_j}}\\right)$$\n",
      "\n",
      "where $m$ is the number of units in the hidden layer, and $j$ indexes the hidden units. When $\\rho_j$ is close to $\\rho$, the KL divergence will be close to 0.\n",
      "\n",
      "The new loss function is then the original cross-entropy loss plus the KL divergence:\n",
      "\n",
      "$$f(x) = \\left(- \\sum_{k=1}^{n}\\ \\left(\\ x_k {\\rm{\\log}}(\\bar{x}_k) + (1 - x_k){\\rm{log}}(1 - \\bar{x}_k)\\ \\right) \\right)+ \\beta {\\rm{KL}}(\\rho || \\hat{\\rho_j})$$\n",
      "\n",
      "where $\\beta$ is a parameter that controls the weight of the sparsity term. Regular gradient descent with backpropagation can be used to train the parameters. The only difference is in one term of the derivative, which is explained further in [Ng (2011)](http://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf), but is beyond the scope of this brief introduction.\n",
      "\n",
      "<!--\n",
      "The average activation across all hidden units also follows a Bernoulli distribution, since order matters (for example, if we have 6 hidden units, 000111 is very different from 111000, because different units are activated). \n",
      "-->\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stacking autoencoders ###\n",
      "\n",
      "Now that we've gone over a few variants of autoencoders, we can talk about going deeper. Autoencoders with many hidden layers are called Deep Autoencoders. They can model more complex relationships among the training data. They use a symmetric architecture, like we've already seen, but with more layers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### References ###\n",
      "\n",
      "Delalleau, O. 2/7/13. Google Groups: theano-users. Answer to \"Why use tied weights in autoencoders?\". [Online](https://groups.google.com/forum/#!topic/theano-users/QilEmkFvDoE)\n",
      "\n",
      "Hinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimum description length, and Helmholtz free energy. Advances in Neural Information Processing Systems 6. J. D. Cowan, G. Tesauro and J. Alspector (Eds.), Morgan Kaufmann: San Mateo, CA.\n",
      "\n",
      "Larochelle, H. (2013). Youtube: Neural Networks (6.1 - 6.6). Universit\u00e9 de Sherbrooke. [Online](http://bit.ly/1H7wN9h)\n",
      "\n",
      "Michl, P. (2013) Structure learning with deep autoencoders. Network Modeling Seminar. Universitat Heidelberg. [Online](https://www.mathi.uni-heidelberg.de/~pmichl/files/talks/Network%20modeling%20seminar%20-%20Structure%20learning%20with%20deep%20autoencoders.pdf)\n",
      "\n",
      "Ng, Andrew (2011). Sparse Autoencoder. CS294A Lecture Notes. Stanford University. [Online](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}