{
 "metadata": {
  "name": "",
  "signature": "sha256:848fff46d2dcac3df0e07c7726f7d57c78fb9f912b6919330f101a5eeb51b1a9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Steps for building the stochastic variational inference algorithm for a Gaussian mixture ###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/GaussianMixturePlate.png\" alt=\"Plate model of a simplified Gaussian mixture model\" style=\"width:500px\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####1) Specify the model assumptions####\n",
      "\n",
      "In a Gaussian mixture model, we assume that each observation $x_i$ was generated by one and only one of the $k$ clusters in the model. \n",
      "\n",
      "Each component Gaussian distribution will have mean $\\mu_k$ and covariance matrix $\\omega_k$\n",
      "\n",
      "Each observation $x_i$ is generated by one and only one of the $k$ clusters, indicated by $z_i$.\n",
      "\n",
      "Gaussian components are bivariate.\n",
      "\n",
      "\n",
      "####2) Get the complete conditionals####\n",
      "\n",
      "A complete conditional is the conditional distribution of a variable or parameter (same thing in Bayesian inference) of interest, given all of the other variables in the model (both hidden and observed) (Hoffman et al 2013).\n",
      "\n",
      "The complete conditionals are given by Blei (2014), and are explained here.\n",
      "\n",
      "The complete conditional for the cluster assignment ($z_i$):\n",
      "\n",
      "Note that $P(z_i) \\sim {\\rm{Categorical}}(\\pi_i)$\n",
      "\n",
      "According to the plate diagram, the cluster assignment $z_i$ for observation $x_i$ is conditionally independent of all other cluster assignments $z_j$. So the complete conditional\n",
      "\n",
      "$$P(z_i \\mid \\mu_{1:k}, z_{-i}, x_{1:n})$$\n",
      "\n",
      "is the same as \n",
      "\n",
      "$$P(z_i \\mid \\mu_{z_i}, x_{i})$$\n",
      "\n",
      "From conditional probability:\n",
      "\n",
      "\\begin{align}\n",
      "P(z_i \\mid \\mu_{z_i}, x_i) &\\propto P(z_i) P(\\mu_{1:k}, x_i) \\\\[0.5em]\n",
      "&= \\pi_{z_i}  \\ell(x_i; \\mu_{z_i}, \\sigma^2) \\\\[0.5em]\n",
      "\\end{align}\n",
      "\n",
      "So the complete conditional for the cluster assignments is a Categorical distribution, where the parameter is equal to the likelihood of the observation under the $k^{th}$ cluster.\n",
      "\n",
      "-------\n",
      "\n",
      "The complete conditional for the cluster means ($\\mu_k$):\n",
      "\n",
      "Note that $\\mu_k \\sim MVN(0, \\omega^2)$\n",
      "\n",
      "According to the plate diagram, the cluster mean $\\mu_k$ is conditionally independent of all other cluster means. So the complete conditional\n",
      "\n",
      "\\begin{align}\n",
      "P(\\mu_k \\mid \\mu_{-k}, z_{1:n}, x_{1:n}) &= P(\\mu_k \\mid z_{1:n}, x_{1:n}) \\\\[0.5em]\n",
      "&= MVN(\\hat\\mu_k, \\hat\\omega_k) \\\\[0.5em]\n",
      "\\end{align}\n",
      "\n",
      "where \n",
      "\n",
      "$$\\hat\\mu_k = \\left(\\displaystyle\\frac{\\frac{\\sum_{i=1}^n z_i^{k}}{\\sigma^2}}{\\frac{\\sum_{i=1}^n z_i^{k}}{\\sigma^2} + \\frac{1}{\\omega^2}}\\right) \\frac{\\sum_{i=1}^n z_i^{k} x_i}{\\sum_{i=1}^n z_i^{k}}$$\n",
      "\n",
      "$$\\hat\\omega_k = \\left(\\frac{\\sum_{i=1}^n z_i^{k}}{\\sigma^2} + \\frac{1}{\\omega^2}\\right)^{-1}$$\n",
      "\n",
      "and $\\sigma^2$ and $\\omega^2$ are both 2x2 matrices, and are fixed constants, according to the plate diagram.\n",
      "\n",
      "\n",
      "####3) Get the mean field variational family####\n",
      "\n",
      "Per Hoffman et al (2013), \"in mean-field variational inference, the variational distribution is in the same family as the complete conditional\".\n",
      "\n",
      "Let the global variational parameters (the parameters governing the variational distribution of $\\mu_k$) be denoted $\\lambda_k$.\n",
      "\n",
      "Let the local variational parameters (the parameters governing the variational distribution of $z_i$) be denoted $\\phi_i$.\n",
      "\n",
      "So the variational distributions are:\n",
      "\n",
      "$$Q(\\mu_k \\mid \\lambda_k) \\sim {\\rm{MVN}}(\\lambda_k, \\hat\\omega_k)$$\n",
      "\n",
      "$$Q(z_i \\mid \\phi_i) \\sim {\\rm{Cat}}(\\pi_{z_i} \\ell(x_i ; \\mu_{z_i}, \\sigma^2))$$\n",
      "\n",
      "####4) Get the stochastic variational inference algorithm.####"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The generic algorithm (from the [stochastic variational inference presentation](https://github.com/caugusta/variational-inference/raw/master/PresStochVarInf.ipynb)) and from Hoffman et al (2013) is:\n",
      "    \n",
      "Randomly initialize global parameter(s) $\\lambda$\n",
      "\n",
      "Set a step size schedule $\\epsilon_t$\n",
      "\n",
      "Repeat\n",
      "\n",
      "1) Sample an observation $x_i$ from your dataset, according to a DU[1, n] distribution.\n",
      "\n",
      "2) Compute the local variational parameter $\\phi_i$ for that particular observation, according to:\n",
      "\n",
      "  $$\\phi_i = E_{\\lambda^{(t-1)}}\\left[\\eta_g (x_i, \\beta) \\right]$$\n",
      "  \n",
      "3) Make believe that $x_i$ is the only observation in the dataset, and calculate\n",
      "\n",
      "  $$\\hat{\\lambda} = E_{\\phi} \\left[ \\eta_g (x_i, z_i)\\right]$$\n",
      "  \n",
      "4) Update the current estimate of the global variational parameter:\n",
      "\n",
      "  $$\\lambda^{t} = (1 - \\epsilon_t) \\lambda^{(t-1)} + \\epsilon_t\\hat{\\lambda}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $\\eta_g(x_i, z_i)$ are the natural (canonical) parameters of the global complete conditional."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the specific algorithm, for gaussian mixture models, is:\n",
      "\n",
      "Randomly initialize global parameter(s) $\\lambda$\n",
      "\n",
      "Set a step size schedule $\\rho_t = (1 + t)^{-\\kappa}$\n",
      "\n",
      "Repeat\n",
      "\n",
      "1) Sample an observation $x_i$ from your dataset, according to a DU[1, n] distribution.\n",
      "\n",
      "2) Compute the local variational parameter $\\phi_i$ for that particular observation, according to:\n",
      "\n",
      "  $$\\phi_i = \\pi_{z_i} \\ell(x_i; \\lambda^{t-1}, \\sigma^2) $$\n",
      "  \n",
      "Since the expectation of a categorical distribution is the parameter itself, and we evaluate this expectation holding the global variational parameter $\\lambda$ constant. This is evaluated for all $k$ Gaussian clusters (so in a more complete notation, $\\lambda^{k, (t-1)}$ would be the global variational parameter modifying the mean $\\mu_k$ of cluster $k$ at the previous iteration.)\n",
      "  \n",
      "3) Make believe that $x_i$ is the only observation in the dataset, and calculate\n",
      "\n",
      "  $$\\hat{\\lambda} = \\hat\\mu$$\n",
      "  \n",
      "Since the expecatation of a MVN distribution is just the mean of the distribution, $\\mu_k$. This is, again, evaluated for all $k$ clusters. Recall the definition of $\\hat\\mu_k$ given above - we hold $z_i$ constant at levels $\\phi_i$.\n",
      "  \n",
      "4) Update the current estimate of the global variational parameter:\n",
      "\n",
      "  $$\\lambda^{t} = (1 - \\rho_t) \\lambda^{(t-1)} + \\rho_t\\hat{\\lambda}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------\n",
      "\n",
      "More compactly:\n",
      "    \n",
      "#### Algorithm ####\n",
      "\n",
      "1. Set $\\pi$, $\\sigma^2$, $\\omega$, initialize $z$\n",
      "\n",
      "2. Initialize $\\lambda$ randomly.\n",
      "\n",
      "3. Set the step size schedule to $\\rho_t = (1 + t)^{-\\kappa}$ ($\\kappa$ will be chosen via cross-validation)\n",
      "\n",
      "4. Repeat\n",
      "\n",
      "a) Sample $x_i$ uniformly from the dataset\n",
      "\n",
      "b) Compute $\\phi_{z_i} = \\ell(x_i; \\mu_{z_i}, \\sigma^2)$, holding $\\mu$ constant at the value $\\lambda$.\n",
      "\n",
      "c) For all clusters $k$, compute $\\hat{\\lambda_k} = \\hat{\\mu_k}$, holding $z_i$ constant at the level $\\phi_{z_i}$, where \n",
      "\n",
      "$$\\hat\\mu_k = \\left(\\frac{\\frac{\\sum_{i=1}^n z_i^{k}}{\\sigma^2}}{\\frac{\\sum_{i=1}^n z_i^{k}}{\\sigma^2} + \\frac{1}{\\omega^2}}\\right) \\frac{\\sum_{i=1}^n z_i^{k} x_i}{\\sum_{i=1}^n z_i^{k}}$$\n",
      "\n",
      "d) Update $\\lambda_k = (1 - \\rho_t)\\lambda_k^{(t-1)} + \\rho_t (\\hat{\\lambda_k})$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}