{
 "metadata": {
  "name": "",
  "signature": "sha256:2d54227d1b26806681009088b161120d48bdb3f1ce80ff50b30b31e08998428d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Variational and Stochastic Variational Inference\n",
      "\n",
      "As a reference for papers that I read on variational and stochastic variational inference, I've put together some explanations from the papers themselves, with a few more details. I start with Bishop (1998).\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Outline**\n",
      "\n",
      "Motivating variational inference\n",
      "\n",
      "Discussing vanilla variational inference, with examples\n",
      "\n",
      "Extension to stochastic variational inference, with examples\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " --------------------------------------\n",
      "\n",
      " ## Motivating variational inference ##\n",
      "\n",
      "Following the notation from Bishop 1998, suppose we have a neural network composed of hidden units H and visible units V. There will also be model parameters w.\n",
      "\n",
      "![alt text](https://github.com/caugusta/variational-inference/raw/master/NeuralNetImage.png \"A neural network with a visible layer V, hidden layer H and output\")\n",
      "\n",
      " Using the notation from Bishop 1998:\n",
      " We have visible units V, hidden units H, model parameters w.\n",
      " The model is defined by the distribution P(H, V|w), that is, \n",
      " the joint distribution of the hidden and visible units given the model parameters.\n",
      "\n",
      " The likelihood of the data given a particular setting of the model parameters is the probability of seeing a particular configuration\n",
      " of visible units under ANY setting of the hidden units, given specific values of model parameters. So the probability of seeing the data regardless of the setting of the hidden variables is:\n",
      " \n",
      " $$\\sum_H P(H, V \\mid w) = P(V \\mid w)$$\n",
      " \n",
      "  This process of summing over unwanted variables is called marginalization. If the variables were instead continuous, this would be an integration instead of a sum.\n",
      " \n",
      " \n",
      " We use the joint distribution and the likelihood to construct the posterior distribution:\n",
      " \n",
      " $$P(H \\mid V, w) = \\frac{P(H, V \\mid w)}{P(V \\mid w)}$$\n",
      " \n",
      " That is, the probability of a particular setting of the hidden units, given fixed visible units and model parameters,\n",
      " is the joint probability of the hidden and visible units given the model parameters, divided by the probability that \n",
      " we see a particular setting of the visible units given the model parameters.\n",
      " A simpler version of the previous equation is P(B|A) = P(B, A)/P(A), which is exactly the formula for conditional probability, \n",
      " as seen in many second-year statistics courses.\n",
      " \n",
      " \n",
      " In a lot of practical problems, the posterior distribution is intractable (the sum over all H has too many terms in it - it's computationally infeasible), but we still need to make inferences based on the posterior. If we want to know, for example, the expected value of a particular hidden variable, we have to marginalize over the others in the posterior. One solution involves sampling from the posterior in a Markov chain Monte Carlo (MCMC) approach. However, often MCMC doesn't scale well to high-dimensional problems. Another solution is given by variational methods, which approximate the intractable posterior distribution using a known function. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------------------------------------\n",
      "### Mean field variational inference ###\n",
      "\n",
      "In statistical physics, mean field theory has long been used to model complex systems using relatively simple ones. A huge assumption is made, though, that the hidden units $H_1, H_2, ..., H_n$ are independent. This assumption is often false, which leads to alternate methods of variational inference, but it illustrates some nice properties, so that's where we'll start.\n",
      "\n",
      "Say we want to approximate the intractable posterior distribution $P(H \\mid V, w)$\n",
      "\n",
      "We can do so using a function $Q$ that is 'close' to the posterior according to some distance metric.\n",
      "\n",
      "That distance metric is based on maximizing the log-likelihood of the data, ${\\rm{ln}}\\ P(V \\mid w)$. Remember log-likelihood is the log probability of the data we see (the visible units V), given the model parameter settings we have.\n",
      "\n",
      "The log-likelihood can be broken down in the following way (Bishop, 1998):\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(V \\mid w) &= {\\rm{ln}} \\sum_H P(H, V \\mid w) \\\\\n",
      "&= {\\rm{ln}} \\sum_H Q(H \\mid V) \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\\\\n",
      "&= {\\rm{ln}}\\ E_Q \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\right] {\\rm{\\, since\\ E_X(g(X)) = \\sum_x g(x)f(x),\\ by\\ definition\\ of\\ expectation,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(H \\mid V)}}\\\\\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{\\, by\\ Jensen's\\ Inequality}} \\\\\n",
      "&= {\\rm{L}}(Q, w) \\\\\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{ln}}\\ P(V \\mid w) &= {\\rm{ln}} \\sum_H P(H, V \\mid w) \\\\\n",
        "&= {\\rm{ln}} \\sum_H Q(H \\mid V) \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\\\\n",
        "&= {\\rm{ln}}\\ E_Q \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\right] {\\rm{\\, since\\ E_X(g(X)) = \\sum_x g(x)f(x),\\ by\\ definition\\ of\\ expectation,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(H \\mid V)}}\\\\\n",
        "&\\ge E_Q\\ {\\rm{ln}} \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{\\, by\\ Jensen's\\ Inequality}} \\\\\n",
        "&= {\\rm{L}}(Q, w) \\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x10251bd90>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Aside: Jensen's Inequality applied here is ${\\rm{ln}}(E(X)) \\ge E({\\rm{ln}} (X))$, and more information is available [here](http://en.wikipedia.org/wiki/Jensen's_inequality). The vast majority of this derivation is given in Bishop (1998)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have a lower bound ${\\rm{L}}(Q, w)$ on the log probability of the visible units. We want to choose $Q(H \\mid V)$ such that this lower bound is as close to the true log probability as possible. Bishop (1998) goes on to say that the KL divergence between $Q$ and $P$, given by\n",
      "\n",
      "$${\\rm{KL}}(Q \\mid P) = -\\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right]$$\n",
      "\n",
      "is the difference between the true log likelihood and the lower bound. I didn't believe that, so by building up from Bishop (2006) (Chapter 10) and Murphy (2012) (Chapter 21), here is the proof:\n",
      "\n",
      "CLAIM: ${\\rm{ln}}\\ P(V \\mid W) = {\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P)$\n",
      "\n",
      "PROOF:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
      "&= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)P(V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{,\\ again\\ due\\ to\\ conditional\\ probability}}\\\\[0.5em]\n",
      "&= \\sum_H Q(H \\mid V) \\left[{\\rm{ln}}\\ P(H \\mid V, w) + {\\rm{ln}}\\ P(V \\mid w) - {\\rm{ln}}\\ {Q(H \\mid V)}\\right] \\\\[0.5em]\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
        "&= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)P(V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{,\\ again\\ due\\ to\\ conditional\\ probability}}\\\\[0.5em]\n",
        "&= \\sum_H Q(H \\mid V) \\left[{\\rm{ln}}\\ P(H \\mid V, w) + {\\rm{ln}}\\ P(V \\mid w) - {\\rm{ln}}\\ {Q(H \\mid V)}\\right] \\\\[0.5em]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102523110>"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the definition of the KL divergence:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$${\\rm{KL}}(Q \\mid P) = -\\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right]$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= \\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right] \\\\[0.5em] \n",
      "&=\\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
      "&=\\sum_H Q(H \\mid V){\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w) - \\sum_H Q(H \\mid V){\\rm{ln\\ }} {Q(H \\mid V)} - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
      "&={\\sum_H  Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w)}\n",
      "\\end{align}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= \\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right] \\\\[0.5em] \n",
        "&=\\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
        "&=\\sum_H Q(H \\mid V){\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w) - \\sum_H Q(H \\mid V){\\rm{ln\\ }} {Q(H \\mid V)} - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
        "&={\\sum_H  Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w)}\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x1025230d0>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall $Q(H \\mid V)$ is a probability mass function, so $\\sum\\limits_H Q(H \\mid V) = 1$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= {\\sum_H  Q(H \\mid V){\\rm{ln}}\\ P(V \\mid w)} \\\\\n",
      "&= {\\rm{ln}}\\ P(V \\mid w) \\sum_H  Q(H \\mid V) {\\rm{\\ , Since\\ ln\\ }} P(V \\mid w) {\\rm{\\ has\\ no\\ H}} \\\\\n",
      "&= {\\rm{ln}}\\ P(V \\mid w)\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= {\\sum_H  Q(H \\mid V){\\rm{ln}}\\ P(V \\mid w)} \\\\\n",
        "&= {\\rm{ln}}\\ P(V \\mid w) \\sum_H  Q(H \\mid V) {\\rm{\\ , Since\\ ln\\ }} P(V \\mid w) {\\rm{\\ has\\ no\\ H}} \\\\\n",
        "&= {\\rm{ln}}\\ P(V \\mid w)\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x10251be10>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore ${\\rm{ln}}\\ P(V \\mid w) = {\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P)$, as desired."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "END OF PROOF\n",
      "\n",
      "Now, if we could choose any $Q$ we like, we would choose the one that makes the KL divergence exactly equal to 0. The KL divergence is 0 only if $Q = P$, which means we wouldn't get anywhere with the problem, since $P$ is intractable. Instead, we restrict the function space to the set of functions that factorize, which brings us finally to the mean field assumption of independent hidden units:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$Q(H \\mid V) = \\prod_{i=1}^{n} Q_i(H_i \\mid V)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is where things get tricky. Now that we have a space of functions over which we'll minimize the KL divergence between the true log probability and the approximation $Q$, how do we find the best functions $Q_i$? From Bishop (2006):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
      "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V)\\ {\\rm{ln\\ }} \\left[\\frac{P(H, V \\mid w)}{\\prod_{i=1}^{n} Q_i(H_i \\mid V)}\\right] \\\\[0.5em] \n",
      "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) - {\\rm{ln\\ }} \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\right] \\\\[0.5em]\n",
      "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\sum_{i=1}^{n} {\\rm{ln\\ }} Q_i(H_i \\mid V) \\right] {\\rm{,\\ and\\ now\\ say\\ we\\ isolate\\ the\\ }} j^{th} {\\rm{\\ hidden\\ variable\\ in\\ }} Q\\\\[0.5em]\n",
      "&= \\sum_{H_j} \\sum_{H \\setminus j} Q_j(H_j \\mid V) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\left({\\rm{ln\\ }} Q_j(H_j \\mid V) + \\sum_{H \\setminus j} {\\rm{ln\\ }} Q_i(H_i \\mid V)\\right) \\right] \\\\[0.5em]\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
        "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V)\\ {\\rm{ln\\ }} \\left[\\frac{P(H, V \\mid w)}{\\prod_{i=1}^{n} Q_i(H_i \\mid V)}\\right] \\\\[0.5em] \n",
        "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) - {\\rm{ln\\ }} \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\right] \\\\[0.5em]\n",
        "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\sum_{i=1}^{n} {\\rm{ln\\ }} Q_i(H_i \\mid V) \\right] {\\rm{,\\ and\\ now\\ say\\ we\\ isolate\\ the\\ }} j^{th} {\\rm{\\ hidden\\ variable\\ in\\ }} Q\\\\[0.5em]\n",
        "&= \\sum_{H_j} \\sum_{H \\setminus j} Q_j(H_j \\mid V) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\left({\\rm{ln\\ }} Q_j(H_j \\mid V) + \\sum_{H \\setminus j} {\\rm{ln\\ }} Q_i(H_i \\mid V)\\right) \\right] \\\\[0.5em]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x1025330d0>"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now expanding:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) = \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w)\n",
      "  \\mspace{150mu}\n",
      "  \\notag\\\\\n",
      "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j | V) \\\\\n",
      "  \\mspace{150mu}\n",
      "  \\notag\\\\\n",
      "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} Q_i(H_i | V) \\\\\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) = \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w)\n",
        "  \\mspace{150mu}\n",
        "  \\notag\\\\\n",
        "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j | V) \\\\\n",
        "  \\mspace{150mu}\n",
        "  \\notag\\\\\n",
        "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} Q_i(H_i | V) \\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102533390>"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall each $Q_i (H_i \\mid V)$ is a probability mass function (pmf), so $\\sum_H Q_i(H_i V) = 1$\n",
      "\n",
      "and $\\sum_{H \\setminus j} \\prod_{H \\setminus j} Q_i(H_i \\mid V) = \\sum_{H \\setminus j} Q_1(H_1 \\mid V) Q_2(H_2 \\mid V) \\cdots Q_n(H_n |V)$ (remember unit $j$ is not in this product)\n",
      "\n",
      "Now if $H_1 \\perp H_2 \\perp \\cdots \\perp H_n$ (that is, $\\forall k \\ne l, h_k \\perp h_l$), from the mean field assumption, then $Q_1(H_1 \\mid V) Q_2(H_2 \\mid V) \\cdots Q_n(H_n |V)$ is exactly a joint distribution of all of the hidden variables (recall P(AB) = P(A)P(B) if and only if A and B are independent)\n",
      "\n",
      "Which means we have a sum of a probability mass function over its entire range: \n",
      "\n",
      "$\\sum_{H \\setminus j} Q_1(H_1 \\mid V) Q_2(H_2 \\mid V) \\cdots Q_n(H_n |V) = 1$, by the law of total probability.\n",
      "\n",
      "That means we're left with:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }}Q_j(H_j \\mid V) + const \\\\\n",
      "\\end{align}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }}Q_j(H_j \\mid V) + const \\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102533450>"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $const$ is constant with respect to the hidden variable $j$. Now rearrange the first term:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) \\\\[0.5em]\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} P(H, V \\mid w) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\\\[0.5em]\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) \\left[ E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w) \\right]\n",
      "\\end{align}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) \\\\[0.5em]\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} P(H, V \\mid w) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\\\[0.5em]\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) \\left[ E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w) \\right]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102521b50>"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w) = \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} P(H, V \\mid w) \\prod_{H \\setminus j} Q_i(H_i \\mid V) =  {\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j \\mid V) + const \\\\[0.5em]\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\left[ \\frac{\\tilde{P}(H, V \\mid w)}{Q_j (H_j \\mid V)} \\right] + const \\\\[0.5em]\n",
      "&= -{\\rm{KL}}\\left(Q_j(H_j \\mid V) \\mid \\tilde{P(H, V \\mid w)}\\right) + const\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j \\mid V) + const \\\\[0.5em]\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\left[ \\frac{\\tilde{P}(H, V \\mid w)}{Q_j (H_j \\mid V)} \\right] + const \\\\[0.5em]\n",
        "&= -{\\rm{KL}}\\left(Q_j(H_j \\mid V) \\mid \\tilde{P(H, V \\mid w)}\\right) + const\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x10251fb90>"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So minimizing the lower bound is the same as minimizing the KL divergence between the newly defined distribution $\\tilde{P}(H, V \\mid w)$ and the distribution for the $j^{th}$ hidden node, $Q_j(H_j \\mid V)$. The KL divergence is at a minimum when $Q_j(H_j \\mid V) = \\tilde{P}(H, V \\mid w)$. Call this optimal $Q$ distribution $Q^{*}$\n",
      "\n",
      "Recall ${\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w) = E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)$\n",
      "\n",
      "So $ {\\rm{\\ ln\\ }} Q^{*}_j(H_j \\mid V) = E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)$\n",
      "\n",
      "And exponentiating both sides yields:\n",
      "\n",
      "$ Q^{*}_j(H_j \\mid V) = {\\rm{exp}}\\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)$\n",
      "\n",
      "Don't forget to divide by a normalization term to ensure $Q_j$ is a valid distribution (although Murphy (2012) argues this can be skipped), and remember we can always choose $const$ to be 0:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "Q^{*}_j(H_j \\mid V) = \\frac{{\\rm{exp}} \\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}{\\sum_{H \\setminus j} \\rm{exp}\\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}\\\\\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "Q^{*}_j(H_j \\mid V) = \\frac{{\\rm{exp}} \\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}{\\sum_{H \\setminus j} \\rm{exp}\\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}\\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x101c85610>"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, finally, we have our optimal distribution for $Q_j$. Note that this must be done for each $Q_j$ independently. This is a good spot, I think, to take advantage of parallelism. Again according to Murphy (2012), we usually work with the following form:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "${\\rm{\\ ln\\ }} Q^{*}_j(H_j \\mid V) = {\\rm{exp}} \\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right) + const$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " **References**: \n",
      " \n",
      " Bishop, C. Pattern Recognition and Machine Learning. Springer, 2006 (Chapter 10)\n",
      " \n",
      " Hoffman, M. D. et al. Stochastic Variational Inference. Journal of Machine Learning Research, vol. 14. (2013), pp 1303-1347.\n",
      " \n",
      " Bishop, C. Variational Learning in Graphical Models and Neural Networks. 1998\n",
      " \n",
      " Mnih, A. and Gregor, K. Neural Variational Inference and Learning in Belief Networks. 31st ICML, 2014; JMLR vol 32.\n",
      " \n",
      " Volz, E. and Meyers L.A. Epidemic thresholds in dynamic contact networks. J.R. Soc. Interface (2009) vol 6, 233-241.\n",
      " \n",
      " Murphy, K.P. Machine Learning: A Proabilistic Perspective. MIT Press, 2012."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Building up stochastic variational inference from vanilla variational inference\n",
      "Start from Bishop 1998.\n",
      "Add from Bishop 2006, with my proofs.\n",
      "Add from Hoffman et al 2013, applied to Bishop 1998 examples, and compared with vanilla variational inference.\n",
      "How can Neural Variational Inference be added to this?\n",
      "Apply to a dynamic model, structured similarly to Epidemic Thresholds on Dynamic Contact Networks work."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}