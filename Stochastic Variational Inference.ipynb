{
 "metadata": {
  "name": "Stochastic Variational Inference.ipynb",
  "signature": "sha256:2d54227d1b26806681009088b161120d48bdb3f1ce80ff50b30b31e08998428d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Introduction to Variational and Stochastic Variational Inference\n",
      "\n",
      "As a reference for papers that I read on variational and stochastic variational inference, I've put together some explanations from the papers themselves, with a few more details. I start with Bishop (1998).\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Outline**\n",
      "\n",
      "Motivating variational inference\n",
      "\n",
      "Discussing vanilla variational inference, with examples\n",
      "\n",
      "Extension to stochastic variational inference, with examples\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " --------------------------------------\n",
      "\n",
      " ## Motivating variational inference ##\n",
      "\n",
      "Following the notation from Bishop 1998, suppose we have a neural network composed of hidden units H and visible units V. There will also be model parameters w.\n",
      "\n",
      "![alt text](https://github.com/caugusta/variational-inference/raw/master/NeuralNetImage.png \"A neural network with a visible layer V, hidden layer H and output\")\n",
      "\n",
      " Using the notation from Bishop 1998:\n",
      " We have visible units V, hidden units H, model parameters w.\n",
      " The model is defined by the distribution P(H, V|w), that is, \n",
      " the joint distribution of the hidden and visible units given the model parameters.\n",
      "\n",
      " The likelihood of the data given a particular setting of the model parameters is the probability of seeing a particular configuration\n",
      " of visible units under ANY setting of the hidden units, given specific values of model parameters. So the probability of seeing the data regardless of the setting of the hidden variables is:\n",
      " \n",
      " $$\\sum_H P(H, V \\mid w) = P(V \\mid w)$$\n",
      " \n",
      "  This process of summing over unwanted variables is called marginalization. If the variables were instead continuous, this would be an integration instead of a sum.\n",
      " \n",
      " \n",
      " We use the joint distribution and the likelihood to construct the posterior distribution:\n",
      " \n",
      " $$P(H \\mid V, w) = \\frac{P(H, V \\mid w)}{P(V \\mid w)}$$\n",
      " \n",
      " That is, the probability of a particular setting of the hidden units, given fixed visible units and model parameters,\n",
      " is the joint probability of the hidden and visible units given the model parameters, divided by the probability that \n",
      " we see a particular setting of the visible units given the model parameters.\n",
      " A simpler version of the previous equation is P(B|A) = P(B, A)/P(A), which is exactly the formula for conditional probability, \n",
      " as seen in many second-year statistics courses.\n",
      " \n",
      " \n",
      " In a lot of practical problems, the posterior distribution is intractable (the sum over all H has too many terms in it - it's computationally infeasible), but we still need to make inferences based on the posterior. If we want to know, for example, the expected value of a particular hidden variable, we have to marginalize over the others in the posterior. One solution involves sampling from the posterior in a Markov chain Monte Carlo (MCMC) approach. However, often MCMC doesn't scale well to high-dimensional problems. Another solution is given by variational methods, which approximate the intractable posterior distribution using a known function. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------------------------------------\n",
      "### Mean field variational inference ###\n",
      "\n",
      "In statistical physics, mean field theory has long been used to model complex systems using relatively simple ones. A huge assumption is made, though, that the hidden units $H_1, H_2, ..., H_n$ are independent. This assumption is often false, which leads to alternate methods of variational inference, but it illustrates some nice properties, so that's where we'll start.\n",
      "\n",
      "Say we want to approximate the intractable posterior distribution $P(H \\mid V, w)$\n",
      "\n",
      "We can do so using a function $Q$ that is 'close' to the posterior according to some distance metric.\n",
      "\n",
      "That distance metric is based on maximizing the log-likelihood of the data, ${\\rm{ln}}\\ P(V \\mid w)$. Remember log-likelihood is the log probability of the data we see (the visible units V), given the model parameter settings we have.\n",
      "\n",
      "The log-likelihood can be broken down in the following way (Bishop, 1998):\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{ln}}\\ P(V \\mid w) &= {\\rm{ln}} \\sum_H P(H, V \\mid w) \\\\\n",
      "&= {\\rm{ln}} \\sum_H Q(H \\mid V) \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\\\\n",
      "&= {\\rm{ln}}\\ E_Q \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\right] {\\rm{\\, since\\ E_X(g(X)) = \\sum_x g(x)f(x),\\ by\\ definition\\ of\\ expectation,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(H \\mid V)}}\\\\\n",
      "&\\ge E_Q\\ {\\rm{ln}} \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{\\, by\\ Jensen's\\ Inequality}} \\\\\n",
      "&= {\\rm{L}}(Q, w) \\\\\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{ln}}\\ P(V \\mid w) &= {\\rm{ln}} \\sum_H P(H, V \\mid w) \\\\\n",
        "&= {\\rm{ln}} \\sum_H Q(H \\mid V) \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\\\\n",
        "&= {\\rm{ln}}\\ E_Q \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)} \\right] {\\rm{\\, since\\ E_X(g(X)) = \\sum_x g(x)f(x),\\ by\\ definition\\ of\\ expectation,\\ taking\\ the\\ expectation\\ with\\ respect\\ to\\ Q(H \\mid V)}}\\\\\n",
        "&\\ge E_Q\\ {\\rm{ln}} \\left[ \\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{\\, by\\ Jensen's\\ Inequality}} \\\\\n",
        "&= {\\rm{L}}(Q, w) \\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x10251bd90>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Aside: Jensen's Inequality applied here is ${\\rm{ln}}(E(X)) \\ge E({\\rm{ln}} (X))$, and more information is available [here](http://en.wikipedia.org/wiki/Jensen's_inequality). The vast majority of this derivation is given in Bishop (1998)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have a lower bound ${\\rm{L}}(Q, w)$ on the log probability of the visible units. We want to choose $Q(H \\mid V)$ such that this lower bound is as close to the true log probability as possible. Bishop (1998) goes on to say that the KL divergence between $Q$ and $P$, given by\n",
      "\n",
      "$${\\rm{KL}}(Q \\mid P) = -\\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right]$$\n",
      "\n",
      "is the difference between the true log likelihood and the lower bound. I didn't believe that, so by building up from Bishop (2006) (Chapter 10) and Murphy (2012) (Chapter 21), here is the proof:\n",
      "\n",
      "CLAIM: ${\\rm{ln}}\\ P(V \\mid W) = {\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P)$\n",
      "\n",
      "PROOF:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
      "&= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)P(V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{,\\ again\\ due\\ to\\ conditional\\ probability}}\\\\[0.5em]\n",
      "&= \\sum_H Q(H \\mid V) \\left[{\\rm{ln}}\\ P(H \\mid V, w) + {\\rm{ln}}\\ P(V \\mid w) - {\\rm{ln}}\\ {Q(H \\mid V)}\\right] \\\\[0.5em]\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
        "&= \\sum_H Q(H \\mid V) {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)P(V \\mid w)}{Q(H \\mid V)}\\right] {\\rm{,\\ again\\ due\\ to\\ conditional\\ probability}}\\\\[0.5em]\n",
        "&= \\sum_H Q(H \\mid V) \\left[{\\rm{ln}}\\ P(H \\mid V, w) + {\\rm{ln}}\\ P(V \\mid w) - {\\rm{ln}}\\ {Q(H \\mid V)}\\right] \\\\[0.5em]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102523110>"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the definition of the KL divergence:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$${\\rm{KL}}(Q \\mid P) = -\\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right]$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= \\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right] \\\\[0.5em] \n",
      "&=\\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
      "&=\\sum_H Q(H \\mid V){\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w) - \\sum_H Q(H \\mid V){\\rm{ln\\ }} {Q(H \\mid V)} - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
      "&={\\sum_H  Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w)}\n",
      "\\end{align}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= \\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H \\mid V, w)}{Q(H \\mid V)}\\right] \\\\[0.5em] \n",
        "&=\\sum_H Q(H \\mid V) \\left[{\\rm{ln\\ }} P(H \\mid V, w) + {\\rm{ln\\ }} P(V \\mid w) - {\\rm{ln\\ }} {Q(H \\mid V)}\\right] - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
        "&=\\sum_H Q(H \\mid V){\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w) - \\sum_H Q(H \\mid V){\\rm{ln\\ }} {Q(H \\mid V)} - \\sum_H Q(H \\mid V)\\ {\\rm{ln\\ }} P(H \\mid V, w) + \\sum_H {Q(H \\mid V)} {\\rm{ln\\ }} Q(H \\mid V) \\\\[0.5em]\n",
        "&={\\sum_H  Q(H \\mid V){\\rm{ln\\ }} P(V \\mid w)}\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x1025230d0>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall $Q(H \\mid V)$ is a probability mass function, so $\\sum\\limits_H Q(H \\mid V) = 1$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= {\\sum_H  Q(H \\mid V){\\rm{ln}}\\ P(V \\mid w)} \\\\\n",
      "&= {\\rm{ln}}\\ P(V \\mid w) \\sum_H  Q(H \\mid V) {\\rm{\\ , Since\\ ln\\ }} P(V \\mid w) {\\rm{\\ has\\ no\\ H}} \\\\\n",
      "&= {\\rm{ln}}\\ P(V \\mid w)\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P) &= {\\sum_H  Q(H \\mid V){\\rm{ln}}\\ P(V \\mid w)} \\\\\n",
        "&= {\\rm{ln}}\\ P(V \\mid w) \\sum_H  Q(H \\mid V) {\\rm{\\ , Since\\ ln\\ }} P(V \\mid w) {\\rm{\\ has\\ no\\ H}} \\\\\n",
        "&= {\\rm{ln}}\\ P(V \\mid w)\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x10251be10>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore ${\\rm{ln}}\\ P(V \\mid w) = {\\rm{L}}(Q, w) + {\\rm{KL}}(Q \\mid P)$, as desired."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "END OF PROOF\n",
      "\n",
      "Now, if we could choose any $Q$ we like, we would choose the one that makes the KL divergence exactly equal to 0. The KL divergence is 0 only if $Q = P$, which means we wouldn't get anywhere with the problem, since $P$ is intractable. Instead, we restrict the function space to the set of functions that factorize, which brings us finally to the mean field assumption of independent hidden units:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$Q(H \\mid V) = \\prod_{i=1}^{n} Q_i(H_i \\mid V)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is where things get tricky. Now that we have a space of functions over which we'll minimize the KL divergence between the true log probability and the approximation $Q$, how do we find the best functions $Q_i$? From Bishop (2006):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
      "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V)\\ {\\rm{ln\\ }} \\left[\\frac{P(H, V \\mid w)}{\\prod_{i=1}^{n} Q_i(H_i \\mid V)}\\right] \\\\[0.5em] \n",
      "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) - {\\rm{ln\\ }} \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\right] \\\\[0.5em]\n",
      "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\sum_{i=1}^{n} {\\rm{ln\\ }} Q_i(H_i \\mid V) \\right] {\\rm{,\\ and\\ now\\ say\\ we\\ isolate\\ the\\ }} j^{th} {\\rm{\\ hidden\\ variable\\ in\\ }} Q\\\\[0.5em]\n",
      "&= \\sum_{H_j} \\sum_{H \\setminus j} Q_j(H_j \\mid V) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\left({\\rm{ln\\ }} Q_j(H_j \\mid V) + \\sum_{H \\setminus j} {\\rm{ln\\ }} Q_i(H_i \\mid V)\\right) \\right] \\\\[0.5em]\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_H Q(H \\mid V)\\ {\\rm{ln}} \\left[\\frac{P(H, V \\mid w)}{Q(H \\mid V)}\\right] \\\\[0.5em]\n",
        "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V)\\ {\\rm{ln\\ }} \\left[\\frac{P(H, V \\mid w)}{\\prod_{i=1}^{n} Q_i(H_i \\mid V)}\\right] \\\\[0.5em] \n",
        "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) - {\\rm{ln\\ }} \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\right] \\\\[0.5em]\n",
        "&= \\sum_H \\prod_{i=1}^{n} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\sum_{i=1}^{n} {\\rm{ln\\ }} Q_i(H_i \\mid V) \\right] {\\rm{,\\ and\\ now\\ say\\ we\\ isolate\\ the\\ }} j^{th} {\\rm{\\ hidden\\ variable\\ in\\ }} Q\\\\[0.5em]\n",
        "&= \\sum_{H_j} \\sum_{H \\setminus j} Q_j(H_j \\mid V) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\left[{\\rm{ln\\ }} P(H, V \\mid w) -  \\left({\\rm{ln\\ }} Q_j(H_j \\mid V) + \\sum_{H \\setminus j} {\\rm{ln\\ }} Q_i(H_i \\mid V)\\right) \\right] \\\\[0.5em]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x1025330d0>"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now expanding:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) = \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w)\n",
      "  \\mspace{150mu}\n",
      "  \\notag\\\\\n",
      "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j | V) \\\\\n",
      "  \\mspace{150mu}\n",
      "  \\notag\\\\\n",
      "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} Q_i(H_i | V) \\\\\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) = \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w)\n",
        "  \\mspace{150mu}\n",
        "  \\notag\\\\\n",
        "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j | V) \\\\\n",
        "  \\mspace{150mu}\n",
        "  \\notag\\\\\n",
        "    - \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} Q_i(H_i | V) \\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102533390>"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall each $Q_i (H_i \\mid V)$ is a probability mass function (pmf), so $\\sum_H Q_i(H_i V) = 1$\n",
      "\n",
      "and $\\sum_{H \\setminus j} \\prod_{H \\setminus j} Q_i(H_i \\mid V) = \\sum_{H \\setminus j} Q_1(H_1 \\mid V) Q_2(H_2 \\mid V) \\cdots Q_n(H_n |V)$ (remember unit $j$ is not in this product)\n",
      "\n",
      "Now if $H_1 \\perp H_2 \\perp \\cdots \\perp H_n$ (that is, $\\forall k \\ne l, h_k \\perp h_l$), from the mean field assumption, then $Q_1(H_1 \\mid V) Q_2(H_2 \\mid V) \\cdots Q_n(H_n |V)$ is exactly a joint distribution of all of the hidden variables (recall P(AB) = P(A)P(B) if and only if A and B are independent)\n",
      "\n",
      "Which means we have a sum of a probability mass function over its entire range: \n",
      "\n",
      "$\\sum_{H \\setminus j} Q_1(H_1 \\mid V) Q_2(H_2 \\mid V) \\cdots Q_n(H_n |V) = 1$, by the law of total probability.\n",
      "\n",
      "That means we're left with:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }}Q_j(H_j \\mid V) + const \\\\\n",
      "\\end{align}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }}Q_j(H_j \\mid V) + const \\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102533450>"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $const$ is constant with respect to the hidden variable $j$. Now rearrange the first term:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "\\begin{align}\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) \\\\[0.5em]\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} P(H, V \\mid w) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\\\[0.5em]\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) \\left[ E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w) \\right]\n",
      "\\end{align}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "\\begin{align}\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}\\prod_{H \\setminus j} Q_i(H_i \\mid V) {\\rm{\\ ln\\ }} P(H, V \\mid w) \\\\[0.5em]\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} P(H, V \\mid w) \\prod_{H \\setminus j} Q_i(H_i \\mid V) \\\\[0.5em]\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) \\left[ E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w) \\right]\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x102521b50>"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where $E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w) = \\sum_{H \\setminus j}{\\rm{\\ ln\\ }} P(H, V \\mid w) \\prod_{H \\setminus j} Q_i(H_i \\mid V) =  {\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j \\mid V) + const \\\\[0.5em]\n",
      "&= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\left[ \\frac{\\tilde{P}(H, V \\mid w)}{Q_j (H_j \\mid V)} \\right] + const \\\\[0.5em]\n",
      "&= -{\\rm{KL}}\\left(Q_j(H_j \\mid V) \\mid \\tilde{P(H, V \\mid w)}\\right) + const\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "{\\rm{L}}(Q, w) &= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w) - \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} Q_j(H_j \\mid V) + const \\\\[0.5em]\n",
        "&= \\sum_{H_j} Q_j(H_j \\mid V) {\\rm{\\ ln\\ }} \\left[ \\frac{\\tilde{P}(H, V \\mid w)}{Q_j (H_j \\mid V)} \\right] + const \\\\[0.5em]\n",
        "&= -{\\rm{KL}}\\left(Q_j(H_j \\mid V) \\mid \\tilde{P(H, V \\mid w)}\\right) + const\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x10251fb90>"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So minimizing the lower bound is the same as minimizing the KL divergence between the newly defined distribution $\\tilde{P}(H, V \\mid w)$ and the distribution for the $j^{th}$ hidden node, $Q_j(H_j \\mid V)$. The KL divergence is at a minimum when $Q_j(H_j \\mid V) = \\tilde{P}(H, V \\mid w)$. Call this optimal $Q$ distribution $Q^{*}$\n",
      "\n",
      "Recall ${\\rm{\\ ln\\ }} \\tilde{P}(H, V \\mid w) = E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)$\n",
      "\n",
      "So $ {\\rm{\\ ln\\ }} Q^{*}_j(H_j \\mid V) = E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)$\n",
      "\n",
      "And exponentiating both sides yields:\n",
      "\n",
      "$ Q^{*}_j(H_j \\mid V) = {\\rm{exp}}\\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)$\n",
      "\n",
      "Don't forget to divide by a normalization term to ensure $Q_j$ is a valid distribution (although Murphy (2012) argues this can be skipped), and remember we can always choose $const$ to be 0:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "Q^{*}_j(H_j \\mid V) = \\frac{{\\rm{exp}} \\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}{\\sum_{H \\setminus j} \\rm{exp}\\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}\\\\\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "Q^{*}_j(H_j \\mid V) = \\frac{{\\rm{exp}} \\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}{\\sum_{H \\setminus j} \\rm{exp}\\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right)}\\\\\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x101c85610>"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, finally, we have our optimal distribution for $Q_j$. Note that this must be done for each $Q_j$ independently. This is a good spot, I think, to take advantage of parallelism. Again according to Murphy (2012), we usually work with the following form:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "${\\rm{\\ ln\\ }} Q^{*}_j(H_j \\mid V) = {\\rm{exp}} \\left(E_{H \\setminus j} {\\rm{\\ ln\\ }} P(H, V \\mid w)\\right) + const$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## NEED SOME EXAMPLES HERE ##"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Stochastic Variational Inference##\n",
      "### Hoffman et al (2013) ###"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose now that we consider a broader class of neural networks. We now introduce local and global structure to the model. If we have many hidden variables and many visible units, it becomes necessary to parallelize the computation to speed things up. Variables and parameters can be local (for example, to a minibatch or to a particular observation) or global. The example given in Hoffman et al (2013) is of a Bayesian mixture of Gaussians: the global hidden units $\\beta$ would be the mixture proportions, and the means and variances of the mixture components (each individual Gaussian distribution that is part of the mixture model), whereas the local hidden variable ($z_n$) is the hidden cluster label for the $n^{th}$ observation (visible unit). Following Hoffman et al (2013), suppose we have:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$N$ observations $x_n, n = 1, \\ldots, N$\n",
      "\n",
      "$\\beta$, a vector of global hidden variables\n",
      "\n",
      "$z = z_{1:N}$, local hidden variables. $z_{n, 1:J}$\n",
      "\n",
      "$\\alpha$, a vector of fixed parameters. We choose to allow $\\alpha$ to govern the prior over $\\beta$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"https://github.com/caugusta/variational-inference/raw/master/StochasticPlateModel.png\" alt=\"A plate model for the model with N observations\" style=\"width:250px\">\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A plate model (above) is a schematic for a model distribution. The plate model is a description of the dependencies in the model distribution. In a plate model, a rectangle with a total (N) in the bottom right indicates that all variables or parameters inside the rectangle (the \"plate\") are indexed from 1 to N. So we have a pictoral representation of $z_n : n = 1, \\ldots, N$ and $x_n : n = 1, \\ldots, N$. The notation for unobserved variables (hidden units, in this case $z$ and $\\beta$) in a plate diagram is an unfilled circle. Observed variables (visible units, in this case $x_n$) are filled circles. Outside the plate are variables or parameters that are not repeated N times. That is, $\\beta$ is a vector of global hidden variables, which means the length of the vector is not necessarily $N$. Finally, fixed parameters are denoted using a black dot (in this case, $\\alpha$, which governs the prior distribution over $\\beta$, is a fixed quantity). The arrows between components in the plate model show how each variable or parameter is related. For example, the arrow from $\\alpha$ to $\\beta$ indicates that $\\beta$ dependes on $\\alpha$. Taken as a whole, the plate model shows that the joint (model) distribution $P(x, z, \\beta \\mid \\alpha)$, and illustrates the dependencies in the model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\\begin{align}\n",
      "P(x, z, \\beta \\mid \\alpha) &= P(\\beta \\mid \\alpha) \\prod_{n=1}^N P(x_n, z_n \\mid \\beta)\n",
      "\\end{align}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\\begin{align}\n",
        "P(x, z, \\beta \\mid \\alpha) &= P(\\beta \\mid \\alpha) \\prod_{n=1}^N P(x_n, z_n \\mid \\beta)\n",
        "\\end{align}"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x10b51ff90>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before, the goal is to approximate the posterior distribution of the hidden units given the visible units, $P(\\beta, z \\mid x)$.\n",
      "\n",
      "Now that we have local and global variables to consider, we can make further use of the plate model to show the conditional independence of observation $n$ from all other observations (the n^{th} observation depends only on the variables local to it, and not on variables that are local to other observations). From the plate model, we see $x_n$ and $z_n$ are dependent on $\\beta$ and, though $\\beta$, also on $\\alpha$. Using the notation $x_{-n}$ and $z_{-n}$ to mean the set of all variables in $1, \\ldots, N$ except for the $n^{th}$\n",
      "\n",
      "$P(x_n, z_n \\mid x_{-n}, z_{-n}, \\beta, \\alpha) = P(x_n, z_n \\mid \\beta, \\alpha)$\n",
      "\n",
      "Essentially this boils down to the traditional second-year statistics concept: $P(A \\mid B, C) = P(A \\mid C)$ if A is independent of B."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next thing we need in this stochastic version of variational inference is a restriction on the complete conditional distribution (the conditional distribution of a hidden unit given the other hidden units and the observations). Because of their very useful properties, we will assume these distributions are in the exponential family, and that they take the following form:\n",
      "\n",
      "$P(\\beta \\mid x, z, \\alpha) = h(\\beta) exp\\left[ \\eta_g (x, z, \\alpha)^T t(\\beta) - a_g (\\eta_g(x, z, \\alpha))\\right]$\n",
      "\n",
      "$$P(z_{nj} \\mid x_n, z_{n, -j}, \\beta) = h(z_{nj})exp\\left[\\eta_\\ell(x_n, z_{n, -j}, \\beta)^T t(z_{nj}) - a_\\ell(\\eta_\\ell(x_n, z_{n, -j}, \\beta))\\right]$$ \n",
      "\n",
      "where the subscript $g$ indicates the complete conditional for the global variables and $\\ell$ for local variables.\n",
      "\n",
      "As an aside, this exponential family form is equivalent to the ones given in other contexts, for example in STAT 6802 (Generalized Linear Models) at the  University of Guelph, the exponential family form is given as $f(x \\mid \\theta) = exp \\left[ \\frac{x \\theta - b(\\theta)}{a(\\phi)} + c(x, \\phi) \\right]$, and in many statistical inference courses as $f(x \\mid \\theta) = exp\\left[ a(\\theta) b(x) + c(\\theta) + d(x) \\right]$. \n",
      "\n",
      "The general form of the exponential family in this variational inference context is $f(x \\mid \\theta) = h(x) exp\\left[ \\eta(\\theta) T(x) - a(\\eta)\\right]$, where $\\eta$ is the natural (or canonical) parameter, $T(x)$ is a vector of sufficient statistics, and $a(\\cdot)$ is called the log-normalizer, since its function is to ensure the distribution is normalized. The function $h(\\cdot)$ is called the base measure, and is a function only of the variable of interest. This reminds me a bit of the baseline measure in survival analysis (that is, looking at a person who doesn't have any of the characteristics of interest).\n",
      "\n",
      "In the second equation, the complete conditional for the hidden variable $z_{nj}$, we need to take into account other variables from the n^{th} example/sample/data point/context (these are all synonyms in this case). So we need the global hidden variables $\\beta$, since they apply to everything, and the other variables from the $n^{th}$ example, but none of the variables that relate to another context, because they're independent of the $n^{th}$ context.\n",
      "\n",
      "One of those useful properties about exponential family distributions is that they all have conjugate prior distributions, which is handy in Bayesian statistics. Additionally, since we have a conditional distribution of $\\beta$ given $x, z, \\alpha$, there will be a conjugate distribution for the n^{th} context given the global parameters, which is also in the exponential family: $$p(x, z \\mid \\beta) = h(x_n, z_n) exp\\left[\\beta^T t(x_n, z_n) - a_\\ell(\\beta)\\right]$$, and the prior distribution for $\\beta$, due to exponential family properties, is also in the exponential family: $$p(\\beta) = h(\\beta) exp\\left[\\alpha^T t(\\beta) - a_g (\\alpha)\\right]$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " **References**: \n",
      " \n",
      " Bishop, C. Pattern Recognition and Machine Learning. Springer, 2006 (Chapter 10)\n",
      " \n",
      " Hoffman, M. D. et al. Stochastic Variational Inference. Journal of Machine Learning Research, vol. 14. (2013), pp 1303-1347.\n",
      " \n",
      " Bishop, C. Variational Learning in Graphical Models and Neural Networks. 1998\n",
      " \n",
      " Mnih, A. and Gregor, K. Neural Variational Inference and Learning in Belief Networks. 31st ICML, 2014; JMLR vol 32.\n",
      " \n",
      " Volz, E. and Meyers L.A. Epidemic thresholds in dynamic contact networks. J.R. Soc. Interface (2009) vol 6, 233-241.\n",
      " \n",
      " Murphy, K.P. Machine Learning: A Proabilistic Perspective. MIT Press, 2012."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Building up stochastic variational inference from vanilla variational inference\n",
      "Start from Bishop 1998.\n",
      "Add from Bishop 2006, with my proofs.\n",
      "Add from Hoffman et al 2013, applied to Bishop 1998 examples, and compared with vanilla variational inference.\n",
      "How can Neural Variational Inference be added to this?\n",
      "Apply to a dynamic model, structured similarly to Epidemic Thresholds on Dynamic Contact Networks work."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}