{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Introduction##\n",
    "\n",
    "Infectious disease modelling and medical diagnostics, speech recognition and word tagging, document retrieval and categorization, and error-correcting codes are just some of the applications of probabilistic graphical models. Whether the discussion is centred on directed (Bayesian) or undirected (Markov Random Field) graphs, the problems of learning and inference have been tackled in various ways. In a graphical model framework, 'learning' refers to estimating parameters associated with a specific model (graphical) architecture, and sometimes to selecting the number of parameters to be used in modelling. By contrast, 'inference' is performed once the model has been built, and is used to make predictions about data and to quantify uncertainty on model parameters. While the focus of this work will be on approximate inference techniques, inference and learning procedures are intricately linked, and each should be kept in mind while the other is being performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Inference techniques##\n",
    "\n",
    "Inference on probabilistic graphical models can only be performed exactly when the graph structure is relatively limited. For example, the junction tree algorithm is an efficient algorithm for tree models (Bayesian networks that take the form of directed acyclic graphs). Another common form of exact inference on tree models is belief propagation (BP); however, if the graph is densely connected, BP can be computationally infeasible [1 - the book]. The generalization of BP to graphs that include cycles is called loopy belief propagation, which will be discussed later with an example. Both BP and loopy BP are variational methods.\n",
    "\n",
    "On general graph structures, inference is computationally intractable due to the intractability of the normalization constant (also called the partition function, or the free energy). This constant often involves computation of a multidimensional integral, which cannot be evaluated analytically. Numerical methods are then required to arrive at a solution. \n",
    "\n",
    "These numerical methods can be stochastic or deterministic in origin. The most popular stochastic method is Markov chain Monte Carlo (MCMC), and its many variants. MCMC methods, though exact, can be slow over large graphical structures. As modelling and inference problems evolve to match the speed of data, approximate inference methods become more ubiquitous. Instead of sampling from the unknown, intractable posterior distribution (as in MCMC), variational methods construct a known distribution to approximate the posterior distribution. The parameters of the known distribution are then chosen to minimize the difference between the posterior and the variational distribution. In this way, we obtain a deterministic approximation to the posterior distribution, often in a faster way than with MCMC methods.\n",
    "\n",
    "The most widely used class of variational inference algorithms are derived from the so-called 'mean-field approximation' from the statistical physics literature. MCMC also had its origins in statistical physics. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From http://www.bayespy.org/examples/gmm.html\n",
    "import numpy as np\n",
    "\n",
    "#First generate data in 4 clusters, with 100 individuals per cluster\n",
    "#For example, y0 has mean [0,0] and identity covariance matrix\n",
    "y0 = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], size=100)\n",
    "y1 = np.random.multivariate_normal([10, 10], [[1, 0], [0, 1]], size=100)\n",
    "y2 = np.random.multivariate_normal([5, 5], [[1, 0], [0, 1]], size=100)\n",
    "y3 = np.random.multivariate_normal([-2, 8], [[1, 0], [0, 1]], size=100)\n",
    "y = np.vstack([y0, y1, y2, y3])\n",
    "#y = y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10839a160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bayespy.plot as bpplt\n",
    "#, obs[:,1][50:99], 'bo',\n",
    "#bpplt.pyplot.plot(y[:,0], y[:,1], 'rx')\n",
    "bpplt.pyplot.plot(y[:,0][0:99], y[:,1][0:99], 'rx', y[:,0][100:199], y[:,1][100:199], 'kx', y[:,0][200:299], y[:,1][200:299], 'ko', y[:,0][300:399], y[:,1][300:399], 'ro')\n",
    "bpplt.pyplot.ylabel('y')\n",
    "bpplt.pyplot.xlabel('x')\n",
    "bpplt.pyplot.title('Positions of individuals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda ~ Wishart(n, A)\n",
      "  n =\n",
      "[ 2.]\n",
      "  A =\n",
      "[[[100000.      0.]\n",
      "  [     0. 100000.]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 400 #The number of data points\n",
    "D = 2 #we're in 2 dimensions\n",
    "K = 20 #guess there are a huge number of clusters, and let the algorithm pare it down.\n",
    "\n",
    "#In a Gaussian mixture model the cluster assignment (label) follows a Categorical distribution\n",
    "#and the prior for a cluster assignment follows a Dirichlet distribution (conjugate prior)\n",
    "\n",
    "from bayespy.nodes import Dirichlet, Categorical\n",
    "alpha = Dirichlet(1e-5*np.ones(K), name='alpha')\n",
    "Z = Categorical(alpha, plates=(N,), name='z')\n",
    "\n",
    "#The conjugate prior for a Gaussian (the mean vector) is Gaussian\n",
    "#The conjugate prior for the precision matrix (inverse covariance matrix) is Wishart\n",
    "from bayespy.nodes import Gaussian, Wishart\n",
    "\n",
    "mu = Gaussian(np.zeros(D), 1e-5*np.identity(D), plates=(K,), name='mu')\n",
    "Lambda = Wishart(D, 1e-5*np.identity(D), plates=(K,), name='Lambda')\n",
    "\n",
    "print(Lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: loglike=-3.613117e+03 (0.014 seconds)\n",
      "Iteration 2: loglike=-3.268603e+03 (0.021 seconds)\n",
      "Iteration 3: loglike=-3.136195e+03 (0.014 seconds)\n",
      "Iteration 4: loglike=-2.981444e+03 (0.018 seconds)\n",
      "Iteration 5: loglike=-2.909490e+03 (0.019 seconds)\n",
      "Iteration 6: loglike=-2.867081e+03 (0.018 seconds)\n",
      "Iteration 7: loglike=-2.804979e+03 (0.015 seconds)\n",
      "Iteration 8: loglike=-2.684564e+03 (0.021 seconds)\n",
      "Iteration 9: loglike=-2.432585e+03 (0.016 seconds)\n",
      "Iteration 10: loglike=-2.379510e+03 (0.013 seconds)\n",
      "Iteration 11: loglike=-2.332555e+03 (0.020 seconds)\n",
      "Iteration 12: loglike=-2.266174e+03 (0.015 seconds)\n",
      "Iteration 13: loglike=-2.203237e+03 (0.011 seconds)\n",
      "Iteration 14: loglike=-2.162500e+03 (0.018 seconds)\n",
      "Iteration 15: loglike=-2.097966e+03 (0.013 seconds)\n",
      "Iteration 16: loglike=-2.063617e+03 (0.012 seconds)\n",
      "Iteration 17: loglike=-2.028999e+03 (0.011 seconds)\n",
      "Iteration 18: loglike=-2.026357e+03 (0.015 seconds)\n",
      "Iteration 19: loglike=-2.023448e+03 (0.013 seconds)\n",
      "Iteration 20: loglike=-2.010935e+03 (0.013 seconds)\n",
      "Iteration 21: loglike=-1.995483e+03 (0.013 seconds)\n",
      "Iteration 22: loglike=-1.980892e+03 (0.016 seconds)\n",
      "Iteration 23: loglike=-1.975075e+03 (0.015 seconds)\n",
      "Iteration 24: loglike=-1.962358e+03 (0.014 seconds)\n",
      "Iteration 25: loglike=-1.935638e+03 (0.017 seconds)\n",
      "Iteration 26: loglike=-1.935137e+03 (0.013 seconds)\n",
      "Iteration 27: loglike=-1.934536e+03 (0.013 seconds)\n",
      "Iteration 28: loglike=-1.933879e+03 (0.012 seconds)\n",
      "Iteration 29: loglike=-1.933249e+03 (0.014 seconds)\n",
      "Iteration 30: loglike=-1.932700e+03 (0.014 seconds)\n",
      "Iteration 31: loglike=-1.932225e+03 (0.015 seconds)\n",
      "Iteration 32: loglike=-1.931774e+03 (0.013 seconds)\n",
      "Iteration 33: loglike=-1.931284e+03 (0.012 seconds)\n",
      "Iteration 34: loglike=-1.930693e+03 (0.016 seconds)\n",
      "Iteration 35: loglike=-1.929945e+03 (0.012 seconds)\n",
      "Iteration 36: loglike=-1.929005e+03 (0.012 seconds)\n",
      "Iteration 37: loglike=-1.927838e+03 (0.014 seconds)\n",
      "Iteration 38: loglike=-1.926414e+03 (0.012 seconds)\n",
      "Iteration 39: loglike=-1.925122e+03 (0.012 seconds)\n",
      "Iteration 40: loglike=-1.924234e+03 (0.012 seconds)\n",
      "Iteration 41: loglike=-1.923083e+03 (0.014 seconds)\n",
      "Iteration 42: loglike=-1.919600e+03 (0.014 seconds)\n",
      "Iteration 43: loglike=-1.907613e+03 (0.012 seconds)\n",
      "Iteration 44: loglike=-1.882676e+03 (0.012 seconds)\n",
      "Iteration 45: loglike=-1.882668e+03 (0.016 seconds)\n",
      "Converged at iteration 45.\n"
     ]
    }
   ],
   "source": [
    "#We know the data come from a Gaussian mixture model\n",
    "\n",
    "from bayespy.nodes import Mixture\n",
    "\n",
    "Y = Mixture(Z, Gaussian, mu, Lambda, name='Y') #This forms the mixture model\n",
    "Z.initialize_from_random() #initialize the categorical random variable\n",
    "\n",
    "#Form a VB object\n",
    "\n",
    "from bayespy.inference import VB\n",
    "\n",
    "Q = VB(Y, mu, Lambda, Z, alpha)\n",
    "\n",
    "#Create observations of the data \n",
    "\n",
    "Y.observe(y)\n",
    "\n",
    "# Run VB until convergence\n",
    "\n",
    "Q.update(repeat=1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the clusters\n",
    "\n",
    "bpplt.gaussian_mixture_2d(Y, alpha=alpha, scale=2)\n",
    "bpplt.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM Algorithm ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%http://www.sciencedirect.com/science/article/pii/S0167947311003963\n",
    "\n",
    "\"The expectation (E) step of the EM algorithm gives the posterior probabilities \n",
    "that the ith observation belongs to the kth component\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The M-step for Gaussian mixture models provides formulas for updating mixing proportions, mean vectors, and dispersion matrices\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"When clusters overlap substantially, it can be challenging for any method to provide a good solution\" - I agree, but we won't run into that problem here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trivial example to start us off (two component Gaussian mixture model). Using the notation from Elements of Statistical Learning (2009):\n",
    "I have data from 2 classes, and I want maximum likelihood estimates for $mu_1$, $mu_2$, $sigma^2_1$, $sigma^2_2$, and the mixing probabilities $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pypr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c2a261cca883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpypr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pypr'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
